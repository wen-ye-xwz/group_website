@article{zhang_hierarchical_2022,
 abstract = {Meta-learning enables quick adaptation of machine learning models to new tasks with limited data. While tasks could come from varying distributions in reality, most of the existing meta-learning methods consider both training and testing tasks as from the same uni-component distribution, overlooking two critical needs of a practical solution: (1) the various sources of tasks may compose a multi-component mixture distribution, and (2) novel tasks may come from a distribution that is unseen during meta-training. In this paper, we demonstrate these two challenges can be solved jointly by modeling the density of task instances. We develop a meta-training framework underlain by a novel Hierarchical Gaussian Mixture based Task Generative Model (HTGM). HTGM extends the widely used empirical process of sampling tasks to a theoretical model, which learns task embeddings, fits mixture distribution of tasks, and enables density-based scoring of novel tasks. The framework is agnostic to the encoder and scales well with large backbone networks. The model parameters are learned end-to-end by maximum likelihood estimation via an Expectation-Maximization algorithm. Extensive experiments on benchmark datasets indicate the effectiveness of our method for both sample classification and novel task detection.},
 author = {Zhang, Yizhou and Ni, Jingchao and Cheng, Wei and Chen, Zhengzhang and Tong, Liang and Chen, Haifeng},
 file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\ULDV5CBN\\Zhang et al. - 2022 - Hierarchical Gaussian Mixture based Task Generative Model for Robust Meta-Learning.pdf:application/pdf},
 language = {en},
 month = {September},
 title = {Hierarchical Gaussian Mixture based Task Generative Model for Robust Meta-Learning},
 url = {https://openreview.net/forum?id=A4fSkNAs6E1},
 urldate = {2025-01-08},
 year = {2022}
}
