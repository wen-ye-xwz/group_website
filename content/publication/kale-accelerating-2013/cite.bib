@inproceedings{kale_accelerating_2013,
 abstract = {Active learning, transfer learning, and related techniques are unified by a core theme: efficient and effective use of available data. Active learning offers scalable solutions for building effective supervised learning models while minimizing annotation effort. Transfer learning utilizes existing labeled data from one task to help learning related tasks for which limited labeled data are available. There has been limited research, however, on how to combine these two techniques. In this paper, we present a simple and principled transfer active learning framework that leverages pre-existing labeled data from related tasks to improve the performance of an active learner. We derive an intuitive bound on generalization error for the classifiers learned by this algorithm that provides insight into the algorithm's behavior and the problem in general. Experimental results using several well-known transfer learning data sets confirm our theoretical analysis and demonstrate the effectiveness of our approach.},
 author = {Kale, David and Liu, Yan},
 booktitle = {2013 IEEE 13th International Conference on Data Mining},
 doi = {10.1109/ICDM.2013.160},
 file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\7Ì‹YRSIX3\\6729602.html:text/html},
 keywords = {Acceleration, Machine Learning, Training, Active Learning, Algorithm design and analysis, Labeling, Learning Theory, Query processing, Supervised learning, Transfer Learning, Upper bound},
 month = {December},
 note = {ISSN: 2374-8486},
 pages = {1085--1090},
 title = {Accelerating Active Learning with Transfer Learning},
 url = {https://ieeexplore.ieee.org/abstract/document/6729602},
 urldate = {2025-01-09},
 year = {2013}
}
