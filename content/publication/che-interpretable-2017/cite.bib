@article{che_interpretable_2017,
 abstract = {Exponential surge in health care data, such as longitudinal data from electronic health records (EHR), sensor data from intensive care unit (ICU), etc., is providing new opportunities to discover meaningful data-driven characteristics and patterns ofdiseases. Recently, deep learning models have been employedfor many computational phenotyping and healthcare prediction tasks to achieve state-of-the-art performance. However, deep models lack interpretability which is crucial for wide adoption in medical research and clinical decision-making. In this paper, we introduce a simple yet powerful knowledge-distillation approach called interpretable mimic learning, which uses gradient boosting trees to learn interpretable models and at the same time achieves strong prediction performance as deep learning models. Experiment results on Pediatric ICU dataset for acute lung injury (ALI) show that our proposed method not only outperforms state-of-the-art approaches for morality and ventilator free days prediction tasks but can also provide interpretable models to clinicians.},
 author = {Che, Zhengping and Purushotham, Sanjay and Khemani, Robinder and Liu, Yan},
 file = {PubMed Central Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\Y7NMW9HM\\Che et al. - 2017 - Interpretable Deep Models for ICU Outcome Prediction.pdf:application/pdf},
 issn = {1942-597X},
 journal = {AMIA Annual Symposium Proceedings},
 month = {February},
 pages = {371--380},
 pmcid = {PMC5333206},
 pmid = {28269832},
 title = {Interpretable Deep Models for ICU Outcome Prediction},
 url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333206/},
 urldate = {2025-01-08},
 volume = {2016},
 year = {2017}
}
