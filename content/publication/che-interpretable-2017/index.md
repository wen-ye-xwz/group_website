---
title: Interpretable Deep Models for ICU Outcome Prediction
authors:
- Zhengping Che
- Sanjay Purushotham
- Robinder Khemani
- Yan Liu
date: '2017-02-01'
publishDate: '2025-01-08T22:16:46.995714Z'
publication_types:
- article-journal
publication: '*AMIA Annual Symposium Proceedings*'
abstract: Exponential surge in health care data, such as longitudinal data from electronic
  health records (EHR), sensor data from intensive care unit (ICU), etc., is providing
  new opportunities to discover meaningful data-driven characteristics and patterns
  ofdiseases. Recently, deep learning models have been employedfor many computational
  phenotyping and healthcare prediction tasks to achieve state-of-the-art performance.
  However, deep models lack interpretability which is crucial for wide adoption in
  medical research and clinical decision-making. In this paper, we introduce a simple
  yet powerful knowledge-distillation approach called interpretable mimic learning,
  which uses gradient boosting trees to learn interpretable models and at the same
  time achieves strong prediction performance as deep learning models. Experiment
  results on Pediatric ICU dataset for acute lung injury (ALI) show that our proposed
  method not only outperforms state-of-the-art approaches for morality and ventilator
  free days prediction tasks but can also provide interpretable models to clinicians.
links:
- name: URL
  url: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333206/
---
