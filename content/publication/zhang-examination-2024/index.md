---
title: An Examination on the Effectiveness of Divide-and-Conquer Prompting in Large
  Language Models
authors:
- Yizhou Zhang
- Lun Du
- Defu Cao
- Qiang Fu
- Yan Liu
date: '2024-07-01'
publishDate: '2025-01-08T05:27:41.784538Z'
publication_types:
- manuscript
publication: '*arXiv*'
doi: 10.48550/arXiv.2402.05359
abstract: Foundation models, such as Large language Models (LLMs), have attracted
  significant amount of interest due to their large number of applications. However,
  when handling tasks involving repetitive sub-tasks and/or deceptive contents, such
  as arithmetic calculation and article-level fake news detection, simple instructional
  prompts suffer from inaccurate responses. Existing works show that more complicated
  prompting strategies, such as Chain-of-Thoughts and Least-to-Most, can unlock LLM's
  powerful capacity in diverse areas. Recent researches reveal that simple divide-and-conquer
  prompting strategy, i.e. simply dividing the input sequence to multiple sub-inputs,
  can also substantially improve LLM's performance in some specific tasks such as
  misinformation detection. In this paper, we aim at examining the utility of divide-and-conquer
  prompting strategy and answer on which kind of tasks this strategy gets advantages.
  Specifically, we provide a theoretic analysis to divide-and-conquer prompting strategy
  and help us identify the specific tasks where DaC prompting can bring performance
  boost with theoretic guarantee. We then present two cases (large integer arithmetic
  and fact verification) where experimental results aligns with our theoretic analysis.
tags:
- Computer Science - Artificial Intelligence
- Computer Science - Computation and Language
- Computer Science - Machine Learning
links:
- name: URL
  url: http://arxiv.org/abs/2402.05359
---
