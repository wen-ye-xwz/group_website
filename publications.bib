
@misc{cao_tempo_2024,
	title = {{TEMPO}: {Prompt}-based {Generative} {Pre}-trained {Transformer} for {Time} {Series} {Forecasting}},
	shorttitle = {{TEMPO}},
	url = {http://arxiv.org/abs/2310.04948},
	doi = {10.48550/arXiv.2310.04948},
	abstract = {The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Cao, Defu and Jia, Furong and Arik, Sercan O. and Pfister, Tomas and Zheng, Yixiang and Ye, Wen and Liu, Yan},
	month = apr,
	year = {2024},
	note = {arXiv:2310.04948 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted by ICLR 2024. Camera Ready Version},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\YCUVGU77\\Cao et al. - 2024 - TEMPO Prompt-based Generative Pre-trained Transformer for Time Series Forecasting.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\38M74X4X\\2310.html:text/html},
}

@misc{cao_timedit_2024,
	title = {{TimeDiT}: {General}-purpose {Diffusion} {Transformers} for {Time} {Series} {Foundation} {Model}},
	shorttitle = {{TimeDiT}},
	url = {http://arxiv.org/abs/2409.02322},
	doi = {10.48550/arXiv.2409.02322},
	abstract = {With recent advances in building foundation models for texts and video data, there is a surge of interest in foundation models for time series. A family of models have been developed, utilizing a temporal auto-regressive generative Transformer architecture, whose effectiveness has been proven in Large Language Models. While the empirical results are promising, almost all existing time series foundation models have only been tested on well-curated ``benchmark'' datasets very similar to texts. However, real-world time series exhibit unique challenges, such as variable channel sizes across domains, missing values, and varying signal sampling intervals due to the multi-resolution nature of real-world data. Additionally, the uni-directional nature of temporally auto-regressive decoding limits the incorporation of domain knowledge, such as physical laws expressed as partial differential equations (PDEs). To address these challenges, we introduce the Time Diffusion Transformer (TimeDiT), a general foundation model for time series that employs a denoising diffusion paradigm instead of temporal auto-regressive generation. TimeDiT leverages the Transformer architecture to capture temporal dependencies and employs diffusion processes to generate high-quality candidate samples without imposing stringent assumptions on the target distribution via novel masking schemes and a channel alignment strategy. Furthermore, we propose a finetuning-free model editing strategy that allows the seamless integration of external knowledge during the sampling process without updating any model parameters. Extensive experiments conducted on a varity of tasks such as forecasting, imputation, and anomaly detection, demonstrate the effectiveness of TimeDiT.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Cao, Defu and Ye, Wen and Zhang, Yizhou and Liu, Yan},
	month = sep,
	year = {2024},
	note = {arXiv:2409.02322 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 23 Pages, 6 Figures, 11 Tables. First present at ICML 2024 Workshop on Foundation Models in the Wild},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\82CLVH34\\Cao et al. - 2024 - TimeDiT General-purpose Diffusion Transformers for Time Series Foundation Model.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\W5FPLSNS\\2409.html:text/html},
}

@article{jia_gpt4mts_2024,
	title = {{GPT4MTS}: {Prompt}-based {Large} {Language} {Model} for {Multimodal} {Time}-series {Forecasting}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{GPT4MTS}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/30383},
	doi = {10.1609/aaai.v38i21.30383},
	abstract = {Time series forecasting is an essential area of machine learning with a wide range of real-world applications. Most of the previous forecasting models aim to capture dynamic characteristics from uni-modal numerical historical data. Although extra knowledge can boost the time series forecasting performance, it is hard to collect such information. In addition, how to fuse the multimodal information is non-trivial. In this paper, we first propose a general principle of collecting the corresponding textual information from different data sources with the help of modern large language models (LLM). Then, we propose a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously, named GPT4MTS. In practice, we propose a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication. Through extensive experiments, we demonstrate the effectiveness of our proposed method on forecasting tasks with extra-textual information.},
	language = {en},
	number = {21},
	urldate = {2025-01-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Jia, Furong and Wang, Kevin and Zheng, Yixiang and Cao, Defu and Liu, Yan},
	month = mar,
	year = {2024},
	note = {Number: 21},
	keywords = {AI For Accessibility},
	pages = {23343--23351},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\Y6SNJ5WI\\Jia et al. - 2024 - GPT4MTS Prompt-based Large Language Model for Multimodal Time-series Forecasting.pdf:application/pdf},
}

@misc{ye_beyond_2024,
	title = {Beyond {Forecasting}: {Compositional} {Time} {Series} {Reasoning} for {End}-to-{End} {Task} {Execution}},
	shorttitle = {Beyond {Forecasting}},
	url = {http://arxiv.org/abs/2410.04047},
	doi = {10.48550/arXiv.2410.04047},
	abstract = {In recent decades, there has been substantial advances in time series models and benchmarks across various individual tasks, such as time series forecasting, classification, and anomaly detection. Meanwhile, compositional reasoning in time series is prevalent in real-world applications (e.g., decision-making and compositional question answering) and is in great demand. Unlike simple tasks that primarily focus on predictive accuracy, compositional reasoning emphasizes the synthesis of diverse information from both time series data and various domain knowledge, making it distinct and extremely more challenging. In this paper, we introduce Compositional Time Series Reasoning, a new task of handling intricate multistep reasoning tasks from time series data. Specifically, this new task focuses on various question instances requiring structural and compositional reasoning abilities on time series data, such as decision-making and compositional question answering. As an initial attempt to tackle this novel task, we developed TS-Reasoner, a program-aided approach that utilizes large language model (LLM) to decompose a complex task into steps of programs that leverage existing time series models and numerical subroutines. Unlike existing reasoning work which only calls off-the-shelf modules, TS-Reasoner allows for the creation of custom modules and provides greater flexibility to incorporate domain knowledge as well as user-specified constraints. We demonstrate the effectiveness of our method through a comprehensive set of experiments. These promising results indicate potential opportunities in the new task of time series reasoning and highlight the need for further research.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Ye, Wen and Zhang, Yizhou and Yang, Wei and Tang, Lumingyuan and Cao, Defu and Cai, Jie and Liu, Yan},
	month = oct,
	year = {2024},
	note = {arXiv:2410.04047 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\9RV83KWQ\\Ye et al. - 2024 - Beyond Forecasting Compositional Time Series Reasoning for End-to-End Task Execution.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\H5HT7GFC\\2410.html:text/html},
}

@inproceedings{zhang_toward_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Toward {Mitigating} {Misinformation} and {Social} {Media} {Manipulation} in {LLM} {Era}},
	isbn = {979-8-4007-0172-6},
	url = {https://dl.acm.org/doi/10.1145/3589335.3641256},
	doi = {10.1145/3589335.3641256},
	abstract = {The pervasive abuse of misinformation to influence public opinion on social media has become increasingly evident in various domains, encompassing politics, as seen in presidential elections, and healthcare, most notably during the recent COVID-19 pandemic. This threat has grown in severity as the development of Large Language Models (LLMs) empowers manipulators to generate highly convincing deceptive content with greater efficiency. Furthermore, the recent strides in chatbots integrated with LLMs, such as ChatGPT, have enabled the creation of human-like interactive social bots, posing a significant challenge to both human users and the social-bot-detection systems of social media platforms.These challenges motivate researchers to develop algorithms to mitigate misinformation and social media manipulations. This tutorial introduces the advanced machine learning researches that are helpful for this goal, including (1) detection of social manipulators, (2) learning causal models of misinformation and social manipulation, and (3) LLM-generated misinformation detection. In addition, we also present possible future directions.},
	urldate = {2025-01-07},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Yizhou and Sharma, Karishma and Du, Lun and Liu, Yan},
	month = may,
	year = {2024},
	pages = {1302--1305},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\W2YRQ67F\\Zhang et al. - 2024 - Toward Mitigating Misinformation and Social Media Manipulation in LLM Era.pdf:application/pdf},
}

@misc{zhang_examination_2024,
	title = {An {Examination} on the {Effectiveness} of {Divide}-and-{Conquer} {Prompting} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.05359},
	doi = {10.48550/arXiv.2402.05359},
	abstract = {Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, simple instructional prompts suffer from inaccurate responses. Existing works show that more complicated prompting strategies, such as Chain-of-Thoughts and Least-to-Most, can unlock LLM's powerful capacity in diverse areas. Recent researches reveal that simple divide-and-conquer prompting strategy, i.e. simply dividing the input sequence to multiple sub-inputs, can also substantially improve LLM's performance in some specific tasks such as misinformation detection. In this paper, we aim at examining the utility of divide-and-conquer prompting strategy and answer on which kind of tasks this strategy gets advantages. Specifically, we provide a theoretic analysis to divide-and-conquer prompting strategy and help us identify the specific tasks where DaC prompting can bring performance boost with theoretic guarantee. We then present two cases (large integer arithmetic and fact verification) where experimental results aligns with our theoretic analysis.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Zhang, Yizhou and Du, Lun and Cao, Defu and Fu, Qiang and Liu, Yan},
	month = jul,
	year = {2024},
	note = {arXiv:2402.05359 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Preprint},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\Z84SYSFW\\Zhang et al. - 2024 - An Examination on the Effectiveness of Divide-and-Conquer Prompting in Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\AIVXZCJV\\2402.html:text/html},
}


@inproceedings{cao_large_2023,
	address = {Brooklyn NY USA},
	title = {Large {Scale} {Financial} {Time} {Series} {Forecasting} with {Multi}-faceted {Model}},
	isbn = {979-8-4007-0240-2},
	url = {https://dl.acm.org/doi/10.1145/3604237.3626868},
	doi = {10.1145/3604237.3626868},
	language = {en},
	urldate = {2025-01-08},
	booktitle = {4th {ACM} {International} {Conference} on {AI} in {Finance}},
	publisher = {ACM},
	author = {Cao, Defu and Zheng, Yixiang and Hassanzadeh, Parisa and Lamba, Simran and Liu, Xiaomo and Liu, Yan},
	month = nov,
	year = {2023},
	pages = {472--480},
	file = {Full Text:C\:\\Users\\selvam\\Zotero\\storage\\SPLQG9RE\\Cao et al. - 2023 - Large Scale Financial Time Series Forecasting with Multi-faceted Model.pdf:application/pdf},
}

@article{zhang_hierarchical_2023,
	title = {Hierarchical {Gaussian} {Mixture} based {Task} {Generative} {Model} for {Robust} {Meta}-{Learning}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/982ca2640e64bf7a1908b028ebc8734a-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-08},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhang, Yizhou and Ni, Jingchao and Cheng, Wei and Chen, Zhengzhang and Tong, Liang and Chen, Haifeng and Liu, Yan},
	month = dec,
	year = {2023},
	pages = {48662--48685},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\XDKQ4N3D\\Zhang et al. - 2023 - Hierarchical Gaussian Mixture based Task Generative Model for Robust Meta-Learning.pdf:application/pdf},
}

@inproceedings{zhang_capturing_2023,
	address = {Cham},
	title = {Capturing {Cross}-{Platform} {Interaction} for {Identifying} {Coordinated} {Accounts} of {Misinformation} {Campaigns}},
	isbn = {978-3-031-28238-6},
	doi = {10.1007/978-3-031-28238-6_61},
	abstract = {Recent years have witnessed the increasing abuse of coordinated accounts on multiple social media platforms. Such accounts are usually operated by misinformation campaigns to manipulate the public opinions on different platforms jointly. However, existing methods mainly focus on detecting such accounts by capturing the coordinated activities within a single platform. As a result, their performances are limited as they can not make use of the information from other platforms. In this work, we propose that capturing cross-platform coordinated activities can bring a significant boost to identifying the accounts operated by misinfromation campaigns. To leverage such information in a practical way, we design a novel Conditional Gaussian-distribution Basis to extract cross-platform correlation from Coordinated Activity Set, which can be easily acquired. Experimental results indicate that our methodology outperform baselines and its own variants that can not leverage cross-platform information.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer Nature Switzerland},
	author = {Zhang, Yizhou and Sharma, Karishma and Liu, Yan},
	editor = {Kamps, Jaap and Goeuriot, Lorraine and Crestani, Fabio and Maistro, Maria and Joho, Hideo and Davis, Brian and Gurrin, Cathal and Kruschwitz, Udo and Caputo, Annalina},
	year = {2023},
	pages = {694--702},
}

@inproceedings{trinh_self-supervised_2023,
	address = {Cham},
	title = {Self-supervised {Sim}-to-{Real} {Kinematics} {Reconstruction} for {Video}-{Based} {Assessment} of {Intraoperative} {Suturing} {Skills}},
	isbn = {978-3-031-43996-4},
	doi = {10.1007/978-3-031-43996-4_68},
	abstract = {Suturing technical skill scores are strong predictors of patient functional recovery following robot-assisted radical prostatectomy (RARP), but manual assessment of these skills is a time and resource-intensive process. By automating suturing skill scoring through computer vision methods, we can significantly reduce the burden on healthcare professionals and enhance the quality and quantity of educational feedbacks. Although automated skill assessment on simulated virtual reality (VR) environments have been promising, applying vision methods to live (‘real’) surgical videos has been challenging due to: 1) the lack of kinematic data from the da Vinci® surgical system, a key source of information for determining the movement and trajectory of robotic manipulators and suturing needles, and 2) the lack of training data due to the labor-intensive task of segmenting and scoring individual stitches from live videos. To address these challenges, we developed a self-supervised pre-training paradigm whereby sim-to-real generalizable representations are learned without requiring any live kinematic annotations. Our model is based on a masked autoencoder (MAE), termed as LiveMAE. We augment live stitches with VR images during pre-training and require LiveMAE to reconstruct images from both domains while also predicting the corresponding kinematics. This process learns a visual-to-kinematic mapping that seeks to locate the positions and orientations of surgical manipulators and needles, deriving “kinematics” from live videos without requiring supervision. With an additional skill-specific finetuning step, LiveMAE surpasses supervised learning approaches across 6 technical skill assessments, ranging from 0.56–0.84 AUC (0.70–0.91 AUPRC), with particular improvements of 35.78\% in AUC for wrist rotation skills and 8.7\% for needle driving skills. Mean-squared error for test VR kinematics was as low as 0.045 for each element of the instrument poses. Our contributions provide the foundation to deliver personalized feedback to surgeons training in VR and performing live prostatectomy procedures.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2023},
	publisher = {Springer Nature Switzerland},
	author = {Trinh, Loc and Chu, Tim and Cui, Zijun and Malpani, Anand and Yang, Cherine and Dalieh, Istabraq and Hui, Alvin and Gomez, Oscar and Liu, Yan and Hung, Andrew},
	editor = {Greenspan, Hayit and Madabhushi, Anant and Mousavi, Parvin and Salcudean, Septimiu and Duncan, James and Syeda-Mahmood, Tanveer and Taylor, Russell},
	year = {2023},
	pages = {708--717},
}

@article{nguyen_transferable_2023,
	title = {Transferable and {Interpretable} {Treatment} {Effectiveness} {Prediction} for {Ovarian} {Cancer} via {Multimodal} {Deep} {Learning}},
	volume = {2023},
	issn = {1942-597X},
	abstract = {Ovarian cancer, a potentially life-threatening disease, is often difficult to treat. There is a critical need for innovations that can assist in improved therapy selection. Although deep learning models are showing promising results, they are employed as a "black-box" and require enormous amounts of data. Therefore, we explore the transferable and interpretable prediction of treatment effectiveness for ovarian cancer patients. Unlike existing works focusing on histopathology images, we propose a multimodal deep learning framework which takes into account not only large histopathology images, but also clinical variables to increase the scope of the data. The results demonstrate that the proposed models achieve high prediction accuracy and interpretability, and can also be transferred to other cancer datasets without significant loss of performance.},
	language = {eng},
	journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
	author = {Nguyen, Emily and Cui, Zijun and Kokaraki, Georgia and Carlson, Joseph and Liu, Yan},
	year = {2023},
	pmid = {38222355},
	pmcid = {PMC10785847},
	keywords = {Deep Learning, Female, Humans, Ovarian Neoplasms, Treatment Outcome},
	pages = {550--558},
}

@incollection{niu_time-delayed_2023,
	series = {Proceedings},
	title = {Time-delayed {Multivariate} {Time} {Series} {Predictions}},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611977653.ch37},
	abstract = {A major issue with real-time monitoring is to collect complete data. Hardware or software failures, network issues or, more frequently, time delays can disrupt such a collection. This results in having two versions of the same information: one in real-time but with potentially missing data, and the another, albeit complete, is delayed. Many works have studied how to handle missing data for classification and prediction. However, to the best of our knowledge, they do not consider how to leverage the delayed complete data to assist in learning the representation of real-time available data with missing values. This is despite the fact that the delayed complete data contain all the information (e.g., periodicities and trends). In this paper, we propose a framework to enhance the representation learning of the real-time available data by aligning the representation of past real-time but with missing data to that of past delayed but complete data. We test both a distance metric and contrastive learning to achieve this alignment. We implement our framework on a Transformer-based model and experiment it on three datasets. The efficiency of our solution is evaluated against seven baselines and considering four distinct patterns of missing data. Our experiments show that this proposal has a significant improvement in prediction accuracy (5.21\% on average) over the baselines.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the 2023 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Niu, Hao and Habault, Guillaume and Legaspi, Roberto and Meng, Chuizheng and Cao, Defu and Wada, Shinya and Ono, Chihiro and Liu, Yan},
	month = jan,
	year = {2023},
	doi = {10.1137/1.9781611977653.ch37},
	pages = {325--333},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\DIXC8L4K\\Niu et al. - 2023 - Time-delayed Multivariate Time Series Predictions.pdf:application/pdf},
}

@inproceedings{cao_svgformer_2023,
	title = {{SVGformer}: {Representation} {Learning} for {Continuous} {Vector} {Graphics} using {Transformers}},
	shorttitle = {{SVGformer}},
	url = {https://ieeexplore.ieee.org/document/10204069},
	doi = {10.1109/CVPR52729.2023.00973},
	abstract = {Advances in representation learning have led to great success in understanding and generating data in various domains. However, in modeling vector graphics data, the pure data-driven approach often yields unsatisfactory results in downstream tasks as existing deep learning methods often require the quantization of SVG parameters and cannot exploit the geometric properties explicitly. In this paper, we propose a transformer-based representation learning model (SVG-former) that directly operates on continuous input values and manipulates the geometric information of SVG to encode outline details and long-distance dependencies. SVGfomer can be used for various downstream tasks: reconstruction, classification, interpolation, retrieval, etc. We have conducted extensive experiments on vector font and icon datasets to show that our model can capture high-quality representation information and outperform the previous state-of-the-art on downstream tasks significantly.},
	urldate = {2025-01-08},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Cao, Defu and Wang, Zhaowen and Echevarria, Jose and Liu, Yan},
	month = jun,
	year = {2023},
	note = {ISSN: 2575-7075},
	keywords = {Deep learning, Deep learning architectures and techniques, Graphics, Interpolation, Quantization (signal), Representation learning, Semantics, Transformers},
	pages = {10093--10102},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\\UHUXAVBQ\\10204069.html:text/html},
}

@misc{enouen_sparse_2023,
	title = {Sparse {Interaction} {Additive} {Networks} via {Feature} {Interaction} {Detection} and {Sparse} {Selection}},
	url = {http://arxiv.org/abs/2209.09326},
	doi = {10.48550/arXiv.2209.09326},
	abstract = {There is currently a large gap in performance between the statistically rigorous methods like linear regression or additive splines and the powerful deep methods using neural networks. Previous works attempting to close this gap have failed to fully investigate the exponentially growing number of feature combinations which deep networks consider automatically during training. In this work, we develop a tractable selection algorithm to efficiently identify the necessary feature combinations by leveraging techniques in feature interaction detection. Our proposed Sparse Interaction Additive Networks (SIAN) construct a bridge from these simple and interpretable models to fully connected neural networks. SIAN achieves competitive performance against state-of-the-art methods across multiple large-scale tabular datasets and consistently finds an optimal tradeoff between the modeling capacity of neural networks and the generalizability of simpler methods.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Enouen, James and Liu, Yan},
	month = nov,
	year = {2023},
	note = {arXiv:2209.09326 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\V873E5PM\\Enouen and Liu - 2023 - Sparse Interaction Additive Networks via Feature Interaction Detection and Sparse Selection.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\IL6TW6X5\\2209.html:text/html},
}

@inproceedings{meng_physics-informed_2022,
	title = {Physics-{Informed} {Long}-{Sequence} {Forecasting} {From} {Multi}-{Resolution} {Spatiotemporal} {Data}},
	volume = {3},
	url = {https://www.ijcai.org/proceedings/2022/304},
	doi = {10.24963/ijcai.2022/304},
	abstract = {Electronic proceedings of IJCAI 2022},
	language = {en},
	urldate = {2025-01-08},
	author = {Meng, Chuizheng and Niu, Hao and Habault, Guillaume and Legaspi, Roberto and Wada, Shinya and Ono, Chihiro and Liu, Yan},
	month = jul,
	year = {2022},
	note = {ISSN: 1045-0823},
	pages = {2189--2195},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\PEVQQEB2\\Meng et al. - 2022 - Physics-Informed Long-Sequence Forecasting From Multi-Resolution Spatiotemporal Data.pdf:application/pdf},
}

@misc{cao_dslob_2022,
	title = {{DSLOB}: {A} {Synthetic} {Limit} {Order} {Book} {Dataset} for {Benchmarking} {Forecasting} {Algorithms} under {Distributional} {Shift}},
	shorttitle = {{DSLOB}},
	url = {http://arxiv.org/abs/2211.11513},
	doi = {10.48550/arXiv.2211.11513},
	abstract = {In electronic trading markets, limit order books (LOBs) provide information about pending buy/sell orders at various price levels for a given security. Recently, there has been a growing interest in using LOB data for resolving downstream machine learning tasks (e.g., forecasting). However, dealing with out-of-distribution (OOD) LOB data is challenging since distributional shifts are unlabeled in current publicly available LOB datasets. Therefore, it is critical to build a synthetic LOB dataset with labeled OOD samples serving as a testbed for developing models that generalize well to unseen scenarios. In this work, we utilize a multi-agent market simulator to build a synthetic LOB dataset, named DSLOB, with and without market stress scenarios, which allows for the design of controlled distributional shift benchmarking. Using the proposed synthetic dataset, we provide a holistic analysis on the forecasting performance of three different state-of-the-art forecasting methods. Our results reflect the need for increased researcher efforts to develop algorithms with robustness to distributional shifts in high-frequency time series data.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Cao, Defu and El-Laham, Yousef and Trinh, Loc and Vyetrenko, Svitlana and Liu, Yan},
	month = nov,
	year = {2022},
	note = {arXiv:2211.11513 [q-fin]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Finance - Statistical Finance},
	annote = {Comment: 11 pages, 5 figures, already accepted by NeurIPS 2022 Distribution Shifts Workshop},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\LU37B653\\Cao et al. - 2022 - DSLOB A Synthetic Limit Order Book Dataset for Benchmarking Forecasting Algorithms under Distributi.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\A9LCBPJL\\2211.html:text/html},
}

@misc{meng_mimic-if_2021,
	title = {{MIMIC}-{IF}: {Interpretability} and {Fairness} {Evaluation} of {Deep} {Learning} {Models} on {MIMIC}-{IV} {Dataset}},
	shorttitle = {{MIMIC}-{IF}},
	url = {http://arxiv.org/abs/2102.06761},
	doi = {10.48550/arXiv.2102.06761},
	abstract = {The recent release of large-scale healthcare datasets has greatly propelled the research of data-driven deep learning models for healthcare applications. However, due to the nature of such deep black-boxed models, concerns about interpretability, fairness, and biases in healthcare scenarios where human lives are at stake call for a careful and thorough examinations of both datasets and models. In this work, we focus on MIMIC-IV (Medical Information Mart for Intensive Care, version IV), the largest publicly available healthcare dataset, and conduct comprehensive analyses of dataset representation bias as well as interpretability and prediction fairness of deep learning models for in-hospital mortality prediction. In terms of interpretabilty, we observe that (1) the best performing interpretability method successfully identifies critical features for mortality prediction on various prediction models; (2) demographic features are important for prediction. In terms of fairness, we observe that (1) there exists disparate treatment in prescribing mechanical ventilation among patient groups across ethnicity, gender and age; (2) all of the studied mortality predictors are generally fair while the IMV-LSTM (Interpretable Multi-Variable Long Short-Term Memory) model provides the most accurate and unbiased predictions across all protected groups. We further draw concrete connections between interpretability methods and fairness metrics by showing how feature importance from interpretability methods can be beneficial in quantifying potential disparities in mortality predictors.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Meng, Chuizheng and Trinh, Loc and Xu, Nan and Liu, Yan},
	month = feb,
	year = {2021},
	note = {arXiv:2102.06761 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\EJK9JFHF\\Meng et al. - 2021 - MIMIC-IF Interpretability and Fairness Evaluation of Deep Learning Models on MIMIC-IV Dataset.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\ZY7DLSWZ\\2102.html:text/html},
}

@misc{cao_estimating_2023,
	title = {Estimating {Treatment} {Effects} in {Continuous} {Time} with {Hidden} {Confounders}},
	url = {http://arxiv.org/abs/2302.09446},
	doi = {10.48550/arXiv.2302.09446},
	abstract = {Estimating treatment effects plays a crucial role in causal inference, having many real-world applications like policy analysis and decision making. Nevertheless, estimating treatment effects in the longitudinal setting in the presence of hidden confounders remains an extremely challenging problem. Recently, there is a growing body of work attempting to obtain unbiased ITE estimates from time-dynamic observational data by ignoring the possible existence of hidden confounders. Additionally, many existing works handling hidden confounders are not applicable for continuous-time settings. In this paper, we extend the line of work focusing on deconfounding in the dynamic time setting in the presence of hidden confounders. We leverage recent advancements in neural differential equations to build a latent factor model using a stochastic controlled differential equation and Lipschitz constrained convolutional operation in order to continuously incorporate information about ongoing interventions and irregularly sampled observations. Experiments on both synthetic and real-world datasets highlight the promise of continuous time methods for estimating treatment effects in the presence of hidden confounders.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Cao, Defu and Enouen, James and Liu, Yan},
	month = feb,
	year = {2023},
	note = {arXiv:2302.09446 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Methodology},
	annote = {Comment: 7 pages. First presentation was at ICML 2022 workshop Continuous time methods for machine learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\4MWQK7AS\\Cao et al. - 2023 - Estimating Treatment Effects in Continuous Time with Hidden Confounders.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\G4V6GUYM\\2302.html:text/html},
}

@misc{meng_when_2022,
	title = {When {Physics} {Meets} {Machine} {Learning}: {A} {Survey} of {Physics}-{Informed} {Machine} {Learning}},
	shorttitle = {When {Physics} {Meets} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2203.16797},
	doi = {10.48550/arXiv.2203.16797},
	abstract = {Physics-informed machine learning (PIML), referring to the combination of prior knowledge of physics, which is the high level abstraction of natural phenomenons and human behaviours in the long history, with data-driven machine learning models, has emerged as an effective way to mitigate the shortage of training data, to increase models' generalizability and to ensure the physical plausibility of results. In this paper, we survey an abundant number of recent works in PIML and summarize them from three aspects: (1) motivations of PIML, (2) physics knowledge in PIML, (3) methods of physics knowledge integration in PIML. We also discuss current challenges and corresponding research opportunities in PIML.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Meng, Chuizheng and Seo, Sungyong and Cao, Defu and Griesemer, Sam and Liu, Yan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.16797 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\95DZDKZT\\Meng et al. - 2022 - When Physics Meets Machine Learning A Survey of Physics-Informed Machine Learning.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\FNJQG9EJ\\2203.html:text/html},
}

@misc{zhang_counterfactual_2022,
	title = {Counterfactual {Neural} {Temporal} {Point} {Process} for {Estimating} {Causal} {Influence} of {Misinformation} on {Social} {Media}},
	url = {http://arxiv.org/abs/2210.07518},
	doi = {10.48550/arXiv.2210.07518},
	abstract = {Recent years have witnessed the rise of misinformation campaigns that spread specific narratives on social media to manipulate public opinions on different areas, such as politics and healthcare. Consequently, an effective and efficient automatic methodology to estimate the influence of the misinformation on user beliefs and activities is needed. However, existing works on misinformation impact estimation either rely on small-scale psychological experiments or can only discover the correlation between user behaviour and misinformation. To address these issues, in this paper, we build up a causal framework that model the causal effect of misinformation from the perspective of temporal point process. To adapt the large-scale data, we design an efficient yet precise way to estimate the Individual Treatment Effect(ITE) via neural temporal point process and gaussian mixture models. Extensive experiments on synthetic dataset verify the effectiveness and efficiency of our model. We further apply our model on a real-world dataset of social media posts and engagements about COVID-19 vaccines. The experimental results indicate that our model recognized identifiable causal effect of misinformation that hurts people's subjective emotions toward the vaccines.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Zhang, Yizhou and Cao, Defu and Liu, Yan},
	month = oct,
	year = {2022},
	note = {arXiv:2210.07518 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	annote = {Comment: 19 pages, 8 figures, already accepted by NeurIPS 2022},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\6UGLSRZP\\Zhang et al. - 2022 - Counterfactual Neural Temporal Point Process for Estimating Causal Influence of Misinformation on So.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\VLNAG834\\2210.html:text/html},
}

@inproceedings{niu_mu2rest_2022,
	address = {Berlin, Heidelberg},
	title = {{Mu2ReST}: {Multi}-resolution {Recursive} {Spatio}-{Temporal} {Transformer} for {Long}-{Term} {Prediction}},
	isbn = {978-3-031-05932-2},
	shorttitle = {{Mu2ReST}},
	url = {https://doi.org/10.1007/978-3-031-05933-9_6},
	doi = {10.1007/978-3-031-05933-9_6},
	abstract = {Long-term spatio-temporal prediction (LTSTP) over different resolutions plays a crucial role in planning and dispatching smart city applications, such as smart transportation and smart grid. The Transformer, which has demonstrated superiority in capturing long-term dependencies, was recently studied for spatio-temporal prediction. However, it is difficult to leverage it using both multi-resolution knowledge and spatio-temporal dependencies to aid LTSTP. The challenge typically lies in addressing two issues: (1) efficiently fusing information across multiple resolutions that demands elaborate and complicated modifications to the model, and (2) handling the necessary long-term sequence that makes concurrent space and time attentions too costly to be performed. To address these issues, we proposed a multi-resolution recursive spatio-temporal transformer (Mu2ReST). It implements a novel multi-resolution structure with recursive prediction from coarser to finer resolutions. This proposal reveals that an arduous modification of the model is not the only way to leverage multi-resolution knowledge. It further uses a redesigned lightweight space-time attention implementation to concurrently capture spatial and temporal dependencies. Experiment results using open and commercial urban datasets demonstrate that Mu2ReST outperforms existing methods for multi-resolution LTSTP tasks.},
	urldate = {2025-01-07},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}: 26th {Pacific}-{Asia} {Conference}, {PAKDD} 2022, {Chengdu}, {China}, {May} 16–19, 2022, {Proceedings}, {Part} {I}},
	publisher = {Springer-Verlag},
	author = {Niu, Hao and Meng, Chuizheng and Cao, Defu and Habault, Guillaume and Legaspi, Roberto and Wada, Shinya and Ono, Chihiro and Liu, Yan},
	month = may,
	year = {2022},
	pages = {68--80},
}

@inproceedings{xu_treatment_2021,
	title = {Treatment {Recommendation} with {Preference}-based {Reinforcement} {Learning}},
	url = {https://ieeexplore.ieee.org/document/9667722},
	doi = {10.1109/ICKG52313.2021.00025},
	abstract = {Treatment recommendation is a complex multi-faceted problem with many treatment goals considered by clini-cians and patients, e.g., optimizing the survival rate, mitigating negative impacts, reducing financial expenses, avoiding over-treatment, etc. Recently, deep reinforcement learning (RL) approaches have gained popularity for treatment recommendation. In this paper, we investigate preference-based reinforcement learning approaches for treatment recommendation, where the reward function is itself learned based on treatment goals, without requiring either expert demonstrations in advance or human involvement during policy learning. We first present an open sim-ulation platform11https://sites.google.com/view/tr-with-prl/ to model the evolution of two diseases, namely Cancer and Sepsis, and individuals' reactions to the received treatment. Secondly, we systematically examine preference-based RL for treatment recommendation via simulated experiments and observe high utility in the learned policy in terms of high survival rate and low side effects, with inferred rewards highly correlated to treatment goals. We further explore the transferability of inferred reward functions and guidelines for agent design to provide insights in achieving the right trade-off among various human objectives with preference-based RL approaches for treatment recommendation in the real world.},
	urldate = {2025-01-08},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Knowledge} ({ICBK})},
	author = {Xu, Nan and Kamra, Nitin and Liu, Yan},
	month = dec,
	year = {2021},
	keywords = {Cancer, Conferences, Correlation, Guidelines, Healthcare, Preference Learning, Reinforcement learning, Reinforcement Learning, Trajectory, Treatment Recommendation},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\\QUYYF5DK\\9667722.html:text/html},
}

@misc{zhang_vigdet_2021,
	title = {{VigDet}: {Knowledge} {Informed} {Neural} {Temporal} {Point} {Process} for {Coordination} {Detection} on {Social} {Media}},
	shorttitle = {{VigDet}},
	url = {http://arxiv.org/abs/2110.15454},
	doi = {10.48550/arXiv.2110.15454},
	abstract = {Recent years have witnessed an increasing use of coordinated accounts on social media, operated by misinformation campaigns to influence public opinion and manipulate social outcomes. Consequently, there is an urgent need to develop an effective methodology for coordinated group detection to combat the misinformation on social media. However, existing works suffer from various drawbacks, such as, either limited performance due to extreme reliance on predefined signatures of coordination, or instead an inability to address the natural sparsity of account activities on social media with useful prior domain knowledge. Therefore, in this paper, we propose a coordination detection framework incorporating neural temporal point process with prior knowledge such as temporal logic or pre-defined filtering functions. Specifically, when modeling the observed data from social media with neural temporal point process, we jointly learn a Gibbs-like distribution of group assignment based on how consistent an assignment is to (1) the account embedding space and (2) the prior knowledge. To address the challenge that the distribution is hard to be efficiently computed and sampled from, we design a theoretically guaranteed variational inference approach to learn a mean-field approximation for it. Experimental results on a real-world dataset show the effectiveness of our proposed method compared to the SOTA model in both unsupervised and semi-supervised settings. We further apply our model on a COVID-19 Vaccine Tweets dataset. The detection result suggests the presence of suspicious coordinated efforts on spreading misinformation about COVID-19 vaccines.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Zhang, Yizhou and Sharma, Karishma and Liu, Yan},
	month = oct,
	year = {2021},
	note = {arXiv:2110.15454 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	annote = {Comment: Accepted by NeurIPS 2021. 17 pages, 5 figures},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\TZPC6IEP\\Zhang et al. - 2021 - VigDet Knowledge Informed Neural Temporal Point Process for Coordination Detection on Social Media.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\2APNQ5V4\\2110.html:text/html},
}

@article{hung_road_2022,
	title = {Road to automating robotic suturing skills assessment: {Battling} mislabeling of the ground truth},
	volume = {171},
	issn = {1532-7361},
	shorttitle = {Road to automating robotic suturing skills assessment},
	doi = {10.1016/j.surg.2021.08.014},
	abstract = {OBJECTIVE: To automate surgeon skills evaluation using robotic instrument kinematic data. Additionally, to implement an unsupervised mislabeling detection algorithm to identify potentially mislabeled samples that can be removed to improve model performance.
METHODS: Video recordings and instrument kinematic data were derived from suturing exercises completed on the Mimic FlexVR robotic simulator. A structured human consensus-building process was developed to determine Robotic Anastomosis Competency Evaluation technical scores across 3 human graders. A 2-layer long short-term memory-based classification model used instrument kinematic data to automate suturing skills assessment. An unsupervised label analyzer (NoiseRank) was used to identify potential mislabeling of skills data. Performance of the long short-term memory model's technical skill score prediction was measured by best area under the curve over the training runs. NoiseRank outputted a ranked list of rated skills assessments based on likelihood of mislabeling.
RESULTS: 22 surgeons performed 226 suturing attempts, which were broken down into 1,404 individual skill assessment points. Automation of needle entry angle, needle driving, and needle withdrawal technical skill scores performed better (area under the curve 0.698-0.705) than needle positioning (0.532) at baseline using all available data. Potential mislabels were subsequently identified by NoiseRank and removed, improving model performance across all domains (area under the curve 0.551-0.766).
CONCLUSION: Using ground truth labels from human graders and robotic instrument kinematic data, machine learning models have automated assessment of detailed suturing technical skills with good performance. Further, an unsupervised mislabeling detection algorithm projected mislabeled data, allowing for their removal and subsequent improvement of model performance.},
	language = {eng},
	number = {4},
	journal = {Surgery},
	author = {Hung, Andrew J. and Rambhatla, Sirisha and Sanford, Daniel I. and Pachauri, Nilay and Vanstrum, Erik and Nguyen, Jessica H. and Liu, Yan},
	month = apr,
	year = {2022},
	pmid = {34538647},
	pmcid = {PMC8924023},
	keywords = {Clinical Competence, Humans, Robotic Surgical Procedures, Robotics, Surgeons, Sutures},
	pages = {915--919},
	file = {PubMed Central Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\6Z88K9HQ\\Hung et al. - 2022 - Road to automating robotic suturing skills assessment Battling mislabeling of the ground truth.pdf:application/pdf},
}

@misc{rambhatla_towards_2020,
	title = {Towards {Accurate} {Spatiotemporal} {COVID}-19 {Risk} {Scores} using {High} {Resolution} {Real}-{World} {Mobility} {Data}},
	url = {http://arxiv.org/abs/2012.07283},
	doi = {10.48550/arXiv.2012.07283},
	abstract = {As countries look towards re-opening of economic activities amidst the ongoing COVID-19 pandemic, ensuring public health has been challenging. While contact tracing only aims to track past activities of infected users, one path to safe reopening is to develop reliable spatiotemporal risk scores to indicate the propensity of the disease. Existing works which aim to develop risk scores either rely on compartmental model-based reproduction numbers (which assume uniform population mixing) or develop coarse-grain spatial scores based on reproduction number (R0) and macro-level density-based mobility statistics. Instead, in this paper, we develop a Hawkes process-based technique to assign relatively fine-grain spatial and temporal risk scores by leveraging high-resolution mobility data based on cell-phone originated location signals. While COVID-19 risk scores also depend on a number of factors specific to an individual, including demography and existing medical conditions, the primary mode of disease transmission is via physical proximity and contact. Therefore, we focus on developing risk scores based on location density and mobility behaviour. We demonstrate the efficacy of the developed risk scores via simulation based on real-world mobility data. Our results show that fine-grain spatiotemporal risk scores based on high-resolution mobility data can provide useful insights and facilitate safe re-opening.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Rambhatla, Sirisha and Zeighami, Sepanta and Shahabi, Kameron and Shahabi, Cyrus and Liu, Yan},
	month = dec,
	year = {2020},
	note = {arXiv:2012.07283 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\IZ67DVKA\\Rambhatla et al. - 2020 - Towards Accurate Spatiotemporal COVID-19 Risk Scores using High Resolution Real-World Mobility Data.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\S6TA6C9X\\2012.html:text/html},
}

@article{hung_pd58-08_2021,
	title = {{PD58}-08 {AUTOMATING} {SUTURING} {SKILLS} {ASSESSMENT} {WITH} {A} {LIMITED} {SURGEON} {DATASET}: {META} {LEARNING}},
	copyright = {© 2021 by American Urological Association Education and Research, Inc.},
	shorttitle = {{PD58}-08 {AUTOMATING} {SUTURING} {SKILLS} {ASSESSMENT} {WITH} {A} {LIMITED} {SURGEON} {DATASET}},
	url = {https://www.auajournals.org/doi/10.1097/JU.0000000000002092.08},
	doi = {10.1097/JU.0000000000002092.08},
	abstract = {INTRODUCTION AND OBJECTIVE:Automating technical skills assessment with machine learning models can revolutionize
how we credential surgeons, and it can identify critical errors before they become
patient complications. Common to many machine learning ...},
	language = {EN},
	urldate = {2025-01-08},
	journal = {The Journal of Urology},
	author = {Hung, Andrew J. and Rambhatla, Sirisha and Sanford, Daniel I. and Pachauri, Nilay and Nguyen, Jessica H. and Liu, Yan},
	month = sep,
	year = {2021},
	note = {Publisher: Wolters KluwerPhiladelphia, PA},
	file = {Full Text:C\:\\Users\\selvam\\Zotero\\storage\\THGD93PB\\Hung et al. - 2021 - PD58-08 AUTOMATING SUTURING SKILLS ASSESSMENT WITH A LIMITED SURGEON DATASET META LEARNING.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\3Y2GCSRX\\JU.0000000000002092.html:text/html},
}

@article{kamra_polsird_2021,
	title = {{PolSIRD}: {Modeling} {Epidemic} {Spread} {Under} {Intervention} {Policies}: {Analyzing} the {First} {Wave} of {COVID}-19 in the {USA}},
	volume = {5},
	issn = {2509-4971},
	shorttitle = {{PolSIRD}},
	doi = {10.1007/s41666-021-00099-3},
	abstract = {Epidemic spread in a population is traditionally modeled via compartmentalized models which represent the free evolution of disease in the absence of any intervention policies. In addition, these models assume full observability of disease cases and do not account for under-reporting. We present a mathematical model, namely PolSIRD, which accounts for the under-reporting by introducing an observation mechanism. It also captures the effects of intervention policies on the disease spread parameters by leveraging intervention policy data along with the reported disease cases. Furthermore, we allow our recurrent model to learn the initial hidden state of all compartments end-to-end along with other parameters via gradient-based training. We apply our model to the spread of the recent global outbreak of COVID-19 in the USA, where our model outperforms the methods employed by the CDC in predicting the spread. We also provide counterfactual simulations from our model to analyze the effect of lifting the intervention policies prematurely and our model correctly predicts the second wave of the epidemic.},
	language = {eng},
	number = {3},
	journal = {Journal of Healthcare Informatics Research},
	author = {Kamra, Nitin and Zhang, Yizhou and Rambhatla, Sirisha and Meng, Chuizheng and Liu, Yan},
	month = sep,
	year = {2021},
	pmid = {34151134},
	pmcid = {PMC8202228},
	keywords = {COVID-19, Epidemic spread modeling, Intervention policies for epidemics, Machine learning for epidemic spread modeling, Spatiotemporal spread modeling},
	pages = {231--248},
	file = {Full Text:C\:\\Users\\selvam\\Zotero\\storage\\XUTIJYNX\\Kamra et al. - 2021 - PolSIRD Modeling Epidemic Spread Under Intervention Policies Analyzing the First Wave of COVID-19.pdf:application/pdf},
}

@inproceedings{kamra_gradient-based_2021,
	title = {Gradient-based optimization for multi-resource spatial coverage problems},
	url = {https://proceedings.mlr.press/v161/kamra21a.html},
	abstract = {Resource allocation for coverage of geographical spaces is a challenging problem in robotics, sensor networks and security domains. Conventional solution approaches either: (a) rely on exploiting spatio-temporal structure of specific coverage problems, or (b) use genetic algorithms when targeting general coverage problems where no special exploitable structure exists. In this work, we propose the coverage gradient theorem, which provides a gradient estimator for a broad class of spatial coverage objectives using a combination of Newton-Leibniz theorem and implicit boundary differentiation. We also propose a tractable framework to approximate the coverage objectives and their gradients using spatial discretization and empirically demonstrate the efficacy of our framework on multi-resource spatial coverage problems.},
	language = {en},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Kamra, Nitin and Liu, Yan},
	month = dec,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1885--1894},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\7RXVV8R7\\Kamra and Liu - 2021 - Gradient-based optimization for multi-resource spatial coverage problems.pdf:application/pdf;Supplementary PDF:C\:\\Users\\selvam\\Zotero\\storage\\5GB7IYDE\\Kamra and Liu - 2021 - Gradient-based optimization for multi-resource spatial coverage problems.pdf:application/pdf},
}

@misc{meng_cross-node_2021,
	title = {Cross-{Node} {Federated} {Graph} {Neural} {Network} for {Spatio}-{Temporal} {Data} {Modeling}},
	url = {http://arxiv.org/abs/2106.05223},
	doi = {10.48550/arXiv.2106.05223},
	abstract = {Vast amount of data generated from networks of sensors, wearables, and the Internet of Things (IoT) devices underscores the need for advanced modeling techniques that leverage the spatio-temporal structure of decentralized data due to the need for edge computation and licensing (data access) issues. While federated learning (FL) has emerged as a framework for model training without requiring direct data sharing and exchange, effectively modeling the complex spatio-temporal dependencies to improve forecasting capabilities still remains an open problem. On the other hand, state-of-the-art spatio-temporal forecasting models assume unfettered access to the data, neglecting constraints on data sharing. To bridge this gap, we propose a federated spatio-temporal model -- Cross-Node Federated Graph Neural Network (CNFGNN) -- which explicitly encodes the underlying graph structure using graph neural network (GNN)-based architecture under the constraint of cross-node federated learning, which requires that data in a network of nodes is generated locally on each node and remains decentralized. CNFGNN operates by disentangling the temporal dynamics modeling on devices and spatial dynamics on the server, utilizing alternating optimization to reduce the communication cost, facilitating computations on the edge devices. Experiments on the traffic flow forecasting task show that CNFGNN achieves the best forecasting performance in both transductive and inductive learning settings with no extra computation cost on edge devices, while incurring modest communication cost.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Meng, Chuizheng and Rambhatla, Sirisha and Liu, Yan},
	month = jun,
	year = {2021},
	note = {arXiv:2106.05223 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: To be published in the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 21)},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\YPFV79MN\\Meng et al. - 2021 - Cross-Node Federated Graph Neural Network for Spatio-Temporal Data Modeling.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\ANJKC8AD\\2106.html:text/html},
}

@misc{sharma_identifying_2021,
	title = {Identifying {Coordinated} {Accounts} on {Social} {Media} through {Hidden} {Influence} and {Group} {Behaviours}},
	url = {http://arxiv.org/abs/2008.11308},
	doi = {10.48550/arXiv.2008.11308},
	abstract = {Disinformation campaigns on social media, involving coordinated activities from malicious accounts towards manipulating public opinion, have become increasingly prevalent. Existing approaches to detect coordinated accounts either make very strict assumptions about coordinated behaviours, or require part of the malicious accounts in the coordinated group to be revealed in order to detect the rest. To address these drawbacks, we propose a generative model, AMDN-HAGE (Attentive Mixture Density Network with Hidden Account Group Estimation) which jointly models account activities and hidden group behaviours based on Temporal Point Processes (TPP) and Gaussian Mixture Model (GMM), to capture inherent characteristics of coordination which is, accounts that coordinate must strongly influence each other's activities, and collectively appear anomalous from normal accounts. To address the challenges of optimizing the proposed model, we provide a bilevel optimization algorithm with theoretical guarantee on convergence. We verified the effectiveness of the proposed method and training algorithm on real-world social network data collected from Twitter related to coordinated campaigns from Russia's Internet Research Agency targeting the 2016 U.S. Presidential Elections, and to identify coordinated campaigns related to the COVID-19 pandemic. Leveraging the learned model, we find that the average influence between coordinated account pairs is the highest.On COVID-19, we found coordinated group spreading anti-vaccination, anti-masks conspiracies that suggest the pandemic is a hoax and political scam.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Sharma, Karishma and Zhang, Yizhou and Ferrara, Emilio and Liu, Yan},
	month = may,
	year = {2021},
	note = {arXiv:2008.11308 [cs]},
	keywords = {Computer Science - Social and Information Networks},
	annote = {Comment: KDD'2021 (Accepted)},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\LBZXGQPH\\Sharma et al. - 2021 - Identifying Coordinated Accounts on Social Media through Hidden Influence and Group Behaviours.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\FUL37M49\\2008.html:text/html},
}

@article{xu_simulating_2021,
	title = {{SIMULATING} {CONTINUOUS}-{TIME} {HUMAN} {MOBILITY} {TRAJECTORIES}},
	abstract = {Recent pandemic events have greatly emphasized the need to understand how humans navigate in modern day cities for effective public health policy implementation. In this paper, we propose a two-stage generative model, DeltaGAN, to simulate realistic human mobility trajectories. Compared with existing work where time was discretized, DeltaGAN generates continuous visitation time to better capture temporal irregularity in human mobility behaviors. Conditioned on the generated time, DeltaGAN synthesizes realistic trajectories by limiting the range of accessible location candidates. Experimental results demonstrate that our model achieves consistently better performance than baselines when comparing distribution similarities with real-world GPS trajectories via 6 individual trajectory and geographical metrics. We further validate the utility of DeltaGAN on COVID-19 spread simulation and observe the diffusion process under generated trajectories is consistent with that under real data.},
	language = {en},
	author = {Xu, Nan and Trinh, Loc and Rambhatla, Sirisha and Zeng, Zhen and Chen, Jiahao and Assefa, Samuel and Liu, Yan},
	year = {2021},
	file = {PDF:C\:\\Users\\selvam\\Zotero\\storage\\DF633KQX\\Xu et al. - 2021 - SIMULATING CONTINUOUS-TIME HUMAN MOBILITY TRAJECTORIES.pdf:application/pdf},
}

@misc{trinh_examination_2021,
	title = {An {Examination} of {Fairness} of {AI} {Models} for {Deepfake} {Detection}},
	url = {http://arxiv.org/abs/2105.00558},
	doi = {10.48550/arXiv.2105.00558},
	abstract = {Recent studies have demonstrated that deep learning models can discriminate based on protected classes like race and gender. In this work, we evaluate bias present in deepfake datasets and detection models across protected subgroups. Using facial datasets balanced by race and gender, we examine three popular deepfake detectors and find large disparities in predictive performances across races, with up to 10.7\% difference in error rate between subgroups. A closer look reveals that the widely used FaceForensics++ dataset is overwhelmingly composed of Caucasian subjects, with the majority being female Caucasians. Our investigation of the racial distribution of deepfakes reveals that the methods used to create deepfakes as positive training signals tend to produce "irregular" faces - when a person's face is swapped onto another person of a different race or gender. This causes detectors to learn spurious correlations between the foreground faces and fakeness. Moreover, when detectors are trained with the Blended Image (BI) dataset from Face X-Rays, we find that those detectors develop systematic discrimination towards certain racial subgroups, primarily female Asians.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Trinh, Loc and Liu, Yan},
	month = may,
	year = {2021},
	note = {arXiv:2105.00558 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in the 30th International Joint Conference on Artificial Intelligence (IJCAI-21)},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\UENI8KJD\\Trinh and Liu - 2021 - An Examination of Fairness of AI Models for Deepfake Detection.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\RDR2GGRN\\2105.html:text/html},
}

@article{seo_physics-aware_2020,
	title = {Physics-aware {Spatiotemporal} {Modules} with {Auxiliary} {Tasks} for {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=p65lWYKpqKz},
	abstract = {Modeling the dynamics of real-world physical systems is critical for spatiotemporal prediction tasks, but challenging when data is limited. The scarcity of real-world data and the difficulty in reproducing the data distribution hinder directly applying meta-learning techniques. Although the knowledge of governing partial differential equations (PDE) of the data can be helpful for the fast adaptation to few observations, it is mostly infeasible to exactly find the equation for observations in real-world physical systems. In this work, we propose a framework, physics-aware meta-learning with auxiliary tasks whose spatial modules incorporate PDE-independent knowledge and temporal modules utilize the generalized features from the spatial modules to be adapted to the limited data, respectively. The framework is inspired by a local conservation law expressed mathematically as a continuity equation and does not require the exact form of governing equation to model the spatiotemporal observations. The proposed method mitigates the need for a large number of real-world tasks for meta-learning by leveraging spatial information in simulated data to meta-initialize the spatial modules. We apply the proposed framework to both synthetic and real-world spatiotemporal prediction tasks and demonstrate its superior performance with limited observations.},
	language = {en},
	urldate = {2025-01-08},
	author = {Seo, Sungyong and Meng, Chuizheng and Rambhatla, Sirisha and Liu, Yan},
	month = oct,
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\AJHUJ8QA\\Seo et al. - 2020 - Physics-aware Spatiotemporal Modules with Auxiliary Tasks for Meta-Learning.pdf:application/pdf},
}

@misc{sharma_network_2020,
	title = {Network {Inference} from a {Mixture} of {Diffusion} {Models} for {Fake} {News} {Mitigation}},
	url = {http://arxiv.org/abs/2008.03450},
	doi = {10.48550/arXiv.2008.03450},
	abstract = {The dissemination of fake news intended to deceive people, influence public opinion and manipulate social outcomes, has become a pressing problem on social media. Moreover, information sharing on social media facilitates diffusion of viral information cascades. In this work, we focus on understanding and leveraging diffusion dynamics of false and legitimate contents in order to facilitate network interventions for fake news mitigation. We analyze real-world Twitter datasets comprising fake and true news cascades, to understand differences in diffusion dynamics and user behaviours with regards to fake and true contents. Based on the analysis, we model the diffusion as a mixture of Independent Cascade models (MIC) with parameters \${\textbackslash}theta\_T, {\textbackslash}theta\_F\$ over the social network graph; and derive unsupervised inference techniques for parameter estimation of the diffusion mixture model from observed, unlabeled cascades. Users influential in the propagation of true and fake contents are identified using the inferred diffusion dynamics. Characteristics of the identified influential users reveal positive correlation between influential users identified for fake news and their relative appearance in fake news cascades. Identified influential users tend to be related to topics of more viral information cascades than less viral ones; and identified fake news influential users have relatively fewer counts of direct followers, compared to the true news influential users. Intervention analysis on nodes and edges demonstrates capacity of the inferred diffusion dynamics in supporting network interventions for mitigation.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Sharma, Karishma and He, Xinran and Seo, Sungyong and Liu, Yan},
	month = aug,
	year = {2020},
	note = {arXiv:2008.03450 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\3WR92WBY\\Sharma et al. - 2020 - Network Inference from a Mixture of Diffusion Models for Fake News Mitigation.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\NJ6TE92I\\2008.html:text/html},
}


@misc{cao_spectral_2021,
	title = {Spectral {Temporal} {Graph} {Neural} {Network} for {Trajectory} {Prediction}},
	url = {http://arxiv.org/abs/2106.02930},
	doi = {10.48550/arXiv.2106.02930},
	abstract = {An effective understanding of the contextual environment and accurate motion forecasting of surrounding agents is crucial for the development of autonomous vehicles and social mobile robots. This task is challenging since the behavior of an autonomous agent is not only affected by its own intention, but also by the static environment and surrounding dynamically interacting agents. Previous works focused on utilizing the spatial and temporal information in time domain while not sufficiently taking advantage of the cues in frequency domain. To this end, we propose a Spectral Temporal Graph Neural Network (SpecTGNN), which can capture inter-agent correlations and temporal dependency simultaneously in frequency domain in addition to time domain. SpecTGNN operates on both an agent graph with dynamic state information and an environment graph with the features extracted from context images in two streams. The model integrates graph Fourier transform, spectral graph convolution and temporal gated convolution to encode history information and forecast future trajectories. Moreover, we incorporate a multi-head spatio-temporal attention mechanism to mitigate the effect of error propagation in a long time horizon. We demonstrate the performance of SpecTGNN on two public trajectory prediction benchmark datasets, which achieves state-of-the-art performance in terms of prediction accuracy.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Cao, Defu and Li, Jiachen and Ma, Hengbo and Tomizuka, Masayoshi},
	month = jun,
	year = {2021},
	note = {arXiv:2106.02930 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: ICRA 2021},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\LL9PREF8\\Cao et al. - 2021 - Spectral Temporal Graph Neural Network for Trajectory Prediction.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\GJ87N7S5\\2106.html:text/html},
}

@inproceedings{xu_treatment_2021,
	title = {Treatment {Recommendation} with {Preference}-based {Reinforcement} {Learning}},
	url = {https://ieeexplore.ieee.org/document/9667722},
	doi = {10.1109/ICKG52313.2021.00025},
	abstract = {Treatment recommendation is a complex multi-faceted problem with many treatment goals considered by clini-cians and patients, e.g., optimizing the survival rate, mitigating negative impacts, reducing financial expenses, avoiding over-treatment, etc. Recently, deep reinforcement learning (RL) approaches have gained popularity for treatment recommendation. In this paper, we investigate preference-based reinforcement learning approaches for treatment recommendation, where the reward function is itself learned based on treatment goals, without requiring either expert demonstrations in advance or human involvement during policy learning. We first present an open sim-ulation platform11https://sites.google.com/view/tr-with-prl/ to model the evolution of two diseases, namely Cancer and Sepsis, and individuals' reactions to the received treatment. Secondly, we systematically examine preference-based RL for treatment recommendation via simulated experiments and observe high utility in the learned policy in terms of high survival rate and low side effects, with inferred rewards highly correlated to treatment goals. We further explore the transferability of inferred reward functions and guidelines for agent design to provide insights in achieving the right trade-off among various human objectives with preference-based RL approaches for treatment recommendation in the real world.},
	urldate = {2025-01-08},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Knowledge} ({ICBK})},
	author = {Xu, Nan and Kamra, Nitin and Liu, Yan},
	month = dec,
	year = {2021},
	keywords = {Cancer, Conferences, Correlation, Guidelines, Healthcare, Preference Learning, Reinforcement learning, Reinforcement Learning, Trajectory, Treatment Recommendation},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\\QUYYF5DK\\9667722.html:text/html},
}

@misc{zhang_vigdet_2021,
	title = {{VigDet}: {Knowledge} {Informed} {Neural} {Temporal} {Point} {Process} for {Coordination} {Detection} on {Social} {Media}},
	shorttitle = {{VigDet}},
	url = {http://arxiv.org/abs/2110.15454},
	doi = {10.48550/arXiv.2110.15454},
	abstract = {Recent years have witnessed an increasing use of coordinated accounts on social media, operated by misinformation campaigns to influence public opinion and manipulate social outcomes. Consequently, there is an urgent need to develop an effective methodology for coordinated group detection to combat the misinformation on social media. However, existing works suffer from various drawbacks, such as, either limited performance due to extreme reliance on predefined signatures of coordination, or instead an inability to address the natural sparsity of account activities on social media with useful prior domain knowledge. Therefore, in this paper, we propose a coordination detection framework incorporating neural temporal point process with prior knowledge such as temporal logic or pre-defined filtering functions. Specifically, when modeling the observed data from social media with neural temporal point process, we jointly learn a Gibbs-like distribution of group assignment based on how consistent an assignment is to (1) the account embedding space and (2) the prior knowledge. To address the challenge that the distribution is hard to be efficiently computed and sampled from, we design a theoretically guaranteed variational inference approach to learn a mean-field approximation for it. Experimental results on a real-world dataset show the effectiveness of our proposed method compared to the SOTA model in both unsupervised and semi-supervised settings. We further apply our model on a COVID-19 Vaccine Tweets dataset. The detection result suggests the presence of suspicious coordinated efforts on spreading misinformation about COVID-19 vaccines.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Zhang, Yizhou and Sharma, Karishma and Liu, Yan},
	month = oct,
	year = {2021},
	note = {arXiv:2110.15454 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	annote = {Comment: Accepted by NeurIPS 2021. 17 pages, 5 figures},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\TZPC6IEP\\Zhang et al. - 2021 - VigDet Knowledge Informed Neural Temporal Point Process for Coordination Detection on Social Media.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\2APNQ5V4\\2110.html:text/html},
}

@article{hung_road_2022,
	title = {Road to automating robotic suturing skills assessment: {Battling} mislabeling of the ground truth},
	volume = {171},
	issn = {1532-7361},
	shorttitle = {Road to automating robotic suturing skills assessment},
	doi = {10.1016/j.surg.2021.08.014},
	abstract = {OBJECTIVE: To automate surgeon skills evaluation using robotic instrument kinematic data. Additionally, to implement an unsupervised mislabeling detection algorithm to identify potentially mislabeled samples that can be removed to improve model performance.
METHODS: Video recordings and instrument kinematic data were derived from suturing exercises completed on the Mimic FlexVR robotic simulator. A structured human consensus-building process was developed to determine Robotic Anastomosis Competency Evaluation technical scores across 3 human graders. A 2-layer long short-term memory-based classification model used instrument kinematic data to automate suturing skills assessment. An unsupervised label analyzer (NoiseRank) was used to identify potential mislabeling of skills data. Performance of the long short-term memory model's technical skill score prediction was measured by best area under the curve over the training runs. NoiseRank outputted a ranked list of rated skills assessments based on likelihood of mislabeling.
RESULTS: 22 surgeons performed 226 suturing attempts, which were broken down into 1,404 individual skill assessment points. Automation of needle entry angle, needle driving, and needle withdrawal technical skill scores performed better (area under the curve 0.698-0.705) than needle positioning (0.532) at baseline using all available data. Potential mislabels were subsequently identified by NoiseRank and removed, improving model performance across all domains (area under the curve 0.551-0.766).
CONCLUSION: Using ground truth labels from human graders and robotic instrument kinematic data, machine learning models have automated assessment of detailed suturing technical skills with good performance. Further, an unsupervised mislabeling detection algorithm projected mislabeled data, allowing for their removal and subsequent improvement of model performance.},
	language = {eng},
	number = {4},
	journal = {Surgery},
	author = {Hung, Andrew J. and Rambhatla, Sirisha and Sanford, Daniel I. and Pachauri, Nilay and Vanstrum, Erik and Nguyen, Jessica H. and Liu, Yan},
	month = apr,
	year = {2022},
	pmid = {34538647},
	pmcid = {PMC8924023},
	keywords = {Clinical Competence, Humans, Robotic Surgical Procedures, Robotics, Surgeons, Sutures},
	pages = {915--919},
	file = {PubMed Central Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\6Z88K9HQ\\Hung et al. - 2022 - Road to automating robotic suturing skills assessment Battling mislabeling of the ground truth.pdf:application/pdf},
}

@misc{rambhatla_towards_2020,
	title = {Towards {Accurate} {Spatiotemporal} {COVID}-19 {Risk} {Scores} using {High} {Resolution} {Real}-{World} {Mobility} {Data}},
	url = {http://arxiv.org/abs/2012.07283},
	doi = {10.48550/arXiv.2012.07283},
	abstract = {As countries look towards re-opening of economic activities amidst the ongoing COVID-19 pandemic, ensuring public health has been challenging. While contact tracing only aims to track past activities of infected users, one path to safe reopening is to develop reliable spatiotemporal risk scores to indicate the propensity of the disease. Existing works which aim to develop risk scores either rely on compartmental model-based reproduction numbers (which assume uniform population mixing) or develop coarse-grain spatial scores based on reproduction number (R0) and macro-level density-based mobility statistics. Instead, in this paper, we develop a Hawkes process-based technique to assign relatively fine-grain spatial and temporal risk scores by leveraging high-resolution mobility data based on cell-phone originated location signals. While COVID-19 risk scores also depend on a number of factors specific to an individual, including demography and existing medical conditions, the primary mode of disease transmission is via physical proximity and contact. Therefore, we focus on developing risk scores based on location density and mobility behaviour. We demonstrate the efficacy of the developed risk scores via simulation based on real-world mobility data. Our results show that fine-grain spatiotemporal risk scores based on high-resolution mobility data can provide useful insights and facilitate safe re-opening.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Rambhatla, Sirisha and Zeighami, Sepanta and Shahabi, Kameron and Shahabi, Cyrus and Liu, Yan},
	month = dec,
	year = {2020},
	note = {arXiv:2012.07283 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\IZ67DVKA\\Rambhatla et al. - 2020 - Towards Accurate Spatiotemporal COVID-19 Risk Scores using High Resolution Real-World Mobility Data.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\S6TA6C9X\\2012.html:text/html},
}

@article{hung_pd58-08_2021,
	title = {{PD58}-08 {AUTOMATING} {SUTURING} {SKILLS} {ASSESSMENT} {WITH} {A} {LIMITED} {SURGEON} {DATASET}: {META} {LEARNING}},
	copyright = {© 2021 by American Urological Association Education and Research, Inc.},
	shorttitle = {{PD58}-08 {AUTOMATING} {SUTURING} {SKILLS} {ASSESSMENT} {WITH} {A} {LIMITED} {SURGEON} {DATASET}},
	url = {https://www.auajournals.org/doi/10.1097/JU.0000000000002092.08},
	doi = {10.1097/JU.0000000000002092.08},
	abstract = {INTRODUCTION AND OBJECTIVE:Automating technical skills assessment with machine learning models can revolutionize
how we credential surgeons, and it can identify critical errors before they become
patient complications. Common to many machine learning ...},
	language = {EN},
	urldate = {2025-01-08},
	journal = {The Journal of Urology},
	author = {Hung, Andrew J. and Rambhatla, Sirisha and Sanford, Daniel I. and Pachauri, Nilay and Nguyen, Jessica H. and Liu, Yan},
	month = sep,
	year = {2021},
	note = {Publisher: Wolters KluwerPhiladelphia, PA},
	file = {Full Text:C\:\\Users\\selvam\\Zotero\\storage\\THGD93PB\\Hung et al. - 2021 - PD58-08 AUTOMATING SUTURING SKILLS ASSESSMENT WITH A LIMITED SURGEON DATASET META LEARNING.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\3Y2GCSRX\\JU.0000000000002092.html:text/html},
}

@article{kamra_polsird_2021,
	title = {{PolSIRD}: {Modeling} {Epidemic} {Spread} {Under} {Intervention} {Policies}: {Analyzing} the {First} {Wave} of {COVID}-19 in the {USA}},
	volume = {5},
	issn = {2509-4971},
	shorttitle = {{PolSIRD}},
	doi = {10.1007/s41666-021-00099-3},
	abstract = {Epidemic spread in a population is traditionally modeled via compartmentalized models which represent the free evolution of disease in the absence of any intervention policies. In addition, these models assume full observability of disease cases and do not account for under-reporting. We present a mathematical model, namely PolSIRD, which accounts for the under-reporting by introducing an observation mechanism. It also captures the effects of intervention policies on the disease spread parameters by leveraging intervention policy data along with the reported disease cases. Furthermore, we allow our recurrent model to learn the initial hidden state of all compartments end-to-end along with other parameters via gradient-based training. We apply our model to the spread of the recent global outbreak of COVID-19 in the USA, where our model outperforms the methods employed by the CDC in predicting the spread. We also provide counterfactual simulations from our model to analyze the effect of lifting the intervention policies prematurely and our model correctly predicts the second wave of the epidemic.},
	language = {eng},
	number = {3},
	journal = {Journal of Healthcare Informatics Research},
	author = {Kamra, Nitin and Zhang, Yizhou and Rambhatla, Sirisha and Meng, Chuizheng and Liu, Yan},
	month = sep,
	year = {2021},
	pmid = {34151134},
	pmcid = {PMC8202228},
	keywords = {COVID-19, Epidemic spread modeling, Intervention policies for epidemics, Machine learning for epidemic spread modeling, Spatiotemporal spread modeling},
	pages = {231--248},
	file = {Full Text:C\:\\Users\\selvam\\Zotero\\storage\\XUTIJYNX\\Kamra et al. - 2021 - PolSIRD Modeling Epidemic Spread Under Intervention Policies Analyzing the First Wave of COVID-19.pdf:application/pdf},
}

@inproceedings{kamra_gradient-based_2021,
	title = {Gradient-based optimization for multi-resource spatial coverage problems},
	url = {https://proceedings.mlr.press/v161/kamra21a.html},
	abstract = {Resource allocation for coverage of geographical spaces is a challenging problem in robotics, sensor networks and security domains. Conventional solution approaches either: (a) rely on exploiting spatio-temporal structure of specific coverage problems, or (b) use genetic algorithms when targeting general coverage problems where no special exploitable structure exists. In this work, we propose the coverage gradient theorem, which provides a gradient estimator for a broad class of spatial coverage objectives using a combination of Newton-Leibniz theorem and implicit boundary differentiation. We also propose a tractable framework to approximate the coverage objectives and their gradients using spatial discretization and empirically demonstrate the efficacy of our framework on multi-resource spatial coverage problems.},
	language = {en},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Kamra, Nitin and Liu, Yan},
	month = dec,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1885--1894},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\7RXVV8R7\\Kamra and Liu - 2021 - Gradient-based optimization for multi-resource spatial coverage problems.pdf:application/pdf;Supplementary PDF:C\:\\Users\\selvam\\Zotero\\storage\\5GB7IYDE\\Kamra and Liu - 2021 - Gradient-based optimization for multi-resource spatial coverage problems.pdf:application/pdf},
}

@misc{meng_cross-node_2021,
	title = {Cross-{Node} {Federated} {Graph} {Neural} {Network} for {Spatio}-{Temporal} {Data} {Modeling}},
	url = {http://arxiv.org/abs/2106.05223},
	doi = {10.48550/arXiv.2106.05223},
	abstract = {Vast amount of data generated from networks of sensors, wearables, and the Internet of Things (IoT) devices underscores the need for advanced modeling techniques that leverage the spatio-temporal structure of decentralized data due to the need for edge computation and licensing (data access) issues. While federated learning (FL) has emerged as a framework for model training without requiring direct data sharing and exchange, effectively modeling the complex spatio-temporal dependencies to improve forecasting capabilities still remains an open problem. On the other hand, state-of-the-art spatio-temporal forecasting models assume unfettered access to the data, neglecting constraints on data sharing. To bridge this gap, we propose a federated spatio-temporal model -- Cross-Node Federated Graph Neural Network (CNFGNN) -- which explicitly encodes the underlying graph structure using graph neural network (GNN)-based architecture under the constraint of cross-node federated learning, which requires that data in a network of nodes is generated locally on each node and remains decentralized. CNFGNN operates by disentangling the temporal dynamics modeling on devices and spatial dynamics on the server, utilizing alternating optimization to reduce the communication cost, facilitating computations on the edge devices. Experiments on the traffic flow forecasting task show that CNFGNN achieves the best forecasting performance in both transductive and inductive learning settings with no extra computation cost on edge devices, while incurring modest communication cost.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Meng, Chuizheng and Rambhatla, Sirisha and Liu, Yan},
	month = jun,
	year = {2021},
	note = {arXiv:2106.05223 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: To be published in the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 21)},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\YPFV79MN\\Meng et al. - 2021 - Cross-Node Federated Graph Neural Network for Spatio-Temporal Data Modeling.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\ANJKC8AD\\2106.html:text/html},
}

@misc{sharma_identifying_2021,
	title = {Identifying {Coordinated} {Accounts} on {Social} {Media} through {Hidden} {Influence} and {Group} {Behaviours}},
	url = {http://arxiv.org/abs/2008.11308},
	doi = {10.48550/arXiv.2008.11308},
	abstract = {Disinformation campaigns on social media, involving coordinated activities from malicious accounts towards manipulating public opinion, have become increasingly prevalent. Existing approaches to detect coordinated accounts either make very strict assumptions about coordinated behaviours, or require part of the malicious accounts in the coordinated group to be revealed in order to detect the rest. To address these drawbacks, we propose a generative model, AMDN-HAGE (Attentive Mixture Density Network with Hidden Account Group Estimation) which jointly models account activities and hidden group behaviours based on Temporal Point Processes (TPP) and Gaussian Mixture Model (GMM), to capture inherent characteristics of coordination which is, accounts that coordinate must strongly influence each other's activities, and collectively appear anomalous from normal accounts. To address the challenges of optimizing the proposed model, we provide a bilevel optimization algorithm with theoretical guarantee on convergence. We verified the effectiveness of the proposed method and training algorithm on real-world social network data collected from Twitter related to coordinated campaigns from Russia's Internet Research Agency targeting the 2016 U.S. Presidential Elections, and to identify coordinated campaigns related to the COVID-19 pandemic. Leveraging the learned model, we find that the average influence between coordinated account pairs is the highest.On COVID-19, we found coordinated group spreading anti-vaccination, anti-masks conspiracies that suggest the pandemic is a hoax and political scam.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Sharma, Karishma and Zhang, Yizhou and Ferrara, Emilio and Liu, Yan},
	month = may,
	year = {2021},
	note = {arXiv:2008.11308 [cs]},
	keywords = {Computer Science - Social and Information Networks},
	annote = {Comment: KDD'2021 (Accepted)},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\LBZXGQPH\\Sharma et al. - 2021 - Identifying Coordinated Accounts on Social Media through Hidden Influence and Group Behaviours.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\FUL37M49\\2008.html:text/html},
}

@article{xu_simulating_2021,
	title = {{SIMULATING} {CONTINUOUS}-{TIME} {HUMAN} {MOBILITY} {TRAJECTORIES}},
	abstract = {Recent pandemic events have greatly emphasized the need to understand how humans navigate in modern day cities for effective public health policy implementation. In this paper, we propose a two-stage generative model, DeltaGAN, to simulate realistic human mobility trajectories. Compared with existing work where time was discretized, DeltaGAN generates continuous visitation time to better capture temporal irregularity in human mobility behaviors. Conditioned on the generated time, DeltaGAN synthesizes realistic trajectories by limiting the range of accessible location candidates. Experimental results demonstrate that our model achieves consistently better performance than baselines when comparing distribution similarities with real-world GPS trajectories via 6 individual trajectory and geographical metrics. We further validate the utility of DeltaGAN on COVID-19 spread simulation and observe the diffusion process under generated trajectories is consistent with that under real data.},
	language = {en},
	author = {Xu, Nan and Trinh, Loc and Rambhatla, Sirisha and Zeng, Zhen and Chen, Jiahao and Assefa, Samuel and Liu, Yan},
	year = {2021},
	file = {PDF:C\:\\Users\\selvam\\Zotero\\storage\\DF633KQX\\Xu et al. - 2021 - SIMULATING CONTINUOUS-TIME HUMAN MOBILITY TRAJECTORIES.pdf:application/pdf},
}

@misc{trinh_examination_2021,
	title = {An {Examination} of {Fairness} of {AI} {Models} for {Deepfake} {Detection}},
	url = {http://arxiv.org/abs/2105.00558},
	doi = {10.48550/arXiv.2105.00558},
	abstract = {Recent studies have demonstrated that deep learning models can discriminate based on protected classes like race and gender. In this work, we evaluate bias present in deepfake datasets and detection models across protected subgroups. Using facial datasets balanced by race and gender, we examine three popular deepfake detectors and find large disparities in predictive performances across races, with up to 10.7\% difference in error rate between subgroups. A closer look reveals that the widely used FaceForensics++ dataset is overwhelmingly composed of Caucasian subjects, with the majority being female Caucasians. Our investigation of the racial distribution of deepfakes reveals that the methods used to create deepfakes as positive training signals tend to produce "irregular" faces - when a person's face is swapped onto another person of a different race or gender. This causes detectors to learn spurious correlations between the foreground faces and fakeness. Moreover, when detectors are trained with the Blended Image (BI) dataset from Face X-Rays, we find that those detectors develop systematic discrimination towards certain racial subgroups, primarily female Asians.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Trinh, Loc and Liu, Yan},
	month = may,
	year = {2021},
	note = {arXiv:2105.00558 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in the 30th International Joint Conference on Artificial Intelligence (IJCAI-21)},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\UENI8KJD\\Trinh and Liu - 2021 - An Examination of Fairness of AI Models for Deepfake Detection.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\RDR2GGRN\\2105.html:text/html},
}

@article{seo_physics-aware_2020,
	title = {Physics-aware {Spatiotemporal} {Modules} with {Auxiliary} {Tasks} for {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=p65lWYKpqKz},
	abstract = {Modeling the dynamics of real-world physical systems is critical for spatiotemporal prediction tasks, but challenging when data is limited. The scarcity of real-world data and the difficulty in reproducing the data distribution hinder directly applying meta-learning techniques. Although the knowledge of governing partial differential equations (PDE) of the data can be helpful for the fast adaptation to few observations, it is mostly infeasible to exactly find the equation for observations in real-world physical systems. In this work, we propose a framework, physics-aware meta-learning with auxiliary tasks whose spatial modules incorporate PDE-independent knowledge and temporal modules utilize the generalized features from the spatial modules to be adapted to the limited data, respectively. The framework is inspired by a local conservation law expressed mathematically as a continuity equation and does not require the exact form of governing equation to model the spatiotemporal observations. The proposed method mitigates the need for a large number of real-world tasks for meta-learning by leveraging spatial information in simulated data to meta-initialize the spatial modules. We apply the proposed framework to both synthetic and real-world spatiotemporal prediction tasks and demonstrate its superior performance with limited observations.},
	language = {en},
	urldate = {2025-01-08},
	author = {Seo, Sungyong and Meng, Chuizheng and Rambhatla, Sirisha and Liu, Yan},
	month = oct,
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\AJHUJ8QA\\Seo et al. - 2020 - Physics-aware Spatiotemporal Modules with Auxiliary Tasks for Meta-Learning.pdf:application/pdf},
}

@misc{sharma_network_2020,
	title = {Network {Inference} from a {Mixture} of {Diffusion} {Models} for {Fake} {News} {Mitigation}},
	url = {http://arxiv.org/abs/2008.03450},
	doi = {10.48550/arXiv.2008.03450},
	abstract = {The dissemination of fake news intended to deceive people, influence public opinion and manipulate social outcomes, has become a pressing problem on social media. Moreover, information sharing on social media facilitates diffusion of viral information cascades. In this work, we focus on understanding and leveraging diffusion dynamics of false and legitimate contents in order to facilitate network interventions for fake news mitigation. We analyze real-world Twitter datasets comprising fake and true news cascades, to understand differences in diffusion dynamics and user behaviours with regards to fake and true contents. Based on the analysis, we model the diffusion as a mixture of Independent Cascade models (MIC) with parameters \${\textbackslash}theta\_T, {\textbackslash}theta\_F\$ over the social network graph; and derive unsupervised inference techniques for parameter estimation of the diffusion mixture model from observed, unlabeled cascades. Users influential in the propagation of true and fake contents are identified using the inferred diffusion dynamics. Characteristics of the identified influential users reveal positive correlation between influential users identified for fake news and their relative appearance in fake news cascades. Identified influential users tend to be related to topics of more viral information cascades than less viral ones; and identified fake news influential users have relatively fewer counts of direct followers, compared to the true news influential users. Intervention analysis on nodes and edges demonstrates capacity of the inferred diffusion dynamics in supporting network interventions for mitigation.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Sharma, Karishma and He, Xinran and Seo, Sungyong and Liu, Yan},
	month = aug,
	year = {2020},
	note = {arXiv:2008.03450 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\3WR92WBY\\Sharma et al. - 2020 - Network Inference from a Mixture of Diffusion Models for Fake News Mitigation.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\NJ6TE92I\\2008.html:text/html},
}


@misc{kamra_multi-agent_2020,
	title = {Multi-agent {Trajectory} {Prediction} with {Fuzzy} {Query} {Attention}},
	url = {http://arxiv.org/abs/2010.15891},
	doi = {10.48550/arXiv.2010.15891},
	abstract = {Trajectory prediction for scenes with multiple agents and entities is a challenging problem in numerous domains such as traffic prediction, pedestrian tracking and path planning. We present a general architecture to address this challenge which models the crucial inductive biases of motion, namely, inertia, relative motion, intents and interactions. Specifically, we propose a relational model to flexibly model interactions between agents in diverse environments. Since it is well-known that human decision making is fuzzy by nature, at the core of our model lies a novel attention mechanism which models interactions by making continuous-valued (fuzzy) decisions and learning the corresponding responses. Our architecture demonstrates significant performance gains over existing state-of-the-art predictive models in diverse domains such as human crowd trajectories, US freeway traffic, NBA sports data and physics datasets. We also present ablations and augmentations to understand the decision-making process and the source of gains in our model.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Kamra, Nitin and Zhu, Hao and Trivedi, Dweep and Zhang, Ming and Liu, Yan},
	month = oct,
	year = {2020},
	note = {arXiv:2010.15891 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2020 Camera-ready version. Code: https://github.com/nitinkamra1992/FQA},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\YTLBQXLZ\\Kamra et al. - 2020 - Multi-agent Trajectory Prediction with Fuzzy Query Attention.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\AITEKB9D\\2010.html:text/html},
}

@inproceedings{seo_physics-aware_2019,
	title = {Physics-aware {Difference} {Graph} {Networks} for {Sparsely}-{Observed} {Dynamics}},
	url = {https://openreview.net/forum?id=r1gelyrtwH},
	abstract = {Sparsely available data points cause numerical error on finite differences which hinders us from modeling the dynamics of physical systems. The discretization error becomes even larger when the sparse data are irregularly distributed or defined on an unstructured grid, making it hard to build deep learning models to handle physics-governing observations on the unstructured grid. In this paper, we propose a novel architecture, Physics-aware Difference Graph Networks (PA-DGN), which exploits neighboring information to learn finite differences inspired by physics equations. PA-DGN leverages data-driven end-to-end learning to discover underlying dynamical relations between the spatial and temporal differences in given sequential observations. We demonstrate the superiority of PA-DGN in the approximation of directional derivatives and the prediction of graph signals on the synthetic data and the real-world climate observations from weather stations.},
	language = {en},
	urldate = {2025-01-08},
	author = {Seo*, Sungyong and Meng*, Chuizheng and Liu, Yan},
	month = sep,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\PFUAY4NT\\Seo et al. - 2019 - Physics-aware Difference Graph Networks for Sparsely-Observed Dynamics.pdf:application/pdf},
}

@article{li_generative_2020,
	title = {Generative {Attention} {Networks} for {Multi}-{Agent} {Behavioral} {Modeling}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6209},
	doi = {10.1609/aaai.v34i05.6209},
	abstract = {Understanding and modeling behavior of multi-agent systems is a central step for artificial intelligence. Here we present a deep generative model which captures behavior generating process of multi-agent systems, supports accurate predictions and inference, infers how agents interact in a complex system, as well as identifies agent groups and interaction types. Built upon advances in deep generative models and a novel attention mechanism, our model can learn interactions in highly heterogeneous systems with linear complexity in the number of agents. We apply this model to three multi-agent systems in different domains and evaluate performance on a diverse set of tasks including behavior prediction, interaction analysis and system identification. Experimental results demonstrate its ability to model multi-agent systems, yielding improved performance over competitive baselines. We also show the model can successfully identify agent groups and interaction types in these systems. Our model offers new opportunities to predict complex multi-agent behaviors and takes a step forward in understanding interactions in multi-agent systems.},
	language = {en},
	number = {05},
	urldate = {2025-01-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Guangyu and Jiang, Bo and Zhu, Hao and Che, Zhengping and Liu, Yan},
	month = apr,
	year = {2020},
	note = {Number: 05},
	pages = {7195--7202},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\KSD4MXIF\\Li et al. - 2020 - Generative Attention Networks for Multi-Agent Behavioral Modeling.pdf:application/pdf},
}

@misc{tsang_how_2020,
	title = {How does this interaction affect me? {Interpretable} attribution for feature interactions},
	shorttitle = {How does this interaction affect me?},
	url = {http://arxiv.org/abs/2006.10965},
	doi = {10.48550/arXiv.2006.10965},
	abstract = {Machine learning transparency calls for interpretable explanations of how inputs relate to predictions. Feature attribution is a way to analyze the impact of features on predictions. Feature interactions are the contextual dependence between features that jointly impact predictions. There are a number of methods that extract feature interactions in prediction models; however, the methods that assign attributions to interactions are either uninterpretable, model-specific, or non-axiomatic. We propose an interaction attribution and detection framework called Archipelago which addresses these problems and is also scalable in real-world settings. Our experiments on standard annotation labels indicate our approach provides significantly more interpretable explanations than comparable methods, which is important for analyzing the impact of interactions on predictions. We also provide accompanying visualizations of our approach that give new insights into deep neural networks.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Tsang, Michael and Rambhatla, Sirisha and Liu, Yan},
	month = jun,
	year = {2020},
	note = {arXiv:2006.10965 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\HKWFPKVB\\Tsang et al. - 2020 - How does this interaction affect me Interpretable attribution for feature interactions.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\VMN2M3TW\\2006.html:text/html},
}

@inproceedings{tsang_feature_2019,
	title = {Feature {Interaction} {Interpretability}: {A} {Case} for {Explaining} {Ad}-{Recommendation} {Systems} via {Neural} {Interaction} {Detection}},
	shorttitle = {Feature {Interaction} {Interpretability}},
	url = {https://openreview.net/forum?id=BkgnhTEtDS},
	abstract = {Recommendation is a prevalent application of machine learning that affects many users; therefore, it is important for recommender models to be accurate and interpretable. In this work, we propose a method to both interpret and augment the predictions of black-box recommender systems. In particular, we propose to interpret feature interactions from a source recommender model and explicitly encode these interactions in a target recommender model, where both source and target models are black-boxes. By not assuming the structure of the recommender system, our approach can be used in general settings. In our experiments, we focus on a prominent use of machine learning recommendation: ad-click prediction. We found that our interaction interpretations are both informative and predictive, e.g., significantly outperforming existing recommender models. What's more, the same approach to interpret interactions can provide new insights into domains even beyond recommendation, such as text and image classification.},
	language = {en},
	urldate = {2025-01-08},
	author = {Tsang, Michael and Cheng, Dehua and Liu, Hanpeng and Feng, Xue and Zhou, Eric and Liu, Yan},
	month = sep,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\W2IY9A8M\\Tsang et al. - 2019 - Feature Interaction Interpretability A Case for Explaining Ad-Recommendation Systems via Neural Int.pdf:application/pdf},
}

@inproceedings{shi_predicting_2020,
	title = {Predicting {Origin}-{Destination} {Flow} via {Multi}-{Perspective} {Graph} {Convolutional} {Network}},
	url = {https://ieeexplore.ieee.org/document/9101359/authors#authors},
	doi = {10.1109/ICDE48307.2020.00178},
	abstract = {Predicting Origin-Destination (OD) flow is a crucial problem for intelligent transportation. However, it is extremely challenging because of three reasons: first, correlations exist between both origins and destinations; second, the correlations are dynamic across the time; at last, there are multiple correlations from different aspects. To the best of our knowledge, existing models for OD flow prediction cannot tackle all of these three issues simultaneously. We propose Multi-Perspective Graph Convolutional Networks (MPGCN) to capture the complex dependencies. Our proposed model first utilizes long short-term memory (LSTM) network to extract temporal features for each OD pair and then learns the spatial dependency of origins and destinations by a two-dimensional graph convolutional network. Furthermore, we design a dynamic graph together with two static graphs to capture the complicated spatial dependencies and use an average strategy to obtain the final predicted OD flow. We conduct extensive experiments on two large-scale and real-world datasets, which not only demonstrate our design philosophy but also validate the effectiveness of the proposed model.},
	urldate = {2025-01-08},
	booktitle = {2020 {IEEE} 36th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Shi, Hongzhi and Yao, Quanming and Guo, Qi and Li, Yaguang and Zhang, Lingyu and Ye, Jieping and Li, Yong and Liu, Yan},
	month = apr,
	year = {2020},
	note = {ISSN: 2375-026X},
	keywords = {Convolution, Correlation, Feature extraction, graph convolutional network, Origin-destination traffic prediction, Predictive models, spatial-temporal data analysis, Tensile stress, Transportation, Two dimensional displays},
	pages = {1818--1821},
}

@misc{sharma_noiserank_2020,
	title = {{NoiseRank}: {Unsupervised} {Label} {Noise} {Reduction} with {Dependence} {Models}},
	shorttitle = {{NoiseRank}},
	url = {http://arxiv.org/abs/2003.06729},
	doi = {10.48550/arXiv.2003.06729},
	abstract = {Label noise is increasingly prevalent in datasets acquired from noisy channels. Existing approaches that detect and remove label noise generally rely on some form of supervision, which is not scalable and error-prone. In this paper, we propose NoiseRank, for unsupervised label noise reduction using Markov Random Fields (MRF). We construct a dependence model to estimate the posterior probability of an instance being incorrectly labeled given the dataset, and rank instances based on their estimated probabilities. Our method 1) Does not require supervision from ground-truth labels, or priors on label or noise distribution. 2) It is interpretable by design, enabling transparency in label noise removal. 3) It is agnostic to classifier architecture/optimization framework and content modality. These advantages enable wide applicability in real noise settings, unlike prior works constrained by one or more conditions. NoiseRank improves state-of-the-art classification on Food101-N ({\textasciitilde}20\% noise), and is effective on high noise Clothing-1M ({\textasciitilde}40\% noise).},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Sharma, Karishma and Donmez, Pinar and Luo, Enming and Liu, Yan and Yalniz, I. Zeki},
	month = mar,
	year = {2020},
	note = {arXiv:2003.06729 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\VQ7AGT57\\Sharma et al. - 2020 - NoiseRank Unsupervised Label Noise Reduction with Dependence Models.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\FBKMQPWD\\2003.html:text/html},
}

@inproceedings{liu_costco_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {{CoSTCo}: {A} {Neural} {Tensor} {Completion} {Model} for {Sparse} {Tensors}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {{CoSTCo}},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330881},
	doi = {10.1145/3292500.3330881},
	abstract = {Low-rank tensor factorization has been widely used for many real world tensor completion problems. While most existing factorization models assume a multilinearity relationship between tensor entries and their corresponding factors, real world tensors tend to have more complex interactions than multilinearity. In many recent works, it is observed that multilinear models perform worse than nonlinear models. We identify one potential reason for this inferior performance: the nonlinearity inside data obfuscates the underlying low-rank structure such that the tensor seems to be a high-rank tensor. Solving this problem requires a model to simultaneously capture the complex interactions and preserve the low-rank structure. In addition, the model should be scalable and robust to missing observations in order to learn from large yet sparse real world tensors. We propose a novel convolutional neural network (CNN) based model, named CoSTCo (Convolutional Sparse Tensor Completion). Our model leverages the expressive power of CNN to model the complex interactions inside tensors and its parameter sharing scheme to preserve the desired low-rank structure. CoSTCo is scalable as it does not involve computation- or memory- heavy tasks such as Kronecker product. We conduct extensive experiments on several real world large sparse tensors and the experimental results show that our model clearly outperforms both linear and nonlinear state-of-the-art tensor completion methods.},
	urldate = {2025-01-07},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Hanpeng and Li, Yaguang and Tsang, Michael and Liu, Yan},
	month = jul,
	year = {2019},
	pages = {324--334},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\9M388JA5\\Liu et al. - 2019 - CoSTCo A Neural Tensor Completion Model for Sparse Tensors.pdf:application/pdf},
}

@article{chen_current_2019,
	title = {Current status of artificial intelligence applications in urology and their potential to influence clinical practice},
	volume = {124},
	issn = {1464-410X},
	doi = {10.1111/bju.14852},
	abstract = {OBJECTIVE: To investigate the applications of artificial intelligence (AI) in diagnosis, treatment and outcome predictionin urologic diseases and evaluate its advantages over traditional models and methods.
MATERIALS AND METHODS: A literature search was performed after PROSPERO registration (CRD42018103701) and in compliance with Preferred Reported Items for Systematic Reviews and Meta-Analyses (PRISMA) methods. Articles between 1994 and 2018 using the search terms "urology", "artificial intelligence", "machine learning" were included and categorized by the application of AI in urology. Review articles, editorial comments, articles with no full-text access, and nonurologic studies were excluded.
RESULTS: Initial search yielded 231 articles, but after excluding duplicates and following full-text review and examination of article references, only 111 articles were included in the final analysis. AI applications in urology include: utilizing radiomic imaging or ultrasonic echo data to improve or automate cancer detection or outcome prediction, utilizing digitized tissue specimen images to automate detection of cancer on pathology slides, and combining patient clinical data, biomarkers, or gene expression to assist disease diagnosis or outcome prediction. Some studies employed AI to plan brachytherapy and radiation treatments while others used video based or robotic automated performance metrics to objectively evaluate surgical skill. Compared to conventional statistical analysis, 71.8\% of studies concluded that AI is superior in diagnosis and outcome prediction.
CONCLUSION: AI has been widely adopted in urology. Compared to conventional statistics AI approaches are more accurate in prediction and more explorative for analyzing large data cohorts. With an increasing library of patient data accessible to clinicians, AI may help facilitate evidence-based and individualized patient care.},
	language = {eng},
	number = {4},
	journal = {BJU international},
	author = {Chen, Jian and Remulla, Daphne and Nguyen, Jessica H. and Dua, Aastha and Liu, Yan and Dasgupta, Prokar and Hung, Andrew J.},
	month = oct,
	year = {2019},
	pmid = {31219658},
	keywords = {artificial intelligence, decision support techniques, diagnosis, prediction, urology},
	pages = {567--577},
}

@inproceedings{kamra_deepfp_2019,
	address = {Berlin, Heidelberg},
	title = {{DeepFP} for {Finding} {Nash} {Equilibrium} in {Continuous} {Action} {Spaces}},
	isbn = {978-3-030-32429-2},
	url = {https://doi.org/10.1007/978-3-030-32430-8_15},
	doi = {10.1007/978-3-030-32430-8_15},
	abstract = {Finding Nash equilibrium in continuous action spaces is a challenging problem and has applications in domains such as protecting geographic areas from potential attackers. We present DeepFP, an approximate extension of fictitious play in continuous action spaces. DeepFP represents players’ approximate best responses via generative neural networks which are highly expressive implicit density approximators. It additionally uses a game-model network which approximates the players’ expected payoffs given their actions, and trains the networks end-to-end in a model-based learning regime. Further, DeepFP allows using domain-specific oracles if available and can hence exploit techniques such as mathematical programming to compute best responses for structured games. We demonstrate stable convergence to Nash equilibrium on several classic games and also apply DeepFP to a large forest security domain with a novel defender best response oracle. We show that DeepFP learns strategies robust to adversarial exploitation and scales well with growing number of players’ resources.},
	urldate = {2025-01-07},
	booktitle = {Decision and {Game} {Theory} for {Security}: 10th {International} {Conference}, {GameSec} 2019, {Stockholm}, {Sweden}, {October} 30 – {November} 1, 2019, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Kamra, Nitin and Gupta, Umang and Wang, Kai and Fang, Fei and Liu, Yan and Tambe, Milind},
	month = oct,
	year = {2019},
	pages = {238--258},
}

@inproceedings{kamra_deep_2019,
	address = {Richland, SC},
	series = {{AAMAS} '19},
	title = {Deep {Fictitious} {Play} for {Games} with {Continuous} {Action} {Spaces}},
	isbn = {978-1-4503-6309-9},
	abstract = {Fictitious play has been a classic algorithm to solve two-player adversarial games with discrete action spaces. In this work we develop an approximate extension of fictitious play to two-player games with high-dimensional continuous action spaces. We use generative neural networks to approximate players' best responses while also learning a differentiable approximate model to the players' rewards given their actions. Both these networks are trained jointly with gradient-based optimization to emulate fictitious play. We explore our approach in zero-sum games, non zero-sum games and security game domains.},
	urldate = {2025-01-07},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Kamra, Nitin and Gupta, Umang and Wang, Kai and Fang, Fei and Liu, Yan and Tambe, Milind},
	month = may,
	year = {2019},
	pages = {2042--2044},
}

@article{li_structure-informed_nodate,
	title = {Structure-informed {Graph} {Auto}-encoder for {RelationalInference} and {Simulation}},
	abstract = {A variety of real-world applications require the modeling and the simulation of dynamical systems, e.g., physics, transportation and climate. With the increase of complexity, it becomes challenging to infer the true interactions solely based on observational data. In this work, we propose the Structure-informed Graph-Autoencoder for Relational inference and simulation (SUGAR) which incorporates various structural prior knowledge. SUGAR takes the form of a variational auto-encoder whose latent variables represent the underlying interactions among objects. It represents various structural prior knowledge as differentiable constraints on the interaction graph, and optimizes them using gradient-based methods. Experimental results on both synthetic and real-world datasets show our approach clearly outperforms other state-of-the-art methods in terms of both interaction recovery and simulation.},
	language = {en},
	author = {Li, Yaguang and Meng, Chuizheng and Shahabi, Cyrus and Liu, Yan},
	file = {PDF:C\:\\Users\\selvam\\Zotero\\storage\\2IW55RU6\\Li et al. - Structure-informed Graph Auto-encoder for RelationalInference and Simulation.pdf:application/pdf},
}

@inproceedings{li_dbus_2019,
	title = {{DBUS}: {Human} {Driving} {Behavior} {Understanding} {System}},
	shorttitle = {{DBUS}},
	url = {https://ieeexplore.ieee.org/document/9021963/authors#authors},
	doi = {10.1109/ICCVW.2019.00298},
	abstract = {Human driving behavior understanding is a key ingredient for intelligent transportation systems. Either developing self-driving car drives like humans or building V2X systems to improve human driving experience, we need to understand how humans drive and interact with environments. Massive human driving data collected by top ride-sharing platforms and fleet management companies, offers the potential for in-depth understanding of human driving behavior. In this paper, we present DBUS, a real-time driving behavior understanding system which works with front-view videos, GPS/IMU signals collected from daily driving scenarios. Unlike previous work of driving behavior analysis, DBUS focuses on not only the recognition of basic driving actions but also the identification of driver's intentions and attentions. The analysis procedure is designed by mimicking the human intelligence for driving, powered with representation capability of deep neural networks as well as recent advances in visual perception, video temporal segmentation, attention mechanism, etc. Beyond systematic driving behavior analysis, DBUS also supports efficient behavior-based driving scenario search and retrieval, which is essential for practical application when working with large-scale human driving scenario dataset. We perform extensive evaluations of DBUS in term of inference accuracy of intentions, interpretability of inferred driver's attentions, as well as system efficiency. We also provide insightful intuitions as to why and how certain components work based on experience in the development of the system.},
	urldate = {2025-01-08},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	author = {Li, Max Guangyu and Jiang, Bo and Che, Zhengping and Shi, Xuefeng and Liu, Mengyao and Meng, Yiping and Ye, Jieping and Liu, Yan},
	month = oct,
	year = {2019},
	note = {ISSN: 2473-9944},
	keywords = {Acceleration, Autonomous Driving, Behavior Analysis, Brakes, Driving Behavior, Global Positioning System, Safety, Sensor Fusion, Sensors, Vehicles, Video Understanding, Videos},
	pages = {2436--2444},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\\QXUCGJGH\\authors.html:text/html},
}

@misc{che_d2-city_2019,
	title = {D\${\textasciicircum}2\$-{City}: {A} {Large}-{Scale} {Dashcam} {Video} {Dataset} of {Diverse} {Traffic} {Scenarios}},
	shorttitle = {D\${\textasciicircum}2\$-{City}},
	url = {http://arxiv.org/abs/1904.01975},
	doi = {10.48550/arXiv.1904.01975},
	abstract = {Driving datasets accelerate the development of intelligent driving and related computer vision technologies, while substantial and detailed annotations serve as fuels and powers to boost the efficacy of such datasets to improve learning-based models. We propose D\${\textasciicircum}2\$-City, a large-scale comprehensive collection of dashcam videos collected by vehicles on DiDi's platform. D\${\textasciicircum}2\$-City contains more than 10000 video clips which deeply reflect the diversity and complexity of real-world traffic scenarios in China. We also provide bounding boxes and tracking annotations of 12 classes of objects in all frames of 1000 videos and detection annotations on keyframes for the remainder of the videos. Compared with existing datasets, D\${\textasciicircum}2\$-City features data in varying weather, road, and traffic conditions and a huge amount of elaborate detection and tracking annotations. By bringing a diverse set of challenging cases to the community, we expect the D\${\textasciicircum}2\$-City dataset will advance the perception and related areas of intelligent driving.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Che, Zhengping and Li, Guangyu and Li, Tracy and Jiang, Bo and Shi, Xuefeng and Zhang, Xinsheng and Lu, Ying and Wu, Guobin and Liu, Yan and Ye, Jieping},
	month = jun,
	year = {2019},
	note = {arXiv:1904.01975 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\H3TD8LUF\\Che et al. - 2019 - D\$^2\$-City A Large-Scale Dashcam Video Dataset of Diverse Traffic Scenarios.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\K4GDXBZP\\1904.html:text/html},
}

@misc{sharma_combating_2019,
	title = {Combating {Fake} {News}: {A} {Survey} on {Identification} and {Mitigation} {Techniques}},
	shorttitle = {Combating {Fake} {News}},
	url = {http://arxiv.org/abs/1901.06437},
	doi = {10.48550/arXiv.1901.06437},
	abstract = {The proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news, and mitigation of its widespread impact on public opinion. While much of the earlier research was focused on identification of fake news based on its contents or by exploiting users' engagements with the news on social media, there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society. In this survey, we describe the modern-day problem of fake news and, in particular, highlight the technical challenges associated with it. We discuss existing methods and techniques applicable to both identification and mitigation, with a focus on the significant advances in each method and their advantages and limitations. In addition, research has often been limited by the quality of existing datasets and their specific application contexts. To alleviate this problem, we comprehensively compile and summarize characteristic features of available datasets. Furthermore, we outline new directions of research to facilitate future development of effective and interdisciplinary solutions.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Sharma, Karishma and Qian, Feng and Jiang, He and Ruchansky, Natali and Zhang, Ming and Liu, Yan},
	month = jan,
	year = {2019},
	note = {arXiv:1901.06437 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\PKFAFYHG\\Sharma et al. - 2019 - Combating Fake News A Survey on Identification and Mitigation Techniques.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\7IDDZDTC\\1901.html:text/html},
}

@misc{seo_differentiable_2019,
	title = {Differentiable {Physics}-informed {Graph} {Networks}},
	url = {http://arxiv.org/abs/1902.02950},
	doi = {10.48550/arXiv.1902.02950},
	abstract = {While physics conveys knowledge of nature built from an interplay between observations and theory, it has been considered less importantly in deep neural networks. Especially, there are few works leveraging physics behaviors when the knowledge is given less explicitly. In this work, we propose a novel architecture called Differentiable Physics-informed Graph Networks (DPGN) to incorporate implicit physics knowledge which is given from domain experts by informing it in latent space. Using the concept of DPGN, we demonstrate that climate prediction tasks are significantly improved. Besides the experiment results, we validate the effectiveness of the proposed module and provide further applications of DPGN, such as inductive learning and multistep predictions.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Seo, Sungyong and Liu, Yan},
	month = feb,
	year = {2019},
	note = {arXiv:1902.02950 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 11 pages, 9 figures},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\5YX6TUXC\\Seo and Liu - 2019 - Differentiable Physics-informed Graph Networks.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\42JCFLEU\\1902.html:text/html},
}

@article{qian_neural_2018,
	title = {Neural {User} {Response} {Generator}: {Fake} {News} {Detection} with {Collective} {User} {Intelligence}},
	shorttitle = {Neural {User} {Response} {Generator}},
	url = {https://www.ijcai.org/proceedings/2018/533},
	abstract = {Electronic proceedings of IJCAI 2018},
	urldate = {2025-01-08},
	author = {Qian, Feng and Gong, Chengyue and Sharma, Karishma and Liu, Yan},
	year = {2018},
	pages = {3834--3840},
	file = {Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\G8BA8TDJ\\533.html:text/html},
}

@article{kamra_policy_2018,
	title = {Policy {Learning} for {Continuous} {Space} {Security} {Games} {Using} {Neural} {Networks}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11446},
	doi = {10.1609/aaai.v32i1.11446},
	abstract = {A wealth of algorithms centered around (integer) linear programming have been proposed to compute equilibrium strategies in security games with discrete states and actions. However, in practice many domains possess continuous state and action spaces. In this paper, we consider a continuous space security game model with infinite-size action sets for players and present a novel deep learning based approach to extend the existing toolkit for solving security games. Specifically, we present (i) OptGradFP, a novel and general algorithm that searches for the optimal defender strategy in a parameterized continuous search space, and can also be used to learn policies over multiple game states simultaneously; (ii) OptGradFP-NN, a convolutional neural network based implementation of OptGradFP for continuous space security games. We demonstrate the potential to predict good defender strategies via experiments and analysis of OptGradFP and OptGradFP-NN on discrete and continuous game settings.},
	language = {en},
	number = {1},
	urldate = {2025-01-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Kamra, Nitin and Gupta, Umang and Fang, Fei and Liu, Yan and Tambe, Milind},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {Fictitious Play},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\PCJKZFUQ\\Kamra et al. - 2018 - Policy Learning for Continuous Space Security Games Using Neural Networks.pdf:application/pdf},
}

@inproceedings{tsang_neural_2018,
	title = {Neural {Interaction} {Transparency} ({NIT}): {Disentangling} {Learned} {Interactions} for {Improved} {Interpretability}},
	volume = {31},
	shorttitle = {Neural {Interaction} {Transparency} ({NIT})},
	url = {https://proceedings.neurips.cc/paper/2018/hash/74378afe5e8b20910cf1f939e57f0480-Abstract.html},
	urldate = {2025-01-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tsang, Michael and Liu, Hanpeng and Purushotham, Sanjay and Murali, Pavankumar and Liu, Yan},
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\D83UHNZP\\Tsang et al. - 2018 - Neural Interaction Transparency (NIT) Disentangling Learned Interactions for Improved Interpretabil.pdf:application/pdf},
}

@article{hung_utilizing_2018,
	title = {Utilizing {Machine} {Learning} and {Automated} {Performance} {Metrics} to {Evaluate} {Robot}-{Assisted} {Radical} {Prostatectomy} {Performance} and {Predict} {Outcomes}},
	volume = {32},
	issn = {1557-900X},
	doi = {10.1089/end.2018.0035},
	abstract = {PURPOSE: Surgical performance is critical for clinical outcomes. We present a novel machine learning (ML) method of processing automated performance metrics (APMs) to evaluate surgical performance and predict clinical outcomes after robot-assisted radical prostatectomy (RARP).
MATERIALS AND METHODS: We trained three ML algorithms utilizing APMs directly from robot system data (training material) and hospital length of stay (LOS; training label) (≤2 days and {\textgreater}2 days) from 78 RARP cases, and selected the algorithm with the best performance. The selected algorithm categorized the cases as "Predicted as expected LOS (pExp-LOS)" and "Predicted as extended LOS (pExt-LOS)." We compared postoperative outcomes of the two groups (Kruskal-Wallis/Fisher's exact tests). The algorithm then predicted individual clinical outcomes, which we compared with actual outcomes (Spearman's correlation/Fisher's exact tests). Finally, we identified five most relevant APMs adopted by the algorithm during predicting.
RESULTS: The "Random Forest-50" (RF-50) algorithm had the best performance, reaching 87.2\% accuracy in predicting LOS (73 cases as "pExp-LOS" and 5 cases as "pExt-LOS"). The "pExp-LOS" cases outperformed the "pExt-LOS" cases in surgery time (3.7 hours vs 4.6 hours, p = 0.007), LOS (2 days vs 4 days, p = 0.02), and Foley duration (9 days vs 14 days, p = 0.02). Patient outcomes predicted by the algorithm had significant association with the "ground truth" in surgery time (p {\textless} 0.001, r = 0.73), LOS (p = 0.05, r = 0.52), and Foley duration (p {\textless} 0.001, r = 0.45). The five most relevant APMs, adopted by the RF-50 algorithm in predicting, were largely related to camera manipulation.
CONCLUSION: To our knowledge, ours is the first study to show that APMs and ML algorithms may help assess surgical RARP performance and predict clinical outcomes. With further accrual of clinical data (oncologic and functional data), this process will become increasingly relevant and valuable in surgical assessment and training.},
	language = {eng},
	number = {5},
	journal = {Journal of Endourology},
	author = {Hung, Andrew J. and Chen, Jian and Che, Zhengping and Nilanon, Tanachat and Jarc, Anthony and Titus, Micha and Oh, Paul J. and Gill, Inderbir S. and Liu, Yan},
	month = may,
	year = {2018},
	pmid = {29448809},
	keywords = {Aged, Algorithms, artificial intelligence, Databases, Factual, education, Humans, Machine Learning, Male, Middle Aged, Operative Time, prostate neoplasms, Prostatectomy, Prostatic Neoplasms, robotic surgical procedures, Robotic Surgical Procedures},
	pages = {438--444},
}

@misc{che_recurrent_2016,
	title = {Recurrent {Neural} {Networks} for {Multivariate} {Time} {Series} with {Missing} {Values}},
	url = {http://arxiv.org/abs/1606.01865},
	doi = {10.48550/arXiv.1606.01865},
	abstract = {Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},
	month = nov,
	year = {2018},
	note = {arXiv:1606.01865 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\848RQPCD\\Che et al. - 2016 - Recurrent Neural Networks for Multivariate Time Series with Missing Values.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\KTA9CGH7\\1606.html:text/html},
}

@inproceedings{seo_partially_2018,
	address = {New York, NY, USA},
	series = {{AIES} '18},
	title = {Partially {Generative} {Neural} {Networks} for {Gang} {Crime} {Classification} with {Partial} {Information}},
	isbn = {978-1-4503-6012-8},
	url = {https://dl.acm.org/doi/10.1145/3278721.3278758},
	doi = {10.1145/3278721.3278758},
	abstract = {More than 1 million homicides, robberies, and aggravated assaults occur in the United States each year. These crimes are often further classified into different types based on the circumstances surrounding the crime (e.g., domestic violence, gang-related). Despite recent technological advances in AI and machine learning, these additional classification tasks are still done manually by specially trained police officers. In this paper, we provide the first attempt to develop a more automatic system for classifying crimes. In particular, we study the question of classifying whether a given violent crime is gang-related. We introduce a novel Partially Generative Neural Networks (PGNN) that is able to accurately classify gang-related crimes both when full information is available and when there is only partial information. Our PGNN is the first generative-classification model that enables to work when some features of the test examples are missing. Using a crime event dataset from Los Angeles covering 2014-2016, we experimentally show that our PGNN outperforms all other typically used classifiers for the problem of classifying gang-related violent crimes.},
	urldate = {2025-01-07},
	booktitle = {Proceedings of the 2018 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Seo, Sungyong and Chan, Hau and Brantingham, P. Jeffrey and Leap, Jorja and Vayanos, Phebe and Tambe, Milind and Liu, Yan},
	month = dec,
	year = {2018},
	pages = {257--263},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\NU8Z5VA4\\Seo et al. - 2018 - Partially Generative Neural Networks for Gang Crime Classification with Partial Information.pdf:application/pdf},
}

@misc{tsang_detecting_2018,
	title = {Detecting {Statistical} {Interactions} from {Neural} {Network} {Weights}},
	url = {http://arxiv.org/abs/1705.04977},
	doi = {10.48550/arXiv.1705.04977},
	abstract = {Interpreting neural networks is a crucial and challenging task in machine learning. In this paper, we develop a novel framework for detecting statistical interactions captured by a feedforward multilayer neural network by directly interpreting its learned weights. Depending on the desired interactions, our method can achieve significantly better or similar interaction detection performance compared to the state-of-the-art without searching an exponential solution space of possible interactions. We obtain this accuracy and efficiency by observing that interactions between input features are created by the non-additive effect of nonlinear activation functions, and that interacting paths are encoded in weight matrices. We demonstrate the performance of our method and the importance of discovered interactions via experimental results on both synthetic datasets and real-world application datasets.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Tsang, Michael and Cheng, Dehua and Liu, Yan},
	month = feb,
	year = {2018},
	note = {arXiv:1705.04977 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published in ICLR 2018},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\JMTC5SD6\\Tsang et al. - 2018 - Detecting Statistical Interactions from Neural Network Weights.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\8IECRC3P\\1705.html:text/html},
}

@inproceedings{li_multi-task_2018,
	address = {New York, NY, USA},
	series = {{KDD} '18},
	title = {Multi-task {Representation} {Learning} for {Travel} {Time} {Estimation}},
	isbn = {978-1-4503-5552-0},
	url = {https://dl.acm.org/doi/10.1145/3219819.3220033},
	doi = {10.1145/3219819.3220033},
	abstract = {One crucial task in intelligent transportation systems is estimating the duration of a potential trip given the origin location, destination location as well as the departure time. Most existing approaches for travel time estimation assume that the route of the trip is given, which does not hold in real-world applications since the route can be dynamically changed due to traffic conditions, user preferences, etc. As inferring the path from the origin and the destination can be time-consuming and nevertheless error-prone, it is desirable to perform origin-destination travel time estimation, which aims to predict the travel time without online route information. This problem is challenging mainly due to its limited amount of information available and the complicated spatiotemporal dependency. In this paper, we propose a MUlti-task Representation learning model for Arrival Time estimation (MURAT). This model produces meaningful representation that preserves various trip properties in the real-world and at the same time leverages the underlying road network and the spatiotemporal prior knowledge. Further-more, we propose a multi-task learning framework to utilize the path information of historical trips during the training phase which boosts the performance. Experimental results on two large-scale real-world datasets show that the proposed approach achieves clear improvements over state-of-the-art methods},
	urldate = {2025-01-07},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Li, Yaguang and Fu, Kun and Wang, Zheng and Shahabi, Cyrus and Ye, Jieping and Liu, Yan},
	month = jul,
	year = {2018},
	pages = {1695--1704},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\6C6GZ3J8\\Li et al. - 2018 - Multi-task Representation Learning for Travel Time Estimation.pdf:application/pdf},
}

@inproceedings{cheng_matrix_2018,
	title = {Matrix completability analysis via graph k-connectivity},
	url = {https://proceedings.mlr.press/v84/cheng18a.html},
	abstract = {The problem of low-rank matrix completion is continually attracting attention for its applicability to many real-world problems. Still, the large size, extreme sparsity, and non-uniformity of these matrices pose a challenge. In this paper, we make the observation that even when the observed matrix is too sparse for accurate completion, there may be portions of the data where completion is still possible. We propose the completeID algorithm, which exploits the non-uniformity of the observation, to analyze the completability of the input instead of blindly applying completion. Balancing statistical accuracy with computational efficiency, we relate completability to edge-connectivity of the graph associated with the input partially-observed matrix. We develop the MaxKCD algorithm for finding maximally k-edge-connected components efficiently. Experiments across datasets from a variety of applications demonstrate not only the success of completeID but also the importance of completability analysis.},
	language = {en},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Cheng, Dehua and Ruchansky, Natali and Liu, Yan},
	month = mar,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {395--403},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\SAYR2DXY\\Cheng et al. - 2018 - Matrix completability analysis via graph k-connectivity.pdf:application/pdf;Supplementary PDF:C\:\\Users\\selvam\\Zotero\\storage\\FXSSSZXR\\Cheng et al. - 2018 - Matrix completability analysis via graph k-connectivity.pdf:application/pdf},
}

@misc{li_diffusion_2018,
	title = {Diffusion {Convolutional} {Recurrent} {Neural} {Network}: {Data}-{Driven} {Traffic} {Forecasting}},
	shorttitle = {Diffusion {Convolutional} {Recurrent} {Neural} {Network}},
	url = {http://arxiv.org/abs/1707.01926},
	doi = {10.48550/arXiv.1707.01926},
	abstract = {Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain. Traffic forecasting is one canonical example of such learning task. The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting. To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow. Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling. We evaluate the framework on two real-world large scale road network traffic datasets and observe consistent improvement of 12\% - 15\% over state-of-the-art baselines.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Li, Yaguang and Yu, Rose and Shahabi, Cyrus and Liu, Yan},
	month = feb,
	year = {2018},
	note = {arXiv:1707.01926 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2018},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\HIWKLM8X\\Li et al. - 2018 - Diffusion Convolutional Recurrent Neural Network Data-Driven Traffic Forecasting.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\K5NZJHHH\\1707.html:text/html},
}

@inproceedings{seo_automatically_2018,
	title = {Automatically {Inferring} {Data} {Quality} for {Spatiotemporal} {Forecasting}},
	url = {https://openreview.net/forum?id=ByJIWUnpW},
	abstract = {Spatiotemporal forecasting has become an increasingly important prediction task in machine learning and statistics due to its vast applications, such as climate modeling, traffic prediction, video caching predictions, and so on. While numerous studies have been conducted, most existing works assume that the data from different sources or across different locations are equally reliable. Due to cost, accessibility, or other factors, it is inevitable that the data quality could vary, which introduces significant biases into the model and leads to unreliable prediction results. The problem could be exacerbated in black-box prediction models, such as deep neural networks. In this paper, we propose a novel solution that can automatically infer data quality levels of different sources through local variations of spatiotemporal signals without explicit labels. Furthermore, we integrate the estimate of data quality level with graph convolutional networks to exploit their efficient structures. We evaluate our proposed method on forecasting temperatures in Los Angeles.},
	language = {en},
	urldate = {2025-01-08},
	author = {Seo, Sungyong and Mohegh, Arash and Ban-Weiss, George and Liu, Yan},
	month = feb,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\M4M6BYYD\\Seo et al. - 2018 - Automatically Inferring Data Quality for Spatiotemporal Forecasting.pdf:application/pdf},
}

@misc{yu_tensor_2017,
	title = {Tensor {Regression} {Meets} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1710.11345},
	doi = {10.48550/arXiv.1710.11345},
	abstract = {Low-rank tensor regression, a new model class that learns high-order correlation from data, has recently received considerable attention. At the same time, Gaussian processes (GP) are well-studied machine learning models for structure learning. In this paper, we demonstrate interesting connections between the two, especially for multi-way data analysis. We show that low-rank tensor regression is essentially learning a multi-linear kernel in Gaussian processes, and the low-rank assumption translates to the constrained Bayesian inference problem. We prove the oracle inequality and derive the average case learning curve for the equivalent GP model. Our finding implies that low-rank tensor regression, though empirically successful, is highly dependent on the eigenvalues of covariance functions as well as variable correlations.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Yu, Rose and Li, Guangyu and Liu, Yan},
	month = oct,
	year = {2017},
	note = {arXiv:1710.11345 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 17 pages},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\YYAPEAU5\\Yu et al. - 2017 - Tensor Regression Meets Gaussian Processes.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\JP6B86UM\\1710.html:text/html},
}

@misc{purushotham_benchmark_2017,
	title = {Benchmark of {Deep} {Learning} {Models} on {Large} {Healthcare} {MIMIC} {Datasets}},
	url = {http://arxiv.org/abs/1710.08531},
	doi = {10.48550/arXiv.1710.08531},
	abstract = {Deep learning models (aka Deep Neural Networks) have revolutionized many fields including computer vision, natural language processing, speech recognition, and is being increasingly used in clinical healthcare applications. However, few works exist which have benchmarked the performance of the deep learning models with respect to the state-of-the-art machine learning models and prognostic scoring systems on publicly available healthcare datasets. In this paper, we present the benchmarking results for several clinical prediction tasks such as mortality prediction, length of stay prediction, and ICD-9 code group prediction using Deep Learning models, ensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA scores. We used the Medical Information Mart for Intensive Care III (MIMIC-III) (v1.4) publicly available dataset, which includes all patients admitted to an ICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the benchmarking tasks. Our results show that deep learning models consistently outperform all the other approaches especially when the `raw' clinical time series data is used as input features to the models.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Purushotham, Sanjay and Meng, Chuizheng and Che, Zhengping and Liu, Yan},
	month = oct,
	year = {2017},
	note = {arXiv:1710.08531 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Submitted to Journal of Biomedical Informatics (JBI). First two authors have equal contributions},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\A6GDZMZ9\\Purushotham et al. - 2017 - Benchmark of Deep Learning Models on Large Healthcare MIMIC Datasets.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\MWYLWVPL\\1710.html:text/html},
}

@inproceedings{che_hierarchical_2018,
	title = {Hierarchical {Deep} {Generative} {Models} for {Multi}-{Rate} {Multivariate} {Time} {Series}},
	url = {https://proceedings.mlr.press/v80/che18a.html},
	abstract = {Multi-Rate Multivariate Time Series (MR-MTS) are the multivariate time series observations which come with various sampling rates and encode multiple temporal dependencies. State-space models such as Kalman filters and deep learning models such as deep Markov models are mainly designed for time series data with the same sampling rate and cannot capture all the dependencies present in the MR-MTS data. To address this challenge, we propose the Multi-Rate Hierarchical Deep Markov Model (MR-HDMM), a novel deep generative model which uses the latent hierarchical structure with a learnable switch mechanism to capture the temporal dependencies of MR-MTS. Experimental results on two real-world datasets demonstrate that our MR-HDMM model outperforms the existing state-of-the-art deep learning and state-space models on forecasting and interpolation tasks. In addition, the latent hierarchies in our model provide a way to show and interpret the multiple temporal dependencies.},
	language = {en},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Che, Zhengping and Purushotham, Sanjay and Li, Guangyu and Jiang, Bo and Liu, Yan},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {784--793},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\LWINW4Y8\\Che et al. - 2018 - Hierarchical Deep Generative Models for Multi-Rate Multivariate Time Series.pdf:application/pdf;Supplementary PDF:C\:\\Users\\selvam\\Zotero\\storage\\LL2MZA2F\\Che et al. - 2018 - Hierarchical Deep Generative Models for Multi-Rate Multivariate Time Series.pdf:application/pdf},
}

@misc{kamra_deep_2018,
	title = {Deep {Generative} {Dual} {Memory} {Network} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/1710.10368},
	doi = {10.48550/arXiv.1710.10368},
	abstract = {Despite advances in deep learning, neural networks can only learn multiple tasks when trained on them jointly. When tasks arrive sequentially, they lose performance on previously learnt tasks. This phenomenon called catastrophic forgetting is a fundamental challenge to overcome before neural networks can learn continually from incoming data. In this work, we derive inspiration from human memory to develop an architecture capable of learning continuously from sequentially incoming tasks, while averting catastrophic forgetting. Specifically, our contributions are: (i) a dual memory architecture emulating the complementary learning systems (hippocampus and the neocortex) in the human brain, (ii) memory consolidation via generative replay of past experiences, (iii) demonstrating advantages of generative replay and dual memories via experiments, and (iv) improved performance retention on challenging tasks even for low capacity models. Our architecture displays many characteristics of the mammalian memory and provides insights on the connection between sleep and learning.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Kamra, Nitin and Gupta, Umang and Liu, Yan},
	month = may,
	year = {2018},
	note = {arXiv:1710.10368 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\ENK8EF2Y\\Kamra et al. - 2018 - Deep Generative Dual Memory Network for Continual Learning.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\67Q6LBIT\\1710.html:text/html},
}

@misc{goyal_dyngem_2018,
	title = {{DynGEM}: {Deep} {Embedding} {Method} for {Dynamic} {Graphs}},
	shorttitle = {{DynGEM}},
	url = {http://arxiv.org/abs/1805.11273},
	doi = {10.48550/arXiv.1805.11273},
	abstract = {Embedding large graphs in low dimensional spaces has recently attracted significant interest due to its wide applications such as graph visualization, link prediction and node classification. Existing methods focus on computing the embedding for static graphs. However, many graphs in practical applications are dynamic and evolve constantly over time. Naively applying existing embedding algorithms to each snapshot of dynamic graphs independently usually leads to unsatisfactory performance in terms of stability, flexibility and efficiency. In this work, we present an efficient algorithm DynGEM based on recent advances in deep autoencoders for graph embeddings, to address this problem. The major advantages of DynGEM include: (1) the embedding is stable over time, (2) it can handle growing dynamic graphs, and (3) it has better running time than using static embedding methods on each snapshot of a dynamic graph. We test DynGEM on a variety of tasks including graph visualization, graph reconstruction, link prediction and anomaly detection (on both synthetic and real datasets). Experimental results demonstrate the superior stability and scalability of our approach.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Goyal, Palash and Kamra, Nitin and He, Xinran and Liu, Yan},
	month = may,
	year = {2018},
	note = {arXiv:1805.11273 [cs]},
	keywords = {Computer Science - Social and Information Networks},
	annote = {Comment: 3rd International Workshop on Representation Learning for Graphs (ReLiG), IJCAI 2017},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\82QWVHE4\\Goyal et al. - 2018 - DynGEM Deep Embedding Method for Dynamic Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\NPMTMK5U\\1805.html:text/html},
}

@article{kamra_handling_nodate,
	title = {Handling {Continuous} {Space} {Security} {Games} with {Neural} {Networks}},
	abstract = {Despite signiﬁcant research in Security Games, limited efforts have been made to handle game domains with continuous space. Addressing such limitations, in this paper we propose: (i) a continuous space security game model that considers inﬁnitesize action spaces for players; (ii) OptGradFP, a novel and general algorithm that searches for the optimal defender strategy in a parametrized search space; (iii) OptGradFP-NN, a convolutional neural network based implementation of OptGradFP for continuous space security games; (iv) experiments and analysis with OptGradFP-NN. This is the ﬁrst time that neural networks have been used for security games, and it shows the promise of applying deep learning to complex security games which previous approaches fail to handle.},
	language = {en},
	author = {Kamra, Nitin and Fang, Fei and Kar, Debarun and Liu, Yan and Tambe, Milind},
	file = {PDF:C\:\\Users\\selvam\\Zotero\\storage\\4R2ETFXX\\Kamra et al. - Handling Continuous Space Security Games with Neural Networks.pdf:application/pdf},
}

@inproceedings{purushotham_variational_2017,
	title = {Variational {Recurrent} {Adversarial} {Deep} {Domain} {Adaptation}},
	url = {https://openreview.net/forum?id=rk9eAFcxg},
	abstract = {We study the problem of learning domain invariant representations for time series data while transferring the complex temporal latent dependencies between the domains. Our model termed as Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) is built atop a variational recurrent neural network (VRNN) and trains adversarially to capture complex temporal relationships that are domain-invariant. This is (as far as we know) the first to capture and transfer temporal latent dependencies in multivariate time-series data. Through experiments on real-world multivariate healthcare time-series datasets, we empirically demonstrate that learning temporal dependencies helps our model's ability to create domain-invariant representations, allowing our model to outperform current state-of-the-art deep domain adaptation approaches.},
	language = {en},
	urldate = {2025-01-08},
	author = {Purushotham, Sanjay and Carvalho, Wilka and Nilanon, Tanachat and Liu, Yan},
	month = feb,
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\8YIHPIUA\\Purushotham et al. - 2017 - Variational Recurrent Adversarial Deep Domain Adaptation.pdf:application/pdf},
}

@inproceedings{ruchansky_csi_2017,
	title = {{CSI}: {A} {Hybrid} {Deep} {Model} for {Fake} {News} {Detection}},
	shorttitle = {{CSI}},
	url = {http://arxiv.org/abs/1703.06959},
	doi = {10.1145/3132847.3132877},
	abstract = {The topic of fake news has drawn attention both from the public and the academic communities. Such misinformation has the potential of affecting public opinion, providing an opportunity for malicious parties to manipulate the outcomes of public events such as elections. Because such high stakes are at play, automatically detecting fake news is an important, yet challenging problem that is not yet well understood. Nevertheless, there are three generally agreed upon characteristics of fake news: the text of an article, the user response it receives, and the source users promoting it. Existing work has largely focused on tailoring solutions to one particular characteristic which has limited their success and generality. In this work, we propose a model that combines all three characteristics for a more accurate and automated prediction. Specifically, we incorporate the behavior of both parties, users and articles, and the group behavior of users who propagate fake news. Motivated by the three characteristics, we propose a model called CSI which is composed of three modules: Capture, Score, and Integrate. The first module is based on the response and text; it uses a Recurrent Neural Network to capture the temporal pattern of user activity on a given article. The second module learns the source characteristic based on the behavior of users, and the two are integrated with the third module to classify an article as fake or not. Experimental analysis on real-world data demonstrates that CSI achieves higher accuracy than existing models, and extracts meaningful latent representations of both users and articles.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the 2017 {ACM} on {Conference} on {Information} and {Knowledge} {Management}},
	author = {Ruchansky, Natali and Seo, Sungyong and Liu, Yan},
	month = nov,
	year = {2017},
	note = {arXiv:1703.06959 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	pages = {797--806},
	annote = {Comment: In Proceedings of the 26th ACM International Conference on Information and Knowledge Management (CIKM) 2017},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\E7HGWJ73\\Ruchansky et al. - 2017 - CSI A Hybrid Deep Model for Fake News Detection.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\NGPRXYGL\\1703.html:text/html},
}

@incollection{yu_deep_2017,
	series = {Proceedings},
	title = {Deep {Learning}: {A} {Generic} {Approach} for {Extreme} {Condition} {Traffic} {Forecasting}},
	shorttitle = {Deep {Learning}},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611974973.87},
	abstract = {Traffic forecasting is a vital part of intelligent transportation systems. It becomes particularly challenging due to short-term (e.g., accidents, constructions) and long-term (e.g., peak-hour, seasonal, weather) traffic patterns. While most of the previously proposed techniques focus on normal condition forecasting, a single framework for extreme condition traffic forecasting does not exist. To address this need, we propose to take a deep learning approach. We build a deep neural network based on long short term memory (LSTM) units. We apply Deep LSTM to forecast peak-hour traffic and manage to identify unique characteristics of the traffic data. We further improve the model for post-accident forecasting with Mixture Deep LSTM model. It jointly models the normal condition traffic and the pattern of accidents. We evaluate our model on a real-world large-scale traffic dataset in Los Angeles. When trained end-to-end with suitable regularization, our approach achieves 30\%–50\% improvement over baselines. We also demonstrate a novel technique to interpret the model with signal stimulation. We note interesting observations from the trained neural network.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the 2017 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Yu, Rose and Li, Yaguang and Shahabi, Cyrus and Demiryurek, Ugur and Liu, Yan},
	month = jun,
	year = {2017},
	doi = {10.1137/1.9781611974973.87},
	pages = {777--785},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\6FFSLWJL\\Yu et al. - 2017 - Deep Learning A Generic Approach for Extreme Condition Traffic Forecasting.pdf:application/pdf},
}

@misc{che_boosting_2017,
	title = {Boosting {Deep} {Learning} {Risk} {Prediction} with {Generative} {Adversarial} {Networks} for {Electronic} {Health} {Records}},
	url = {http://arxiv.org/abs/1709.01648},
	doi = {10.48550/arXiv.1709.01648},
	abstract = {The rapid growth of Electronic Health Records (EHRs), as well as the accompanied opportunities in Data-Driven Healthcare (DDH), has been attracting widespread interests and attentions. Recent progress in the design and applications of deep learning methods has shown promising results and is forcing massive changes in healthcare academia and industry, but most of these methods rely on massive labeled data. In this work, we propose a general deep learning framework which is able to boost risk prediction performance with limited EHR data. Our model takes a modified generative adversarial network namely ehrGAN, which can provide plausible labeled EHR data by mimicking real patient records, to augment the training dataset in a semi-supervised learning manner. We use this generative model together with a convolutional neural network (CNN) based prediction model to improve the onset prediction performance. Experiments on two real healthcare datasets demonstrate that our proposed framework produces realistic data samples and achieves significant improvements on classification tasks with the generated data over several stat-of-the-art baselines.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Che, Zhengping and Cheng, Yu and Zhai, Shuangfei and Sun, Zhaonan and Liu, Yan},
	month = sep,
	year = {2017},
	note = {arXiv:1709.01648 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To appear in ICDM 2017. This is the full version of paper with 8 pages},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\2MPYYFWG\\Che et al. - 2017 - Boosting Deep Learning Risk Prediction with Generative Adversarial Networks for Electronic Health Re.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\IF3H8WG6\\1709.html:text/html},
}

@incollection{che_time_2017,
	address = {Cham},
	title = {Time {Series} {Feature} {Learning} with {Applications} to {Health} {Care}},
	isbn = {978-3-319-51394-2},
	url = {https://doi.org/10.1007/978-3-319-51394-2_20},
	abstract = {Exponential growth in mobile health devices and electronic health records has resulted in a surge of large-scale time series data, which demands effective and fast machine learning models for analysis and discovery. In this chapter, we discuss a novel framework based on deep learning which automatically performs feature learning from heterogeneous time series data. It is well-suited for healthcare applications, where available data have many sparse outputs (e.g., rare diagnoses) and exploitable structures (e.g., temporal order and relationships between labels). Furthermore, we introduce a simple yet effective knowledge-distillation approach to learn an interpretable model while achieving the prediction performance of deep models. We conduct experiments on several real-world datasets and show the empirical efficacy of our framework and the interpretability of the mimic models.},
	language = {en},
	urldate = {2025-01-08},
	booktitle = {Mobile {Health}: {Sensors}, {Analytic} {Methods}, and {Applications}},
	publisher = {Springer International Publishing},
	author = {Che, Zhengping and Purushotham, Sanjay and Kale, David and Li, Wenzhe and Bahadori, Mohammad Taha and Khemani, Robinder and Liu, Yan},
	editor = {Rehg, James M. and Murphy, Susan A. and Kumar, Santosh},
	year = {2017},
	doi = {10.1007/978-3-319-51394-2_20},
	pages = {389--409},
}

@article{purushotham_relational_2018,
	title = {Relational {Multi}-{Instance} {Learning} for {Concept} {Annotation} from {Medical} {Time} {Series}},
	url = {https://openreview.net/forum?id=ByJbJwxCW},
	abstract = {Recent advances in computing technology and sensor design have made it easier to collect longitudinal or time series data from patients, resulting in a gigantic amount of available medical data. Most of the medical time series lack annotations or even when the annotations are available they could be subjective and prone to human errors. Earlier works have developed natural language processing techniques to extract concept annotations and/or clinical narratives from doctor notes. However, these approaches are slow and do not use the accompanying medical time series data. To address this issue, we introduce the problem of concept annotation for the medical time series data, i.e., the task of predicting and localizing medical concepts by using the time series data as input. We propose Relational Multi-Instance Learning (RMIL) - a deep Multi Instance Learning framework based on recurrent neural networks, which uses pooling functions and attention mechanisms for the concept annotation tasks. Empirical results on medical datasets show that our proposed models outperform various multi-instance learning models.},
	language = {en},
	urldate = {2025-01-08},
	author = {Purushotham, Sanjay and Che, Zhengping and Jiang, Bo and Nilanon, Tanachat and Liu, Yan},
	month = feb,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\TGUAJ72T\\Purushotham et al. - 2018 - Relational Multi-Instance Learning for Concept Annotation from Medical Time Series.pdf:application/pdf},
}

@article{seo_representation_nodate,
	title = {Representation {Learning} of {Users} and {Items} for {Review} {Rating} {Prediction} {Using} {Attention}-based {Convolutional} {Neural} {Network}},
	abstract = {It is common nowadays for e-commerce websites to encourage their users to rate shopping items and write review text. This review text information has been proven to be very useful in understanding user preferences and item properties, and thus enhances the capability of these websites to make personalized recommendations. In this paper, we propose to model user preferences and item properties using a convolutional neural network (CNN) with attention, motivated by the huge success of CNN for many natural language processing tasks. By using aggregated review text from users and items, we aim to build vector representations of user and item using attention-based CNNs. These vector representations are then used to predict rating values for a user on an item. We train these user and item networks jointly, which enables the interaction between users and items in a way similar to the matrix factorization technique. In addition, the visualization of the attention layer gives us insight on when words are selected by the models that highlight a user’s preferences or an item’s properties. We validate the proposed models on popular review datasets, Yelp and Amazon, and compare results with matrix factorization (MF), and hidden factor and topical (HFT) models. Our experiments show improvement over HFT, which proves the eﬀectiveness of these representations learned from our networks on review text for rating prediction.},
	language = {en},
	author = {Seo, Sungyong and Huang, Jing and Yang, Hao and Liu, Yan},
	file = {PDF:C\:\\Users\\selvam\\Zotero\\storage\\3KF2DQVE\\Seo et al. - Representation Learning of Users and Items for Review Rating Prediction Using Attention-based Convol.pdf:application/pdf},
}

@inproceedings{he_not_2017,
	address = {New York, NY, USA},
	series = {{WSDM} '17},
	title = {Not {Enough} {Data}? {Joint} {Inferring} {Multiple} {Diffusion} {Networks} via {Network} {Generation} {Priors}},
	isbn = {978-1-4503-4675-7},
	shorttitle = {Not {Enough} {Data}?},
	url = {https://dl.acm.org/doi/10.1145/3018661.3018675},
	doi = {10.1145/3018661.3018675},
	abstract = {Network Inference, i.e., discovering latent diffusion networks from observed cascades, has been studied extensively in recent years, leading to a series of excellent work. However, it has been observed that the accuracy of existing methods deteriorates significantly when the number of cascades are limited (compared with the large number of nodes), which is the norm in real world applications. Meanwhile, we are able to collect cascades on many different topics or over a long time period: the associated influence networks (either topic-specific or time-specific) are highly correlated while the number of cascade observations associated with each network is very limited. In this work, we propose a generative model, referred to as the MultiCascades model (MCM), to address the challenge of data scarcity by exploring the commonality between multiple related diffusion networks. MCM builds a hierarchical graphical model, where all the diffusion networks share the same network prior, e.g., the popular Stochastic Blockmodels or the latent space models. The parameters of the network priors can be effectively learned by gleaning evidence from a large number of inferred networks. In return, each individual network can be inferred more accurately thanks to the prior information. Furthermore, we develop efficient inference and learning algorithms so that MCM is scalable for practical applications. The results on both synthetic datasets and real-world datasets demonstrate that MCM infers both topic-specific and time-varying diffusion networks more accurately.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the {Tenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {He, Xinran and Liu, Yan},
	month = feb,
	year = {2017},
	pages = {465--474},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\59EBI6CB\\He and Liu - 2017 - Not Enough Data Joint Inferring Multiple Diffusion Networks via Network Generation Priors.pdf:application/pdf},
}

@article{che_decade_2017,
	title = {{DECADE}: {A} {Deep} {Metric} {Learning} {Model} for {Multivariate} {Time} {Series}},
	abstract = {Determining similarities (or distance) between multivariate time series sequences is a fundamental problem in time series analysis. The complex temporal dependencies and variable lengths of time series make it an extremely challenging task. Most existing work either rely on heuristics which lacks flexibility and theoretical justifications, or build complex algorithms that are not scalable to big data. In this paper, we propose a novel and effective metric learning model for multivariate time series, referred to as Deep ExpeCted Alignment DistancE (DECADE). It yields a valid distance metric for time series with unequal lengths by sampling from an innovative alignment mechanism, namely expected alignment, and captures complex temporal multivariate dependencies in local representation learned by deep networks. On the whole, DECADE can provide valid data-dependent distance metric efficiently via end-toend gradient training. Extensive experiments on both synthetic and application datasets with multivariate time series demonstrate the superior performance of DECADE compared to the state-of-the-art approaches.},
	language = {en},
	journal = {Nova Scotia},
	author = {Che, Zhengping and He, Xinran and Xu, Ke and Liu, Yan},
	year = {2017},
	file = {PDF:C\:\\Users\\selvam\\Zotero\\storage\\944W8663\\Che et al. - 2017 - DECADE A Deep Metric Learning Model for Multivariate Time Series.pdf:application/pdf},
}

@article{che_deep_2018,
	title = {Deep {Learning} {Solutions} for {Classifying} {Patients} on {Opioid} {Use}},
	volume = {2017},
	issn = {1942-597X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5977635/},
	abstract = {Opioid analgesics, as commonly prescribed medications used for relieving pain in patients, are especially prevalent in US these years. However, an increasing amount of opioid misuse and abuse have caused lots of consequences. Researchers and clinicians have attempted to discover the factors leading to opioid long-term use, dependence, and abuse, but only limited incidents are understood from previous works. Motivated by recent successes of deep learning and the abundant amount of electronic health records, we apply state-of-the-art deep and recurrent neural network models on a dataset of more than one hundred thousand opioid users. Our models are shown to achieve robust and superior results on classifying opioid users, and are able to extract key factors for different opioid user groups. This work is also a good demonstration on adopting novel deep learning methods for real-world health care problems.},
	urldate = {2025-01-08},
	journal = {AMIA Annual Symposium Proceedings},
	author = {Che, Zhengping and St. Sauver, Jennifer and Liu, Hongfang and Liu, Yan},
	month = apr,
	year = {2018},
	pmid = {29854117},
	pmcid = {PMC5977635},
	pages = {525--534},
	file = {PubMed Central Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\QMUGBSEV\\Che et al. - 2018 - Deep Learning Solutions for Classifying Patients on Opioid Use.pdf:application/pdf},
}

@article{matsuo_pilot_2017,
	title = {A pilot study in using deep learning to predict limited life expectancy in women with recurrent cervical cancer},
	volume = {217},
	issn = {0002-9378},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7534808/},
	doi = {10.1016/j.ajog.2017.08.012},
	number = {6},
	urldate = {2025-01-08},
	journal = {American journal of obstetrics and gynecology},
	author = {Matsuo, Koji and Purushotham, Sanjay and Moeini, Aida and Li, Guangyu and Machida, Hiroko and Liu, Yan and Roman, Lynda D.},
	month = dec,
	year = {2017},
	pmid = {28843741},
	pmcid = {PMC7534808},
	pages = {703--705},
	file = {PubMed Central Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\LY8FY27F\\Matsuo et al. - 2017 - A pilot study in using deep learning to predict limited life expectancy in women with recurrent cerv.pdf:application/pdf},
}

@article{seo_data_nodate,
	title = {Data {Quality} {Network} for {Spatiotemporal} {Forecasting}},
	abstract = {The importance of learning from spatiotemporal data has been growing with the increasing number of data sources distributed over space. While numerous studies have been done for analyzing the spatiotemporal signals, existing models have assumed that the heterogeneous data sources in spatial domain are equally reliable over time. In this paper, we propose the novel method that infers the time-varying data quality level based on the local variations of spatiotemporal signals without explicitly assigned labels. Furthermore, we extend the formulation of the quality level by combining with graph convolutional networks to exploit the efﬁcient architecture. Finally, we evaluate the proposed method by simulating a forecasting task with real-world climate data.},
	language = {en},
	author = {Seo, Sungyong and Liu, Yan and Mohegh, Arash and Ban-Weiss, George},
	file = {PDF:C\:\\Users\\selvam\\Zotero\\storage\\BKICYHCB\\Seo et al. - Data Quality Network for Spatiotemporal Forecasting.pdf:application/pdf},
}

@inproceedings{seo_interpretable_2017,
	address = {Como Italy},
	title = {Interpretable {Convolutional} {Neural} {Networks} with {Dual} {Local} and {Global} {Attention} for {Review} {Rating} {Prediction}},
	isbn = {978-1-4503-4652-8},
	url = {https://dl.acm.org/doi/10.1145/3109859.3109890},
	doi = {10.1145/3109859.3109890},
	language = {en},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the {Eleventh} {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Seo, Sungyong and Huang, Jing and Yang, Hao and Liu, Yan},
	month = aug,
	year = {2017},
	pages = {297--305},
}


@inproceedings{nilanon_normal_2016,
	title = {Normal / abnormal heart sound recordings classification using convolutional neural network},
	url = {https://ieeexplore.ieee.org/abstract/document/7868810},
	abstract = {As part of the PhysioNet / Computing in Cardiology Challenge 2016, this work focuses on automatic classification of normal / abnormal phonocardiogram (PCG) recording, with the aim of quickly identifying subjects that need further expert diagnosis. To improve the robustness of the classifiers by increasing the number of training samples, the recordings were windowed into 5 second segments and our classifiers were trained to classify these segments. Overall recording classification was then generated using a voting scheme from classification results of its segments. Our features include spectrograms and Mel-frequency cepstrum coefficients. Our best submission result during the official phase (evaluated on a random 20\% of the hidden test set) has a score of 0.813, with 0.735 sensitivity and 0.892 specificity. Two more submissions are still being evaluated.},
	urldate = {2025-01-08},
	booktitle = {2016 {Computing} in {Cardiology} {Conference} ({CinC})},
	author = {Nilanon, Tanachat and Yao, Jiayu and Hao, Junheng and Purushotham, Sanjay and Liu, Yan},
	month = sep,
	year = {2016},
	note = {ISSN: 2325-887X},
	keywords = {Feature extraction, Heart, Logistics, Pathology, Spectrogram, Support vector machines, Training},
	pages = {585--588},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\\VBE7K4P2\\7868810.html:text/html},
}

@inproceedings{he_learning_2016,
	title = {Learning {Influence} {Functions} from {Incomplete} {Observations}},
	volume = {29},
	url = {https://papers.nips.cc/paper_files/paper/2016/hash/68b1fbe7f16e4ae3024973f12f3cb313-Abstract.html},
	abstract = {We study the problem of learning influence functions under incomplete observations of node activations. Incomplete observations are a major concern as most (online and real-world) social networks are not fully observable. We establish both proper and improper PAC learnability of influence functions under randomly missing observations. Proper PAC learnability under the Discrete-Time Linear Threshold (DLT) and Discrete-Time Independent Cascade (DIC) models is established by reducing incomplete observations to complete observations in a modified graph. Our improper PAC learnability result applies for the DLT and DIC models as well as the Continuous-Time Independent Cascade (CIC) model.  It is based on a parametrization in terms of reachability features, and also gives rise to an efficient and practical heuristic. Experiments on synthetic and real-world datasets demonstrate the ability of our method to compensate even for a fairly large fraction of missing observations.},
	urldate = {2025-01-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {He, Xinran and Xu, Ke and Kempe, David and Liu, Yan},
	year = {2016},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\9NCR56UU\\He et al. - 2016 - Learning Influence Functions from Incomplete Observations.pdf:application/pdf},
}

@article{che_interpretable_2017,
	title = {Interpretable {Deep} {Models} for {ICU} {Outcome} {Prediction}},
	volume = {2016},
	issn = {1942-597X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333206/},
	abstract = {Exponential surge in health care data, such as longitudinal data from electronic health records (EHR), sensor data from intensive care unit (ICU), etc., is providing new opportunities to discover meaningful data-driven characteristics and patterns ofdiseases. Recently, deep learning models have been employedfor many computational phenotyping and healthcare prediction tasks to achieve state-of-the-art performance. However, deep models lack interpretability which is crucial for wide adoption in medical research and clinical decision-making. In this paper, we introduce a simple yet powerful knowledge-distillation approach called interpretable mimic learning, which uses gradient boosting trees to learn interpretable models and at the same time achieves strong prediction performance as deep learning models. Experiment results on Pediatric ICU dataset for acute lung injury (ALI) show that our proposed method not only outperforms state-of-the-art approaches for morality and ventilator free days prediction tasks but can also provide interpretable models to clinicians.},
	urldate = {2025-01-08},
	journal = {AMIA Annual Symposium Proceedings},
	author = {Che, Zhengping and Purushotham, Sanjay and Khemani, Robinder and Liu, Yan},
	month = feb,
	year = {2017},
	pmid = {28269832},
	pmcid = {PMC5333206},
	pages = {371--380},
	file = {PubMed Central Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\Y7NMW9HM\\Che et al. - 2017 - Interpretable Deep Models for ICU Outcome Prediction.pdf:application/pdf},
}

@inproceedings{cheng_spals_2016,
	title = {{SPALS}: {Fast} {Alternating} {Least} {Squares} via {Implicit} {Leverage} {Scores} {Sampling}},
	volume = {29},
	shorttitle = {{SPALS}},
	url = {https://papers.nips.cc/paper_files/paper/2016/hash/f4f6dce2f3a0f9dada0c2b5b66452017-Abstract.html},
	abstract = {Tensor CANDECOMP/PARAFAC (CP) decomposition is a powerful but computationally challenging tool in modern data analytics. In this paper, we show ways of sampling intermediate steps of alternating minimization algorithms for computing low rank tensor CP decompositions, leading to the sparse alternating least squares (SPALS) method. Specifically, we sample the the Khatri-Rao product, which arises as an intermediate object during the iterations of alternating least squares. This product captures the interactions between different tensor modes, and form the main computational bottleneck for solving many tensor related tasks. By exploiting the spectral structures of the matrix Khatri-Rao product, we provide efficient access to its statistical leverage scores. When applied to the tensor CP decomposition, our method leads to the first algorithm that runs in sublinear time per-iteration and approximates the output of deterministic alternating least squares algorithms. Empirical evaluations of this approach show significantly speedups over existing randomized and deterministic routines for performing CP decomposition. On a tensor of the size 2.4m by 6.6m by 92k with over 2 billion nonzeros formed by Amazon product reviews, our routine converges in two minutes to the same error as deterministic ALS.},
	urldate = {2025-01-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cheng, Dehua and Peng, Richard and Liu, Yan and Perros, Ioakeim},
	year = {2016},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\VZJ88BAI\\Cheng et al. - 2016 - SPALS Fast Alternating Least Squares via Implicit Leverage Scores Sampling.pdf:application/pdf},
}

@inproceedings{yu_geographic_2016,
	address = {New York, NY, USA},
	series = {{WSDM} '16},
	title = {Geographic {Segmentation} via {Latent} {Poisson} {Factor} {Model}},
	isbn = {978-1-4503-3716-8},
	url = {https://doi.org/10.1145/2835776.2835806},
	doi = {10.1145/2835776.2835806},
	abstract = {Discovering latent structures in spatial data is of critical importance to understanding the user behavior of location-based services. In this paper, we study the problem of geographic segmentation of spatial data, which involves dividing a collection of observations into distinct geo-spatial regions and uncovering abstract correlation structures in the data. We introduce a novel, Latent Poisson Factor (LPF) model to describe spatial count data. The model describes the spatial counts as a Poisson distribution with a mean that factors over a joint item-location latent space. The latent factors are constrained with weak labels to help uncover interesting spatial dependencies. We study the LPF model on a mobile app usage data set and a news article readership data set. We empirically demonstrate its effectiveness on a variety of prediction tasks on these two data sets.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the {Ninth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Yu, Rose and Gelfand, Andrew and Rajan, Suju and Shahabi, Cyrus and Liu, Yan},
	month = feb,
	year = {2016},
	pages = {357--366},
}

@article{yu_survey_2016,
	title = {A {Survey} on {Social} {Media} {Anomaly} {Detection}},
	volume = {18},
	issn = {1931-0145},
	url = {https://doi.org/10.1145/2980765.2980767},
	doi = {10.1145/2980765.2980767},
	abstract = {Social media anomaly detection is of critical importance to prevent malicious activities such as bullying, terrorist attack planning, and fraud information dissemination. With the recent popularity of social media, new types of anomalous behaviors arise, causing concerns from various parties. While a large amount of work have been dedicated to traditional anomaly detection problems, we observe a surge of research interests in the new realm of social media anomaly detection. In this paper, we present a survey on existing approaches to address this problem. We focus on the new type of anomalous phenomena in the social media and review the recent developed techniques to detect those special types of anomalies. We provide a general overview of the problem domain, common formulations, existing methodologies and potential directions. With this work, we hope to call out the attention from the research community on this challenging problem and open up new directions that we can contribute in the future},
	number = {1},
	urldate = {2025-01-08},
	journal = {SIGKDD Explor. Newsl.},
	author = {Yu, Rose and Qiu, Huida and Wen, Zhen and Lin, ChingYung and Liu, Yan},
	month = aug,
	year = {2016},
	pages = {1--14},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\KKH6NJAD\\Yu et al. - 2016 - A Survey on Social Media Anomaly Detection.pdf:application/pdf},
}

@inproceedings{chang_timeline_2016,
	address = {New York, New York, USA},
	series = {{IJCAI}'16},
	title = {Timeline summarization from social media with life cycle models},
	isbn = {978-1-57735-770-4},
	abstract = {The popularity of social media shatters the barrier for online users to create and share information at any place at any time. As a consequence, it has become increasing difficult to locate relevance information about an entity. Timeline has been proven to provide an effective and efficient access to understand an entity by displaying a list of episodes about the entity in chronological order. However, summarizing the timeline about an entity with social media data faces new challenges. First, key timeline episodes about the entity are typically unavailable in existing social media services. Second, the short, noisy and informal nature of social media posts determines that only content-based summarization could be insufficient. In this paper, we investigate the problem of timeline summarization and propose a novel framework Timeline-Sumy, which consists of episode detecting and summary ranking. In episode detecting, we explicitly model temporal information with life cycle models to detect timeline episodes since episodes usually exhibit sudden-rise-and-heavy-tail patterns on time-series. In summary ranking, we rank social media posts in each episode via a learning-to-rank approach. The experimental results on social media datasets demonstrate the effectiveness of the proposed framework.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the {Twenty}-{Fifth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Chang, Yi and Tang, Jiliang and Yin, Dawei and Yamada, Makoto and Liu, Yan},
	month = jul,
	year = {2016},
	pages = {3698--3704},
}

@misc{chen_bochners_2016,
	title = {On {Bochner}'s and {Polya}'s {Characterizations} of {Positive}-{Definite} {Kernels} and the {Respective} {Random} {Feature} {Maps}},
	url = {http://arxiv.org/abs/1610.08861},
	doi = {10.48550/arXiv.1610.08861},
	abstract = {Positive-definite kernel functions are fundamental elements of kernel methods and Gaussian processes. A well-known construction of such functions comes from Bochner's characterization, which connects a positive-definite function with a probability distribution. Another construction, which appears to have attracted less attention, is Polya's criterion that characterizes a subset of these functions. In this paper, we study the latter characterization and derive a number of novel kernels little known previously. In the context of large-scale kernel machines, Rahimi and Recht (2007) proposed a random feature map (random Fourier) that approximates a kernel function, through independent sampling of the probability distribution in Bochner's characterization. The authors also suggested another feature map (random binning), which, although not explicitly stated, comes from Polya's characterization. We show that with the same number of random samples, the random binning map results in an Euclidean inner product closer to the kernel than does the random Fourier map. The superiority of the random binning map is confirmed empirically through regressions and classifications in the reproducing kernel Hilbert space.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Chen, Jie and Cheng, Dehua and Liu, Yan},
	month = oct,
	year = {2016},
	note = {arXiv:1610.08861 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\BANLZF5Q\\Chen et al. - 2016 - On Bochner's and Polya's Characterizations of Positive-Definite Kernels and the Respective Random Fe.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\2GWP34QF\\1610.html:text/html},
}

@inproceedings{deng_latent_2016,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {Latent {Space} {Model} for {Road} {Networks} to {Predict} {Time}-{Varying} {Traffic}},
	isbn = {978-1-4503-4232-2},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939860},
	doi = {10.1145/2939672.2939860},
	abstract = {Real-time traffic prediction from high-fidelity spatiotemporal traffic sensor datasets is an important problem for intelligent transportation systems and sustainability. However, it is challenging due to the complex topological dependencies and high dynamism associated with changing road conditions. In this paper, we propose a Latent Space Model for Road Networks (LSM-RN) to address these challenges holistically. In particular, given a series of road network snapshots, we learn the attributes of vertices in latent spaces which capture both topological and temporal properties. As these latent attributes are time-dependent, they can estimate how traffic patterns form and evolve. In addition, we present an incremental online algorithm which sequentially and adaptively learns the latent attributes from the temporal graph changes. Our framework enables real-time traffic prediction by 1) exploiting real-time sensor readings to adjust/update the existing latent spaces, and 2) training as data arrives and making predictions on-the-fly. By conducting extensive experiments with a large volume of real-world traffic sensor data, we demonstrate the superiority of our framework for real-time traffic prediction on large road networks over competitors as well as baseline graph-based LSM's.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Deng, Dingxiong and Shahabi, Cyrus and Demiryurek, Ugur and Zhu, Linhong and Yu, Rose and Liu, Yan},
	month = aug,
	year = {2016},
	pages = {1525--1534},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\UC9I6PA2\\Deng et al. - 2016 - Latent Space Model for Road Networks to Predict Time-Varying Traffic.pdf:application/pdf},
}

@inproceedings{yu_learning_2016,
	title = {Learning from {Multiway} {Data}: {Simple} and {Efficient} {Tensor} {Regression}},
	shorttitle = {Learning from {Multiway} {Data}},
	url = {https://proceedings.mlr.press/v48/yu16.html},
	abstract = {Tensor regression has shown to be advantageous in learning tasks with multi-directional relatedness. Given massive multiway data, traditional methods are often too slow to operate on or suffer from memory bottleneck. In this paper, we introduce subsampled tensor projected gradient to solve the problem. Our algorithm is impressively simple and efficient. It is built upon projected gradient method with fast tensor power iterations, leveraging randomized sketching for further acceleration. Theoretical analysis shows that our algorithm converges to the correct solution in fixed number of iterations. The memory requirement grows linearly with the size of the problem. We demonstrate superior empirical performance on both multi-linear multi-task learning and spatio-temporal applications.},
	language = {en},
	urldate = {2025-01-08},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yu, Rose and Liu, Yan},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {373--381},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\ASJ2HV7H\\Yu and Liu - 2016 - Learning from Multiway Data Simple and Efficient Tensor Regression.pdf:application/pdf},
}

@inproceedings{bahadori_functional_2015,
	address = {Lille, France},
	series = {{ICML}'15},
	title = {Functional subspace clustering with application to time series},
	abstract = {Functional data, where samples are random functions, are increasingly common and important in a variety of applications, such as health care and traffic analysis. They are naturally high dimensional and lie along complex manifolds. These properties warrant use of the subspace assumption, but most state-of-the-art subspace learning algorithms are limited to linear or other simple settings. To address these challenges, we propose a new framework called Functional Subspace Clustering (FSC). FSC assumes that functional samples lie in deformed linear subspaces and formulates the subspace learning problem as a sparse regression over operators. The resulting problem can be efficiently solved via greedy variable selection, given access to a fast deformation oracle. We provide theoretical guarantees for FSC and show how it can be applied to time series with warped alignments. Experimental results on both synthetic data and real clinical time series show that FSC outperforms both standard time series clustering and state-of-the-art subspace clustering.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 37},
	publisher = {JMLR.org},
	author = {Bahadori, Mohammad Taha and Kale, David and Fan, Yingying and Liu, Yan},
	month = jul,
	year = {2015},
	pages = {228--237},
}

@inproceedings{cheng_efficient_2015,
	title = {Efficient {Sampling} for {Gaussian} {Graphical} {Models} via {Spectral} {Sparsification}},
	url = {https://proceedings.mlr.press/v40/Cheng15.html},
	abstract = {Motivated by a sampling problem basic to computational statistical inference, we develop a toolset based on spectral sparsification for a family of fundamental problems involving Gaussian sampling, matrix functionals, and reversible Markov chains. Drawing on the connection between Gaussian graphical models and the recent breakthroughs in spectral graph theory, we give the first nearly linear time algorithm for the following basic matrix problem: Given an n{\textbackslash}times n Laplacian matrix {\textbackslash}mathbfM and a constant -1 ≤p ≤1, provide efficient access to a sparse n{\textbackslash}times n linear operator {\textbackslash}tilde{\textbackslash}mathbfC such that {\textbackslash}mathbfMp≈{\textbackslash}mathbfC{\textasciitilde}{\textbackslash}mathbfC{\textasciitilde}⊤,where≈denotesspectralsimilarity.Whenpissetto−1,thisgivesthefirstparallelsamplingalgorithmthatisessentiallyoptimalbothintotalworkandrandomnessforGaussianrandomfieldswithsymmetricdiagonallydominant(SDD)precisionmatrices.Itonlyrequires{\textbackslash}emnearlylinearworkand2n{\textbackslash}emi.i.d.randomunivariateGaussiansamplestogenerateann−dimensional{\textbackslash}emi.i.d.Gaussianrandomsampleinpolylogarithmicdepth.Thekeyingredientofourapproachisanintegrationofspectralsparsificationwithmultilevelmethod:Ouralgorithmsarebasedonfactoring{\textbackslash}mathbfMp{\textbackslash}mathbfMp≈{\textbackslash}mathbfC{\textasciitilde}{\textbackslash}mathbfC{\textasciitilde}⊤,where≈denotesspectralsimilarity.Whenpissetto−1,thisgivesthefirstparallelsamplingalgorithmthatisessentiallyoptimalbothintotalworkandrandomnessforGaussianrandomfieldswithsymmetricdiagonallydominant(SDD)precisionmatrices.Itonlyrequires{\textbackslash}emnearlylinearworkand2n{\textbackslash}emi.i.d.randomunivariateGaussiansamplestogenerateann−dimensional{\textbackslash}emi.i.d.Gaussianrandomsampleinpolylogarithmicdepth.Thekeyingredientofourapproachisanintegrationofspectralsparsificationwithmultilevelmethod:Ouralgorithmsarebasedonfactoring{\textbackslash}mathbfMp{\textbackslash}mathbfM{\textasciicircum}p ≈{\textbackslash}tilde{\textbackslash}mathbfC {\textbackslash}tilde{\textbackslash}mathbfC{\textasciicircum}⊤, where ≈denotes spectral similarity. When p is set to -1, this gives the first parallel sampling algorithm that is essentially optimal both in total work and randomness for Gaussian random fields with symmetric diagonally dominant (SDD) precision matrices. It only requires {\textbackslash}em nearly linear work and 2n {\textbackslash}em i.i.d. random univariate Gaussian samples to generate an n-dimensional {\textbackslash}em i.i.d. Gaussian random sample in polylogarithmic depth. The key ingredient of our approach is an integration of spectral sparsification with multilevel method: Our algorithms are based on factoring {\textbackslash}mathbfM{\textasciicircum}p into a product of well-conditioned matrices, then introducing powers and replacing dense matrices with sparse approximations. We give two sparsification methods for this approach that may be of independent interest. The first invokes Maclaurin series on the factors, while the second builds on our new nearly linear time spectral sparsification algorithm for random-walk matrix polynomials. We expect these algorithmic advances will also help to strengthen the connection between machine learning and spectral graph theory, two of the most active fields in understanding large data and networks.},
	language = {en},
	urldate = {2025-01-08},
	booktitle = {Proceedings of {The} 28th {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Cheng, Dehua and Cheng, Yu and Liu, Yan and Peng, Richard and Teng, Shang-Hua},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {364--390},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\NVA55TAI\\Cheng et al. - 2015 - Efficient Sampling for Gaussian Graphical Models via Spectral Sparsification.pdf:application/pdf},
}

@inproceedings{he_hawkestopic_2015,
	title = {{HawkesTopic}: {A} {Joint} {Model} for {Network} {Inference} and {Topic} {Modeling} from {Text}-{Based} {Cascades}},
	shorttitle = {{HawkesTopic}},
	url = {https://proceedings.mlr.press/v37/he15.html},
	abstract = {Understanding the diffusion of information in social network and social media requires modeling the text diffusion process. In this work, we develop the HawkesTopic model (HTM) for analyzing text-based cascades, such as "retweeting a post" or "publishing a follow-up blog post". HTM combines Hawkes processes and topic modeling to simultaneously reason about the information diffusion pathways and the topics characterizing the observed textual information. We show how to jointly infer them with a mean-field variational inference algorithm and validate our approach on both synthetic and real-world data sets, including a news media dataset for modeling information diffusion, and an ArXiv publication dataset for modeling scientific influence. The results show that HTM is significantly more accurate than several baselines for both tasks.},
	language = {en},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {He, Xinran and Rekatsinas, Theodoros and Foulds, James and Getoor, Lise and Liu, Yan},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {871--880},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\VUYU5ADH\\He et al. - 2015 - HawkesTopic A Joint Model for Network Inference and Topic Modeling from Text-Based Cascades.pdf:application/pdf},
}

@misc{cheng_model_2015,
	title = {Model {Selection} for {Topic} {Models} via {Spectral} {Decomposition}},
	url = {http://arxiv.org/abs/1410.6466},
	doi = {10.48550/arXiv.1410.6466},
	abstract = {Topic models have achieved significant successes in analyzing large-scale text corpus. In practical applications, we are always confronted with the challenge of model selection, i.e., how to appropriately set the number of topics. Following recent advances in topic model inference via tensor decomposition, we make a first attempt to provide theoretical analysis on model selection in latent Dirichlet allocation. Under mild conditions, we derive the upper bound and lower bound on the number of topics given a text collection of finite size. Experimental results demonstrate that our bounds are accurate and tight. Furthermore, using Gaussian mixture model as an example, we show that our methodology can be easily generalized to model selection analysis for other latent models.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Cheng, Dehua and He, Xinran and Liu, Yan},
	month = feb,
	year = {2015},
	note = {arXiv:1410.6466 [stat]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
	annote = {Comment: accepted in AISTATS 2015},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\NXYSY5SD\\Cheng et al. - 2015 - Model Selection for Topic Models via Spectral Decomposition.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\QZ4LISZ5\\1410.html:text/html},
}

@misc{cheng_spectral_2015,
	title = {Spectral {Sparsification} of {Random}-{Walk} {Matrix} {Polynomials}},
	url = {http://arxiv.org/abs/1502.03496},
	doi = {10.48550/arXiv.1502.03496},
	abstract = {We consider a fundamental algorithmic question in spectral graph theory: Compute a spectral sparsifier of random-walk matrix-polynomial \$\$L\_{\textbackslash}alpha(G)=D-{\textbackslash}sum\_\{r=1\}{\textasciicircum}d{\textbackslash}alpha\_rD(D{\textasciicircum}\{-1\}A){\textasciicircum}r\$\$ where \$A\$ is the adjacency matrix of a weighted, undirected graph, \$D\$ is the diagonal matrix of weighted degrees, and \${\textbackslash}alpha=({\textbackslash}alpha\_1...{\textbackslash}alpha\_d)\$ are nonnegative coefficients with \${\textbackslash}sum\_\{r=1\}{\textasciicircum}d{\textbackslash}alpha\_r=1\$. Recall that \$D{\textasciicircum}\{-1\}A\$ is the transition matrix of random walks on the graph. The sparsification of \$L\_{\textbackslash}alpha(G)\$ appears to be algorithmically challenging as the matrix power \$(D{\textasciicircum}\{-1\}A){\textasciicircum}r\$ is defined by all paths of length \$r\$, whose precise calculation would be prohibitively expensive. In this paper, we develop the first nearly linear time algorithm for this sparsification problem: For any \$G\$ with \$n\$ vertices and \$m\$ edges, \$d\$ coefficients \${\textbackslash}alpha\$, and \${\textbackslash}epsilon {\textgreater} 0\$, our algorithm runs in time \$O(d{\textasciicircum}2m{\textbackslash}log{\textasciicircum}2n/{\textbackslash}epsilon{\textasciicircum}\{2\})\$ to construct a Laplacian matrix \${\textbackslash}tilde\{L\}=D-{\textbackslash}tilde\{A\}\$ with \$O(n{\textbackslash}log n/{\textbackslash}epsilon{\textasciicircum}\{2\})\$ non-zeros such that \${\textbackslash}tilde\{L\}{\textbackslash}approx\_\{{\textbackslash}epsilon\}L\_{\textbackslash}alpha(G)\$. Matrix polynomials arise in mathematical analysis of matrix functions as well as numerical solutions of matrix equations. Our work is particularly motivated by the algorithmic problems for speeding up the classic Newton's method in applications such as computing the inverse square-root of the precision matrix of a Gaussian random field, as well as computing the \$q\$th-root transition (for \$q{\textbackslash}geq1\$) in a time-reversible Markov model. The key algorithmic step for both applications is the construction of a spectral sparsifier of a constant degree random-walk matrix-polynomials introduced by Newton's method. Our algorithm can also be used to build efficient data structures for effective resistances for multi-step time-reversible Markov models, and we anticipate that it could be useful for other tasks in network analysis.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Cheng, Dehua and Cheng, Yu and Liu, Yan and Peng, Richard and Teng, Shang-Hua},
	month = feb,
	year = {2015},
	note = {arXiv:1502.03496 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Discrete Mathematics, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\8387BFYS\\Cheng et al. - 2015 - Spectral Sparsification of Random-Walk Matrix Polynomials.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\VYL78YFI\\1502.html:text/html},
}

@inproceedings{yu_accelerated_2015,
	title = {Accelerated {Online} {Low} {Rank} {Tensor} {Learning} for {Multivariate} {Spatiotemporal} {Streams}},
	url = {https://proceedings.mlr.press/v37/yua15.html},
	abstract = {Low-rank tensor learning has many applications in machine learning. A series of batch learning algorithms have achieved great successes. However, in many emerging applications, such as climate data analysis, we are confronted with large-scale tensor streams, which poses significant challenges to existing solution in terms of computational costs and limited response time. In this paper, we propose an online accelerated low-rank tensor learning algorithm (ALTO) to solve the problem. At each iteration, we project the current tensor to the subspace of low-rank tensors in order to perform efficient tensor decomposition, then recover the decomposition of the new tensor. By randomly glancing at additional subspaces, we successfully avoid local optima at negligible extra computational cost. We evaluate our method on two tasks in streaming multivariate spatio-temporal analysis: online forecasting and multi-model ensemble, which shows that our method achieves comparable predictive accuracy with significant boost in run time.},
	language = {en},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yu, Rose and Cheng, Dehua and Liu, Yan},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {238--247},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\VVLLK47B\\Yu et al. - 2015 - Accelerated Online Low Rank Tensor Learning for Multivariate Spatiotemporal Streams.pdf:application/pdf},
}

@article{kale_hierarchical_2015,
	series = {{SIAM} {International} {Conference} on {Data} {Mining} 2015, {SDM} 2015},
	title = {Hierarchical active transfer learning: {SIAM} {International} {Conference} on {Data} {Mining} 2015, {SDM} 2015},
	shorttitle = {Hierarchical active transfer learning},
	url = {http://www.scopus.com/inward/record.url?scp=84961943273&partnerID=8YFLogxK},
	doi = {10.1137/1.9781611974010.58},
	abstract = {We describe a unified active transfer learning framework called Hierarchical Active Transfer Learning (HATL). HATL exploits cluster structure shared between different data domains to perform transfer learning by imputing labels for unlabeled target data and to generate effective label queries during active learning. The resulting framework is flexible enough to perform not only adaptive transfer learning and accelerated active learning but also unsupervised and semi-supervised transfer learning. We derive an intuitive and useful upper bound on HATL's error when used to infer labels for unlabeled target points. We also present results on synthetic data that confirm both intuition and our analysis. Finally, we demonstrate HATL's empirical effectiveness on a benchmark data set for sentiment classification.},
	urldate = {2025-01-08},
	journal = {SIAM International Conference on Data Mining 2015, SDM 2015},
	author = {Kale, David and Ghazvininejad, Marjan and Ramakrishna, Anil and He, Jingrui and Liu, Yan},
	editor = {Venkatasubramanian, Suresh and Ye, Jieping},
	year = {2015},
	note = {Publisher: Society for Industrial and Applied Mathematics Publications},
	pages = {514--522},
	annote = {Publisher Copyright:Copyright © SIAM.},
	file = {Full Text:C\:\\Users\\selvam\\Zotero\\storage\\AKQAWIME\\Kale et al. - 2015 - Hierarchical active transfer learning SIAM International Conference on Data Mining 2015, SDM 2015.pdf:application/pdf},
}

@article{yu_glad_2015,
	title = {{GLAD}: {Group} {Anomaly} {Detection} in {Social} {Media} {Analysis}},
	volume = {10},
	issn = {1556-4681},
	shorttitle = {{GLAD}},
	url = {https://doi.org/10.1145/2811268},
	doi = {10.1145/2811268},
	abstract = {Traditional anomaly detection on social media mostly focuses on individual point anomalies while anomalous phenomena usually occur in groups. Therefore, it is valuable to study the collective behavior of individuals and detect group anomalies. Existing group anomaly detection approaches rely on the assumption that the groups are known, which can hardly be true in real world social media applications. In this article, we take a generative approach by proposing a hierarchical Bayes model: Group Latent Anomaly Detection (GLAD) model. GLAD takes both pairwise and point-wise data as input, automatically infers the groups and detects group anomalies simultaneously. To account for the dynamic properties of the social media data, we further generalize GLAD to its dynamic extension d-GLAD. We conduct extensive experiments to evaluate our models on both synthetic and real world datasets. The empirical results demonstrate that our approach is effective and robust in discovering latent groups and detecting group anomalies.},
	number = {2},
	urldate = {2025-01-08},
	journal = {ACM Trans. Knowl. Discov. Data},
	author = {Yu, Rose and He, Xinran and Liu, Yan},
	month = oct,
	year = {2015},
	pages = {18:1--18:22},
}


@inproceedings{bahadori_fast_2014,
	title = {Fast {Multivariate} {Spatio}-temporal {Analysis} via {Low} {Rank} {Tensor} {Learning}},
	volume = {27},
	url = {https://papers.nips.cc/paper_files/paper/2014/hash/aa2a77371374094fe9e0bc1de3f94ed9-Abstract.html},
	abstract = {Accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology, geology, and sociology applications. Existing models usually assume simple inter-dependence among variables, space, and time, and are computationally expensive. We propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis, which can conveniently incorporate different properties in spatio-temporal data, such as spatial clustering and shared structure among variables. We demonstrate how the general framework can be applied to cokriging and forecasting tasks, and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee. We conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.},
	urldate = {2025-01-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bahadori, Mohammad Taha and Yu, Qi (Rose) and Liu, Yan},
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\WNUDYQDS\\Bahadori et al. - 2014 - Fast Multivariate Spatio-temporal Analysis via Low Rank Tensor Learning.pdf:application/pdf},
}

@inproceedings{cheng_fblg_2014,
	address = {New York, NY, USA},
	series = {{KDD} '14},
	title = {{FBLG}: a simple and effective approach for temporal dependence discovery from time series data},
	isbn = {978-1-4503-2956-9},
	shorttitle = {{FBLG}},
	url = {https://doi.org/10.1145/2623330.2623709},
	doi = {10.1145/2623330.2623709},
	abstract = {Discovering temporal dependence structure from multivariate time series has established its importance in many applications. We observe that when we look in reversed order of time, the temporal dependence structure of the time series is usually preserved after switching the roles of cause and effect. Inspired by this observation, we create a new time series by reversing the time stamps of original time series and combine both time series to improve the performance of temporal dependence recovery. We also provide theoretical justification for the proposed algorithm for several existing time series models. We test our approach on both synthetic and real world datasets. The experimental results confirm that this surprisingly simple approach is indeed effective under various circumstances.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Cheng, Dehua and Bahadori, Mohammad Taha and Liu, Yan},
	month = aug,
	year = {2014},
	pages = {382--391},
}

@misc{cheng_scalable_2014,
	title = {Scalable {Parallel} {Factorizations} of {SDD} {Matrices} and {Efficient} {Sampling} for {Gaussian} {Graphical} {Models}},
	url = {http://arxiv.org/abs/1410.5392},
	doi = {10.48550/arXiv.1410.5392},
	abstract = {Motivated by a sampling problem basic to computational statistical inference, we develop a nearly optimal algorithm for a fundamental problem in spectral graph theory and numerical analysis. Given an \$n{\textbackslash}times n\$ SDDM matrix \$\{{\textbackslash}bf {\textbackslash}mathbf\{M\}\}\$, and a constant \$-1 {\textbackslash}leq p {\textbackslash}leq 1\$, our algorithm gives efficient access to a sparse \$n{\textbackslash}times n\$ linear operator \${\textbackslash}tilde\{{\textbackslash}mathbf\{C\}\}\$ such that \$\$\{{\textbackslash}mathbf\{M\}\}{\textasciicircum}\{p\} {\textbackslash}approx {\textbackslash}tilde\{{\textbackslash}mathbf\{C\}\} {\textbackslash}tilde\{{\textbackslash}mathbf\{C\}\}{\textasciicircum}{\textbackslash}top.\$\$ The solution is based on factoring \$\{{\textbackslash}bf {\textbackslash}mathbf\{M\}\}\$ into a product of simple and sparse matrices using squaring and spectral sparsification. For \$\{{\textbackslash}mathbf\{M\}\}\$ with \$m\$ non-zero entries, our algorithm takes work nearly-linear in \$m\$, and polylogarithmic depth on a parallel machine with \$m\$ processors. This gives the first sampling algorithm that only requires nearly linear work and \$n\$ i.i.d. random univariate Gaussian samples to generate i.i.d. random samples for \$n\$-dimensional Gaussian random fields with SDDM precision matrices. For sampling this natural subclass of Gaussian random fields, it is optimal in the randomness and nearly optimal in the work and parallel complexity. In addition, our sampling algorithm can be directly extended to Gaussian random fields with SDD precision matrices.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Cheng, Dehua and Cheng, Yu and Liu, Yan and Peng, Richard and Teng, Shang-Hua},
	month = oct,
	year = {2014},
	note = {arXiv:1410.5392 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Statistics - Computation, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\53EZ5UPL\\Cheng et al. - 2014 - Scalable Parallel Factorizations of SDD Matrices and Efficient Sampling for Gaussian Graphical Model.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\UL2BEMLV\\1410.html:text/html},
}

@article{liu_bayesian_2014,
	title = {Bayesian regularization via graph {Laplacian}},
	copyright = {© Copyright IBM Corp. 2021},
	issn = {19360975},
	url = {https://research.ibm.com/publications/bayesian-regularization-via-graph-laplacian},
	doi = {10.1214/14-BA860},
	abstract = {Regularization plays a critical role in modern statistical research, especially in high-dimensional variable selection problems. Existing Bayesian methods usually assume independence between variables a priori. In this article, we propose a novel Bayesian approach, which explicitly models the dependence structure through a graph Laplacian matrix. We also generalize the graph Laplacian to allow both positively and negatively correlated variables. A prior distribution for the graph Laplacian is then proposed, which allows conjugacy and thereby greatly simplifies the computation. We show that the proposed Bayesian model leads to proper posterior distribution. Connection is made between our method and some existing regularization methods, such as Elastic Net, Lasso, Octagonal Shrinkage and Clustering Algorithm for Regression (OSCAR) and Ridge regression. An efficient Markov Chain Monte Carlo method based on parameter augmentation is developed for posterior computation. Finally, we demonstrate the method through several simulation studies and an application on a real data set involving key performance indicators of electronics companies.},
	language = {en-US},
	urldate = {2025-01-08},
	journal = {Bayesian Analysis},
	author = {Liu, Fei and Chakraborty, Sounak and Li, Fan and Liu, Yan and Lozano, Aurelie C.},
	month = jan,
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\IQWMUGN3\\Liu et al. - 2014 - Bayesian regularization via graph Laplacian.pdf:application/pdf},
}

@inproceedings{cheng_parallel_2014,
	address = {New York, NY, USA},
	series = {{KDD} '14},
	title = {Parallel gibbs sampling for hierarchical dirichlet processes via gamma processes equivalence},
	isbn = {978-1-4503-2956-9},
	url = {https://doi.org/10.1145/2623330.2623708},
	doi = {10.1145/2623330.2623708},
	abstract = {The hierarchical Dirichlet process (HDP) is an intuitive and elegant technique to model data with latent groups. However, it has not been widely used for practical applications due to the high computational costs associated with inference. In this paper, we propose an effective parallel Gibbs sampling algorithm for HDP by exploring its connections with the gamma-gamma-Poisson process. Specifically, we develop a novel framework that combines bootstrap and Reversible Jump MCMC algorithm to enable parallel variable updates. We also provide theoretical convergence analysis based on Gibbs sampling with asynchronous variable updates. Experiment results on both synthetic datasets and two large-scale text collections show that our algorithm can achieve considerable speedup as well as better inference accuracy for HDP compared with existing parallel sampling algorithms.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Cheng, Dehua and Liu, Yan},
	month = aug,
	year = {2014},
	pages = {562--571},
}

@inproceedings{chang_ups_2014,
	title = {Ups and {Downs} in {Buzzes}: {Life} {Cycle} {Modeling} for {Temporal} {Pattern} {Discovery}},
	shorttitle = {Ups and {Downs} in {Buzzes}},
	url = {https://ieeexplore.ieee.org/document/7023395},
	doi = {10.1109/ICDM.2014.28},
	abstract = {In social media analysis, one critical task is detecting burst of topics or buzz, which is reflected by extremely frequent mentions of certain key words in a short time interval. Detecting buzz not only provides useful insights into the information propagation mechanism, but also plays an essential role in preventing malicious rumors. However, buzz modeling is a challenging task because a buzz time-series usually exhibits sudden spikes and heavy tails, which fails most existing time-series models. To deal with buzz time-series sequences, we propose a novel time-series modeling approach which captures the rise and fade temporal patterns via Product Life Cycle (PLC) models, a classical concept in economics. More specifically, we propose a mixture of PLC models to capture the multiple peaks in buzz time-series and furthermore develop a probabilistic graphical model (K-MPLC) to automatically discover inherent life cycle patterns within a collection of buzzes. Our experiment results show that our proposed method significantly outperforms existing state-of-the-art approaches on buzzes clustering.},
	urldate = {2025-01-08},
	booktitle = {2014 {IEEE} {International} {Conference} on {Data} {Mining}},
	author = {Chang, Yi and Yamada, Makoto and Ortega, Antonio and Liu, Yan},
	month = dec,
	year = {2014},
	note = {ISSN: 2374-8486},
	keywords = {Biological system modeling, Clustering algorithms, Graphical models, Measurement, Media, Optimization, Probabilistic logic, Time-Series Clustering, Time-Series Modeling},
	pages = {749--754},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\\WDXUWV5G\\7023395.html:text/html},
}

@incollection{he_linking_2014,
	series = {Proceedings},
	title = {Linking {Heterogeneous} {Input} {Spaces} with {Pivots} for {Multi}-{Task} {Learning}},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611973440.21},
	abstract = {Most existing works on multi-task learning (MTL) assume the same input space for different tasks. In this paper, we address a general setting where different tasks have heterogeneous input spaces. This setting has a lot of potential applications, yet it poses new algorithmic challenges - how can we link seemingly uncorrelated tasks to mutually boost their learning performance?Our key observation is that in many real applications, there might exist some correspondence among the inputs of different tasks, which is referred to as pivots. For such applications, we first propose a learning scheme for multiple tasks and analyze its generalization performance. Then we focus on the problems where only a limited number of the pivots are available, and propose a general framework to leverage the pivot information. The idea is to map the heterogeneous input spaces to a common space, and construct a single prediction model in this space for all the tasks. We further propose an effective optimization algorithm to find both the mappings and the prediction model. Experimental results demonstrate its effectiveness, especially with very limited number of pivots.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the 2014 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {He, Jingrui and Liu, Yan and Yang, Qiang},
	month = apr,
	year = {2014},
	doi = {10.1137/1.9781611973440.21},
	pages = {181--189},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\MVADXB8N\\He et al. - 2014 - Linking Heterogeneous Input Spaces with Pivots for Multi-Task Learning.pdf:application/pdf},
}

@article{kale_computational_2014,
	title = {Computational discovery of physiomes in critically ill children using deep learning},
	abstract = {Critical illness is dynamic and complex, but physicians often diagnose and treat based on parsimonious, static sets of symptoms and signs. For example, the Berlin definition of acute respiratory distress syndrome uses static criteria,
including a threshold on the partial pressure of oxygen. 1 The Pediatric Risk of Mortality (PRISM III) score considers only the extreme values of a handful of variables during a 12-24 hour period. 2 However, the increasing volume of digital health data offers an opportunity to use computational methods to learn richer descriptors of illness (physiomes 3 or phenomes 4 ) that incorporate temporal dynamics and more variables. In this work, we use deep neural networks to mine
patterns from multivariate clinical time series. We apply this to a large pediatric intensive care unit (PICU) database from Children's Hospital Los Angeles (CHLA) 3
to learn patterns that are associated with known critical illnesses.},
	language = {en},
	author = {Kale, David C and Che, Zhengping and Liu, Yan},
	year = {2014},
	file = {PDF:C\:\\Users\\selvam\\Zotero\\storage\\VQ6STZWQ\\Kale et al. - Computational discovery of physiomes in critically ill children using deep learning.pdf:application/pdf},
}

@inproceedings{kale_examination_2014,
	title = {An {Examination} of {Multivariate} {Time} {Series} {Hashing} with {Applications} to {Health} {Care}},
	url = {https://ieeexplore.ieee.org/abstract/document/7023343},
	doi = {10.1109/ICDM.2014.153},
	abstract = {As large-scale multivariate time series data become increasingly common in application domains, such as health care and traffic analysis, researchers are challenged to build efficient tools to analyze it and provide useful insights. Similarity search, as a basic operator for many machine learning and data mining algorithms, has been extensively studied before, leading to several efficient solutions. However, similarity search for multivariate time series data is intrinsically challenging because (1) there is no conclusive agreement on what is a good similarity metric for multivariate time series data and (2) calculating similarity scores between two time series is often computationally expensive. In this paper, we address this problem by applying a generalized hashing framework, namely kernelized locality sensitive hashing, to accelerate time series similarity search with a series of representative similarity metrics. Experiment results on three large-scale clinical data sets demonstrate the effectiveness of the proposed approach.},
	urldate = {2025-01-08},
	booktitle = {2014 {IEEE} {International} {Conference} on {Data} {Mining}},
	author = {Kale, David C. and Gong, Dian and Che, Zhengping and Liu, Yan and Medioni, Gerard and Wetzel, Randall and Ross, Patrick},
	month = dec,
	year = {2014},
	note = {ISSN: 2374-8486},
	keywords = {alignment, Databases, dynamic time warping, Euclidean distance, hashing, Kernel, kernel methods, nearest neighbor, search, similarity, Time measurement, time series, Time series analysis, Vectors},
	pages = {260--269},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\\IN3KLCV2\\7023343.html:text/html},
}

@inproceedings{seo2017graph,
  title={Graph convolutional autoencoder with recurrent neural networks for spatiotemporal forecasting},
  author={Seo, Sungyong and Mohegh, Arash and Ban-Weiss, George and Liu, Yan},
  booktitle={7th International workshop on clmate informatics},
  year={2017}
}

@article{cheng2014analyzing,
  title={Analyzing the Number of Latent Topics via Spectral Decomposition},
  author={Cheng, Dehua and He, Xinran and Liu, Yan},
  journal={stat},
  volume={1050},
  pages={23},
  year={2014},
  publisher={Citeseer}
}


@inproceedings{kale_accelerating_2013,
	title = {Accelerating {Active} {Learning} with {Transfer} {Learning}},
	url = {https://ieeexplore.ieee.org/abstract/document/6729602},
	doi = {10.1109/ICDM.2013.160},
	abstract = {Active learning, transfer learning, and related techniques are unified by a core theme: efficient and effective use of available data. Active learning offers scalable solutions for building effective supervised learning models while minimizing annotation effort. Transfer learning utilizes existing labeled data from one task to help learning related tasks for which limited labeled data are available. There has been limited research, however, on how to combine these two techniques. In this paper, we present a simple and principled transfer active learning framework that leverages pre-existing labeled data from related tasks to improve the performance of an active learner. We derive an intuitive bound on generalization error for the classifiers learned by this algorithm that provides insight into the algorithm's behavior and the problem in general. Experimental results using several well-known transfer learning data sets confirm our theoretical analysis and demonstrate the effectiveness of our approach.},
	urldate = {2025-01-09},
	booktitle = {2013 {IEEE} 13th {International} {Conference} on {Data} {Mining}},
	author = {Kale, David and Liu, Yan},
	month = dec,
	year = {2013},
	note = {ISSN: 2374-8486},
	keywords = {Acceleration, Machine Learning, Training, Active Learning, Algorithm design and analysis, Labeling, Learning Theory, Query processing, Supervised learning, Transfer Learning, Upper bound},
	pages = {1085--1090},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\\H7YRSIX3\\6729602.html:text/html},
}

@inproceedings{bahadori_fast_2013,
	address = {New York, NY, USA},
	series = {{KDD} '13},
	title = {Fast structure learning in generalized stochastic processes with latent factors},
	isbn = {978-1-4503-2174-7},
	url = {https://doi.org/10.1145/2487575.2487578},
	doi = {10.1145/2487575.2487578},
	abstract = {Understanding and quantifying the impact of unobserved processes is one of the major challenges of analyzing multivariate time series data. In this paper, we analyze a flexible stochastic process model, the generalized linear auto-regressive process (GLARP) and identify the conditions under which the impact of hidden variables appears as an additive term to the evolution matrix estimated with the maximum likelihood. In particular, we examine three examples, including two popular models for count data, i.e, Poisson and Conwey-Maxwell Poisson vector auto-regressive processes, and one powerful model for extreme value data, i.e., Gumbel vector auto-regressive processes. We demonstrate that the impact of hidden factors can be separated out via convex optimization in these three models. We also propose a fast greedy algorithm based on the selection of composite atoms in each iteration and provide a performance guarantee for it. Experiments on two synthetic datasets, one social network dataset and one climatology dataset demonstrate the the superior performance of our proposed models.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 19th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Bahadori, Mohammad Taha and Liu, Yan and Xing, Eric P.},
	month = aug,
	year = {2013},
	pages = {284--292},
}

@incollection{bahadori_examination_2013,
	series = {Proceedings},
	title = {An {Examination} of {Practical} {Granger} {Causality} {Inference}},
	isbn = {978-1-61197-262-7},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972832.52},
	abstract = {Learning temporal causal structures among multiple time series is one of the major tasks in mining time series data. Granger causality is one of the most popular techniques in uncovering the temporal dependencies among time series; however it faces two main challenges: (i) the spurious effect of unobserved time series and (ii) the computational challenges in high dimensional settings. In this paper, we utilize the confounder path delays to find a subset of time series that via conditioning on them we are able to cancel out the spurious confounder effects. After study of consistency of different Granger causality techniques, we propose Copula-Granger and show that while it is consistent in high dimensions, it can efficiently capture non-linearity in the data. Extensive experiments on a synthetic and a social networking dataset confirm our theoretical results.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 2013 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Bahadori, Mohammad Taha and Liu, Yan},
	month = may,
	year = {2013},
	doi = {10.1137/1.9781611972832.52},
	pages = {467--475},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\JLFNHBJK\\Bahadori and Liu - 2013 - An Examination of Practical Granger Causality Inference.pdf:application/pdf},
}

@misc{purushotham_collaborative_2012,
	title = {Collaborative {Topic} {Regression} with {Social} {Matrix} {Factorization} for {Recommendation} {Systems}},
	url = {http://arxiv.org/abs/1206.4684},
	doi = {10.48550/arXiv.1206.4684},
	abstract = {Social network websites, such as Facebook, YouTube, Lastfm etc, have become a popular platform for users to connect with each other and share content or opinions. They provide rich information for us to study the influence of user's social circle in their decision process. In this paper, we are interested in examining the effectiveness of social network information to predict the user's ratings of items. We propose a novel hierarchical Bayesian model which jointly incorporates topic modeling and probabilistic matrix factorization of social networks. A major advantage of our model is to automatically infer useful latent topics and social information as well as their importance to collaborative filtering from the training data. Empirical experiments on two large-scale datasets show that our algorithm provides a more effective recommendation system than the state-of-the art approaches. Our results reveal interesting insight that the social circles have more influence on people's decisions about the usefulness of information (e.g., bookmarking preference on Delicious) than personal taste (e.g., music preference on Lastfm). We also examine and discuss solutions on potential information leak in many recommendation systems that utilize social information.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Purushotham, Sanjay and Liu, Yan and Kuo, C.-C. Jay},
	month = jun,
	year = {2012},
	note = {arXiv:1206.4684 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Social and Information Networks},
	annote = {Comment: ICML2012},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\RR2T68PQ\\Purushotham et al. - 2012 - Collaborative Topic Regression with Social Matrix Factorization for Recommendation Systems.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\5HG8EZRZ\\1206.html:text/html},
}

@misc{liu_sparse-gev_2012,
	title = {Sparse-{GEV}: {Sparse} {Latent} {Space} {Model} for {Multivariate} {Extreme} {Value} {Time} {Serie} {Modeling}},
	shorttitle = {Sparse-{GEV}},
	url = {http://arxiv.org/abs/1206.4685},
	doi = {10.48550/arXiv.1206.4685},
	abstract = {In many applications of time series models, such as climate analysis and social media analysis, we are often interested in extreme events, such as heatwave, wind gust, and burst of topics. These time series data usually exhibit a heavy-tailed distribution rather than a Gaussian distribution. This poses great challenges to existing approaches due to the significantly different assumptions on the data distributions and the lack of sufficient past data on extreme events. In this paper, we propose the Sparse-GEV model, a latent state model based on the theory of extreme value modeling to automatically learn sparse temporal dependence and make predictions. Our model is theoretically significant because it is among the first models to learn sparse temporal dependencies among multivariate extreme value time series. We demonstrate the superior performance of our algorithm to the state-of-art methods, including Granger causality, copula approach, and transfer entropy, on one synthetic dataset, one climate dataset and two Twitter datasets.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Liu, Yan and Bahadori, Taha and Li, Hongfei},
	month = jun,
	year = {2012},
	note = {arXiv:1206.4685 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Applications, Statistics - Methodology},
	annote = {Comment: ICML2012},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\JWQTFMLV\\Liu et al. - 2012 - Sparse-GEV Sparse Latent Space Model for Multivariate Extreme Value Time Serie Modeling.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\DSMSF2YX\\1206.html:text/html},
}

@misc{kang_transfer_2013,
	title = {Transfer {Topic} {Modeling} with {Ease} and {Scalability}},
	url = {http://arxiv.org/abs/1301.5686},
	doi = {10.48550/arXiv.1301.5686},
	abstract = {The increasing volume of short texts generated on social media sites, such as Twitter or Facebook, creates a great demand for effective and efficient topic modeling approaches. While latent Dirichlet allocation (LDA) can be applied, it is not optimal due to its weakness in handling short texts with fast-changing topics and scalability concerns. In this paper, we propose a transfer learning approach that utilizes abundant labeled documents from other domains (such as Yahoo! News or Wikipedia) to improve topic modeling, with better model fitting and result interpretation. Specifically, we develop Transfer Hierarchical LDA (thLDA) model, which incorporates the label information from other domains via informative priors. In addition, we develop a parallel implementation of our model for large-scale applications. We demonstrate the effectiveness of our thLDA model on both a microblogging dataset and standard text collections including AP and RCV1 datasets.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Kang, Jeon-Hyung and Ma, Jun and Liu, Yan},
	month = jan,
	year = {2013},
	note = {arXiv:1301.5686 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 2012 SIAM International Conference on Data Mining (SDM12) Pages: \{564-575\}},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\QUYFARHG\\Kang et al. - 2013 - Transfer Topic Modeling with Ease and Scalability.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\5P6LNF6D\\1301.html:text/html},
}

@inproceedings{qiu_granger_2012,
	title = {Granger {Causality} for {Time}-{Series} {Anomaly} {Detection}},
	url = {https://ieeexplore.ieee.org/document/6413806},
	doi = {10.1109/ICDM.2012.73},
	abstract = {Recent developments in industrial systems provide us with a large amount of time series data from sensors, logs, system settings and physical measurements, etc. These data are extremely valuable for providing insights about the complex systems and could be used to detect anomalies at early stages. However, the special characteristics of these time series data, such as high dimensions and complex dependencies between variables, as well as its massive volume, pose great challenges to existing anomaly detection algorithms. In this paper, we propose Granger graphical models as an effective and scalable approach for anomaly detection whose results can be readily interpreted. Specifically, Granger graphical models are a family of graphical models that exploit the temporal dependencies between variables by applying L1-regularized learning to Granger causality. Our goal is to efficiently compute a robust "correlation anomaly" score for each variable via Granger graphical models that can provide insights on the possible reasons of anomalies. We evaluate the effectiveness of our proposed algorithms on both synthetic and application datasets. The results show the proposed algorithm achieves significantly better performance than other baseline algorithms and is scalable for large-scale applications.},
	urldate = {2025-01-09},
	booktitle = {2012 {IEEE} 12th {International} {Conference} on {Data} {Mining}},
	author = {Qiu, Huida and Liu, Yan and Subrahmanya, Niranjan A. and Li, Weichang},
	month = dec,
	year = {2012},
	note = {ISSN: 2374-8486},
	keywords = {Algorithm design and analysis, Anomaly Detection, Data models, Graphical models, Optimization, Principal component analysis, Stochastic processes, Time series analysis, Time Series Analysis},
	pages = {1074--1079},
}

@incollection{bahadori_granger_2012,
	series = {Proceedings},
	title = {Granger {Causality} {Analysis} in {Irregular} {Time} {Series}},
	isbn = {978-1-61197-232-0},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972825.57},
	abstract = {Learning temporal causal structures between time series is one of the key tools for analyzing time series data. In many real-world applications, we are confronted with Irregular Time Series, whose observations are not sampled at equally-spaced time stamps. The irregularity in sampling intervals violates the basic assumptions behind many models for structure learning. In this paper, we propose a nonparametric generalization of the Granger graphical models called Generalized Lasso Granger (GLG) to uncover the temporal dependencies from irregular time series. Via theoretical analysis and extensive experiments, we verify the effectiveness of our model. Furthermore, we apply GLG to the application dataset of δ18O isotope of Oxygen records in Asia and achieve promising results to discover the moisture transportation patterns in a 800-year period.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 2012 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Bahadori, Mohammad Taha and Liu, Yan},
	month = apr,
	year = {2012},
	doi = {10.1137/1.9781611972825.57},
	pages = {660--671},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\GTQBJM84\\Bahadori and Liu - 2012 - Granger Causality Analysis in Irregular Time Series.pdf:application/pdf},
}

@inproceedings{zhou_community_2012,
	address = {New York, NY, USA},
	series = {{KDD} '12},
	title = {Community discovery and profiling with social messages},
	isbn = {978-1-4503-1462-6},
	url = {https://doi.org/10.1145/2339530.2339593},
	doi = {10.1145/2339530.2339593},
	abstract = {Discovering communities from social media and collaboration systems has been of great interest in recent years. Existing work show prospects of modeling contents and social links, aiming at discovering social communities, whose definition varies by application. We believe that a community depends not only on the group of people who actively participate, but also the topics they communicate about or collaborate on. This is especially true for workplace email communications. Within an organization, it is not uncommon that employees multifunction, and groups of employees collaborate on multiple projects at the same time. In this paper, we aim to automatically discovering and profiling users' communities by taking into account both the contacts and the topics. More specifically, we propose a community profiling model called COCOMP, where the communities labels are latent, and each social document corresponds to an information sharing activity among the most probable community members regarding the most relevant community issues. Experiment results on several social communication datasets, including emails and Twitter messages, demonstrate that the model can discover users' communities effectively, and provide concrete semantics.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 18th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Wenjun and Jin, Hongxia and Liu, Yan},
	month = aug,
	year = {2012},
	pages = {388--396},
}

@article{zhang_transfer_2011,
	title = {Transfer {Latent} {Semantic} {Learning}: {Microblog} {Mining} with {Less} {Supervision}},
	volume = {25},
	copyright = {Copyright (c) 2021 Proceedings of the AAAI Conference on Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Transfer {Latent} {Semantic} {Learning}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7916},
	doi = {10.1609/aaai.v25i1.7916},
	abstract = {The increasing volume of information generated on micro-blogging sites such as Twitter raises several challenges to traditional text mining techniques. First, most texts from those sites are abbreviated due to the constraints of limited characters in one post; second, the input usually comes in streams of large-volumes. Therefore, it is of significant importance to develop effective and efficient representations of abbreviated texts for better filtering and mining. In this paper, we introduce a novel transfer learning approach, namely transfer latent semantic learning, that utilizes a large number of related tagged documents with rich information from other sources (source domain) to help build a robust latent semantic space for the abbreviated texts (target domain). This is achieved by simultaneously minimizing the document reconstruction error and the classification error of the labeled examples from the source domain by building a classifier with hinge loss in the latent semantic space. We demonstrate the effectiveness of our method by applying them to the task of classifying and tagging abbreviated texts. Experimental results on both synthetic datasets and real application datasets, including Reuters-21578 and Twitter data, suggest substantial improvements using our approach over existing ones.},
	language = {en},
	number = {1},
	urldate = {2025-01-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Dan and Liu, Yan and Lawrence, Richard and Chenthamarakshan, Vijil},
	month = aug,
	year = {2011},
	note = {Number: 1},
	pages = {561--566},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\QZEPHBFN\\Zhang et al. - 2011 - Transfer Latent Semantic Learning Microblog Mining with Less Supervision.pdf:application/pdf},
}

@article{chang_detecting_2011,
	title = {Detecting {Multilingual} and {Multi}-{Regional} {Query} {Intent} in {Web} {Search}},
	volume = {25},
	copyright = {Copyright (c) 2021 Proceedings of the AAAI Conference on Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/8074},
	doi = {10.1609/aaai.v25i1.8074},
	abstract = {With rapid growth of commercial search engines, detecting multilingual and multi-regional intent underlying search queries becomes a critical challenge to serve international users with diverse language and region requirements. We introduce a query intent probabilistic model, whose input is the number of clicks on documents from different regions and in different language, while the output of this model is a smoothed probabilistic distribution of multilingual and multi-regional query intent. Based on an editorial test to evaluate the accuracy of the intent classifier, our probabilistic model could improve the accuracy of multilingual intent detection for 15\%, and improve multi-regional intent detection for 18\%. To improve web search quality, we propose a set of new ranking features to combine multilingual and multi-regional query intent with document language/region attributes, and apply different approaches in integrating intent information to directly affect ranking. The experiments show that the novel features could provide 2.31\% NDCG@1 improvement and 1.81\% NDCG@5 improvement.},
	language = {en},
	number = {1},
	urldate = {2025-01-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chang, Yi and Zhang, Ruiqiang and Reddy, Srihari and Liu, Yan},
	month = aug,
	year = {2011},
	note = {Number: 1},
	pages = {1134--1139},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\RVXRKVEY\\Chang et al. - 2011 - Detecting Multilingual and Multi-Regional Query Intent in Web Search.pdf:application/pdf},
}

@inproceedings{hauder_framework_2011,
	title = {A {Framework} for {Efficient} {Data} {Analytics} through {Automatic} {Configuration} and {Customization} of {Scientific} {Workflows}},
	url = {https://ieeexplore.ieee.org/document/6123302},
	doi = {10.1109/eScience.2011.59},
	abstract = {Data analytics involves choosing between many different algorithms and experimenting with possible combinations of those algorithms. Existing approaches however do not support scientists with the laborious tasks of exploring the design space of computational experiments. We have developed a framework to assist scientists with data analysis tasks in particular machine learning and data mining. It takes advantage of the unique capabilities of the Wings workflow system to reason about semantic constraints. We show how the framework can rule out invalid workflows and help scientists to explore the design space. We demonstrate our system in the domain of text analytics, and outline the benefits of our approach.},
	urldate = {2025-01-09},
	booktitle = {2011 {IEEE} {Seventh} {International} {Conference} on {eScience}},
	author = {Hauder, Matheus and Gil, Yolanda and Liu, Yan},
	month = dec,
	year = {2011},
	keywords = {Algorithm design and analysis, Clustering algorithms, Correlation, Data Analytics, Machine learning algorithms, Prediction algorithms, Scientific Workflows, Software, Software algorithms},
	pages = {379--386},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\\5ZIXB94H\\6123302.html:text/html},
}

@inproceedings{zhang_multi-view_2011,
	address = {New York, NY, USA},
	series = {{KDD} '11},
	title = {Multi-view transfer learning with a large margin approach},
	isbn = {978-1-4503-0813-7},
	url = {https://doi.org/10.1145/2020408.2020593},
	doi = {10.1145/2020408.2020593},
	abstract = {Transfer learning has been proposed to address the problem of scarcity of labeled data in the target domain by leveraging the data from the source domain. In many real world applications, data is often represented from different perspectives, which correspond to multiple views. For example, a web page can be described by its contents and its associated links. However, most existing transfer learning methods fail to capture the multi-view \{nature\}, and might not be best suited for such applications.To better leverage both the labeled data from the source domain and the features from different views, \{this paper proposes\} a general framework: Multi-View Transfer Learning with a Large Margin Approach (MVTL-LM). On one hand, labeled data from the source domain is effectively utilized to construct a large margin classifier; on the other hand, the data from both domains is employed to impose consistencies among multiple views. As an instantiation of this framework, we propose an efficient optimization method, which is guaranteed to converge to ε precision in O(1/ε) steps. Furthermore, we analyze its error bound, which improves over existing results of related methods. An extensive set of experiments are conducted to demonstrate the advantages of our proposed method over state-of-the-art techniques.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 17th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Dan and He, Jingrui and Liu, Yan and Si, Luo and Lawrence, Richard},
	month = aug,
	year = {2011},
	pages = {1208--1216},
}

@inproceedings{bahadori_learning_2011,
	address = {USA},
	series = {{ICDM} '11},
	title = {Learning with {Minimum} {Supervision}: {A} {General} {Framework} for {Transductive} {Transfer} {Learning}},
	isbn = {978-0-7695-4408-3},
	shorttitle = {Learning with {Minimum} {Supervision}},
	url = {https://doi.org/10.1109/ICDM.2011.92},
	doi = {10.1109/ICDM.2011.92},
	abstract = {Transductive transfer learning is one special type of transfer learning problem, in which abundant labeled examples are available in the source domain and only {\textbackslash}textit\{unlabeled\} examples are available in the target domain. It easily finds applications in spam filtering, microblogging mining and so on. In this paper, we propose a general framework to solve the problem by mapping the input features in both the source domain and target domain into a shared latent space and simultaneously minimizing the feature reconstruction loss and prediction loss. We develop one specific example of the framework, namely latent large-margin transductive transfer learning (LATTL) algorithm, and analyze its theoretic bound of classification loss via Rademacher complexity. We also provide a unified view of several popular transfer learning algorithms under our framework. Experiment results on one synthetic dataset and three application datasets demonstrate the advantages of the proposed algorithm over the other state-of-the-art ones.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 2011 {IEEE} 11th {International} {Conference} on {Data} {Mining}},
	publisher = {IEEE Computer Society},
	author = {Bahadori, Mohammad Taha and Liu, Yan and Zhang, Dan},
	month = dec,
	year = {2011},
	pages = {61--70},
}

@inproceedings{zhang_multiple_2011,
	title = {Multiple {Instance} {Learning} on {Structured} {Data}},
	volume = {24},
	url = {https://papers.nips.cc/paper_files/paper/2011/hash/ec5decca5ed3d6b8079e2e7e7bacc9f2-Abstract.html},
	abstract = {Most existing Multiple-Instance Learning (MIL) algorithms  assume data instances and/or data bags are independently and  identically distributed. But there often exists rich additional  dependency/structure information between instances/bags within many  applications of MIL. Ignoring this structure information limits the  performance of existing MIL algorithms. This paper explores the  research problem as multiple instance learning on structured  data (MILSD) and formulates a novel framework that considers  additional structure information. In particular, an effective and  efficient optimization algorithm has been proposed to solve the  original non-convex optimization problem by using a combination of  Concave-Convex Constraint Programming (CCCP) method and an adapted  Cutting Plane method, which deals with two sets of constraints caused  by learning on  instances within individual bags and learning on  structured data. Our method has the nice convergence property,  with specified precision on each set of constraints. Experimental  results on three different applications, i.e., webpage  classification, market targeting, and protein fold identification,  clearly demonstrate the advantages of the proposed method over  state-of-the-art methods.},
	urldate = {2025-01-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Dan and Liu, Yan and Si, Luo and Zhang, Jian and Lawrence, Richard},
	year = {2011},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\QL6RS9SC\\Zhang et al. - 2011 - Multiple Instance Learning on Structured Data.pdf:application/pdf},
}

@inproceedings{liu_latent_2011,
	address = {New York, NY, USA},
	series = {{KDD} '11},
	title = {Latent graphical models for quantifying and predicting patent quality},
	isbn = {978-1-4503-0813-7},
	url = {https://doi.org/10.1145/2020408.2020586},
	doi = {10.1145/2020408.2020586},
	abstract = {The number of patents filed each year has increased dramatically in recent years, raising concerns that patents of questionable validity are restricting the issuance of truly innovative patents. For this reason, there is a strong demand to develop an objective model to quantify patent quality and characterize the attributes that lead to higher-quality patents. In this paper, we develop a latent graphical model to infer patent quality from related measurements. In addition, we extract advanced lexical features via natural language processing techniques to capture the quality measures such as clarity of claims, originality, and importance of cited prior art. We demonstrate the effectiveness of our approach by validating its predictions with previous court decisions of litigated patents.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 17th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yan and Hseuh, Pei-yun and Lawrence, Rick and Meliksetian, Steve and Perlich, Claudia and Veen, Alejandro},
	month = aug,
	year = {2011},
	pages = {1145--1153},
}

@article{liu_temporal_2011,
	title = {Temporal graphical models for cross-species gene regulatory network discovery},
	volume = {9},
	issn = {1757-6334},
	doi = {10.1142/s0219720011005525},
	abstract = {Many genes and biological processes function in similar ways across different species. Cross-species gene expression analysis, as a powerful tool to characterize the dynamical properties of the cell, has found a number of applications, such as identifying a conserved core set of cell cycle genes. However, to the best of our knowledge, there is limited effort on developing appropriate techniques to capture the causality relations between genes from time-series microarray data across species. In this paper, we present hidden Markov random field regression with L(1) penalty to uncover the regulatory network structure for different species. The algorithm provides a framework for sharing information across species via hidden component graphs and is able to incorporate domain knowledge across species easily. We demonstrate our method on two synthetic datasets and apply it to discover causal graphs from innate immune response data.},
	language = {eng},
	number = {2},
	journal = {Journal of Bioinformatics and Computational Biology},
	author = {Liu, Yan and Niculescu-Mizil, Alexandru and Lozano, Aurélie and Lu, Yong},
	month = apr,
	year = {2011},
	pmid = {21523930},
	keywords = {Algorithms, Animals, Computational Biology, Computer Graphics, Computer Simulation, Gene Regulatory Networks, Humans, Immunogenetic Phenomena, Markov Chains, Mice, Microarray Analysis, Models, Genetic, Regression Analysis, Species Specificity},
	pages = {231--250},
}

@inproceedings{zhang_serendipitous_2011,
	address = {New York, NY, USA},
	series = {{KDD} '11},
	title = {Serendipitous learning: learning beyond the predefined label space},
	isbn = {978-1-4503-0813-7},
	shorttitle = {Serendipitous learning},
	url = {https://doi.org/10.1145/2020408.2020608},
	doi = {10.1145/2020408.2020608},
	abstract = {Most traditional supervised learning methods are developed to learn a model from labeled examples and use this model to classify the unlabeled ones into the same label space predefined by the models. However, in many real world applications, the label spaces for both the labeled/training and unlabeled/testing examples can be different. To solve this problem, this paper proposes a novel notion of Serendipitous Learning (SL), which is defined to address the learning scenarios in which the label space can be enlarged during the testing phase. In particular, a large margin approach is proposed to solve SL. The basic idea is to leverage the knowledge in the labeled examples to help identify novel/unknown classes, and the large margin formulation is proposed to incorporate both the classification loss on the examples within the known categories, as well as the clustering loss on the examples in unknown categories. An efficient optimization algorithm based on CCCP and the bundle method is proposed to solve the optimization problem of the large margin formulation of SL. Moreover, an efficient online learning method is proposed to address the issue of large scale data in online learning scenario, which has been shown to have a guaranteed learning regret. An extensive set of experimental results on two synthetic datasets and two datasets from real world applications demonstrate the advantages of the proposed method over several other baseline algorithms. One limitation of the proposed method is that the number of unknown classes is given in advance. It may be possible to remove this constraint if we model it by using a non-parametric way. We also plan to do experiments on more real world applications in the future.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 17th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Dan and Liu, Yan and Si, Luo},
	month = aug,
	year = {2011},
	pages = {1343--1351},
}

@inproceedings{wang_collaboration_2010,
	address = {New York, NY, USA},
	series = {{CIKM} '10},
	title = {Collaboration analytics: mining work patterns from collaboration activities},
	isbn = {978-1-4503-0099-5},
	shorttitle = {Collaboration analytics},
	url = {https://doi.org/10.1145/1871437.1871748},
	doi = {10.1145/1871437.1871748},
	abstract = {People are increasingly using more and more social softwares, generating flooding communications. User analytics may be performed to mine a person's activities on different social systems and extract patterns, be it interest patterns, social patterns, or work patterns. Such patterns may benefit both the individuals and the organizations the users associated with, as the information is valuable in numerous tasks, including recommendation, evaluation, management, and so on. In this article, we present an actionable solution of user analytics, namely collaboration analytics, by focusing on mining a person's work patterns from her collaboration activities. Our solution effectively makes use of a user's heterogeneous data collected from various collaboration tools to derive an integrated description of the user's collaborative work. A number of ``work areas'', each of which contains its work topics and people involved, are generated for every user. The challenges we face include the clustering of items with short texts and prioritizing/weighting data items based on importance/relevance. Our solutions to those issues will be described in this article. In particular, we mine users' background information from various types of data and use such information to enrich the semantics of the short texts contained in the activity instances on collaboration tools before clustering those instances into work areas. Finally, we have developed a prototype of our collaboration analytics solution and evaluated it with real-world data and people.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 19th {ACM} international conference on {Information} and knowledge management},
	publisher = {Association for Computing Machinery},
	author = {Wang, Qihua and Jin, Hongxia and Liu, Yan},
	month = oct,
	year = {2010},
	pages = {1861--1864},
}

@article{rosset_medical_2010,
	title = {Medical data mining: insights from winning two competitions},
	volume = {20},
	issn = {1573-756X},
	shorttitle = {Medical data mining},
	url = {https://doi.org/10.1007/s10618-009-0158-x},
	doi = {10.1007/s10618-009-0158-x},
	abstract = {Two major data mining competitions in 2008 presented challenges in medical domains: KDD Cup 2008, which concerned cancer detection from mammography data; and Informs Data Mining Challenge 2008, dealing with diagnosis of pneumonia based on patient information from hospital files. Our team won both of these competitions, and in this paper we share our lessons learned and insights. We emphasize the aspects that pertain to the general practice and methodology of medical data mining, rather than to the specifics of each modeling competition. We concentrate on three topics: information leakage, its effect on competitions and proof-of-concept projects; consideration of real-life model performance measures in model construction and evaluation; and relational learning approaches to medical data mining tasks.},
	language = {en},
	number = {3},
	urldate = {2025-01-09},
	journal = {Data Mining and Knowledge Discovery},
	author = {Rosset, Saharon and Perlich, Claudia and Świrszcz, Grzergorz and Melville, Prem and Liu, Yan},
	month = may,
	year = {2010},
	keywords = {Artificial Intelligence, Leakage, Medical data mining, Model evaluation, Relational learning},
	pages = {439--468},
}

@article{chen_learning_2010,
	title = {Learning {Spatial}-{Temporal} {Varying} {Graphs} with {Applications} to {Climate} {Data} {Analysis}},
	volume = {24},
	copyright = {Copyright (c) 2021 Proceedings of the AAAI Conference on Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7658},
	doi = {10.1609/aaai.v24i1.7658},
	abstract = {An important challenge in understanding climate change is to uncover the dependency relationships between various climate observations and forcing factors. Graphical lasso, a recently proposed L1 penalty based structure learning algorithm, has been proven successful for learning underlying dependency structures for the data drawn from a multivariate Gaussian distribution. However, climatological data often turn out to be non-Gaussian, e.g. cloud cover, precipitation, etc. In this paper, we examine nonparametric learning methods to address this challenge. In particular, we develop a methodology to learn dynamic graph structures from spatial-temporal data so that the graph structures at adjacent time or locations are similar. Experimental results demonstrate that our method not only recovers the underlying graph well but also captures the smooth variation properties on both synthetic data and climate data. An important challenge in understanding climate change is to  uncover the dependency relationships between various climate observations and forcing factors. Graphical lasso, a recently proposed An important challenge in understanding climate change is to uncover the dependency relationships between various climate observations and forcing factors. Graphical lasso, a recently proposed L1 penalty based structure learning algorithm, has been proven successful for learning underlying dependency structures for the data drawn from a multivariate Gaussian distribution. However, climatological data often turn out to be non-Gaussian, e.g. cloud cover, precipitation, etc. In this paper, we examine nonparametric learning methods to address this challenge. In particular, we develop a methodology to learn dynamic graph structures from spatial-temporal data so that the graph structures at adjacent time or locations are similar. Experimental results demonstrate that our method not only recovers the underlying graph well but also captures the smooth variation properties on both synthetic data and climate data.},
	language = {en},
	number = {1},
	urldate = {2025-01-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Xi and Liu, Yan and Liu, Han and Carbonell, Jaime},
	month = jul,
	year = {2010},
	note = {Number: 1},
	keywords = {Climate Data Analysis, Graph Structure Learning, Spatial-Temporal Data Mining},
	pages = {425--430},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\5ZXGXM2U\\Chen et al. - 2010 - Learning Spatial-Temporal Varying Graphs with Applications to Climate Data Analysis.pdf:application/pdf},
}

@inproceedings{liu_learning_2010,
	address = {Madison, WI, USA},
	series = {{ICML}'10},
	title = {Learning temporal causal graphs for relational time-series analysis},
	isbn = {978-1-60558-907-7},
	abstract = {Learning temporal causal graph structures from multivariate time-series data reveals important dependency relationships between current observations and histories, and provides a better understanding of complex systems. In this paper, we examine learning tasks where one is presented with multiple multivariate time-series, as well as a relational graph among the different time-series. We propose an L1 regularized hidden Markov random field regression framework to leverage the information provided by the relational graph and jointly infer more accurate temporal causal structures for all time-series. We test the proposed model on climate modeling and cross-species microarray data analysis applications.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Liu, Yan and Niculescu-Mizil, Alexandru and Lozano, Aurélie and Lu, Yong},
	month = jun,
	year = {2010},
	pages = {687--694},
}


@inproceedings{kale_accelerating_2013,
	title = {Accelerating {Active} {Learning} with {Transfer} {Learning}},
	url = {https://ieeexplore.ieee.org/abstract/document/6729602},
	doi = {10.1109/ICDM.2013.160},
	abstract = {Active learning, transfer learning, and related techniques are unified by a core theme: efficient and effective use of available data. Active learning offers scalable solutions for building effective supervised learning models while minimizing annotation effort. Transfer learning utilizes existing labeled data from one task to help learning related tasks for which limited labeled data are available. There has been limited research, however, on how to combine these two techniques. In this paper, we present a simple and principled transfer active learning framework that leverages pre-existing labeled data from related tasks to improve the performance of an active learner. We derive an intuitive bound on generalization error for the classifiers learned by this algorithm that provides insight into the algorithm's behavior and the problem in general. Experimental results using several well-known transfer learning data sets confirm our theoretical analysis and demonstrate the effectiveness of our approach.},
	urldate = {2025-01-09},
	booktitle = {2013 {IEEE} 13th {International} {Conference} on {Data} {Mining}},
	author = {Kale, David and Liu, Yan},
	month = dec,
	year = {2013},
	note = {ISSN: 2374-8486},
	keywords = {Acceleration, Machine Learning, Training, Active Learning, Algorithm design and analysis, Labeling, Learning Theory, Query processing, Supervised learning, Transfer Learning, Upper bound},
	pages = {1085--1090},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\\H7YRSIX3\\6729602.html:text/html},
}

@inproceedings{bahadori_fast_2013,
	address = {New York, NY, USA},
	series = {{KDD} '13},
	title = {Fast structure learning in generalized stochastic processes with latent factors},
	isbn = {978-1-4503-2174-7},
	url = {https://doi.org/10.1145/2487575.2487578},
	doi = {10.1145/2487575.2487578},
	abstract = {Understanding and quantifying the impact of unobserved processes is one of the major challenges of analyzing multivariate time series data. In this paper, we analyze a flexible stochastic process model, the generalized linear auto-regressive process (GLARP) and identify the conditions under which the impact of hidden variables appears as an additive term to the evolution matrix estimated with the maximum likelihood. In particular, we examine three examples, including two popular models for count data, i.e, Poisson and Conwey-Maxwell Poisson vector auto-regressive processes, and one powerful model for extreme value data, i.e., Gumbel vector auto-regressive processes. We demonstrate that the impact of hidden factors can be separated out via convex optimization in these three models. We also propose a fast greedy algorithm based on the selection of composite atoms in each iteration and provide a performance guarantee for it. Experiments on two synthetic datasets, one social network dataset and one climatology dataset demonstrate the the superior performance of our proposed models.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 19th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Bahadori, Mohammad Taha and Liu, Yan and Xing, Eric P.},
	month = aug,
	year = {2013},
	pages = {284--292},
}

@incollection{bahadori_examination_2013,
	series = {Proceedings},
	title = {An {Examination} of {Practical} {Granger} {Causality} {Inference}},
	isbn = {978-1-61197-262-7},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972832.52},
	abstract = {Learning temporal causal structures among multiple time series is one of the major tasks in mining time series data. Granger causality is one of the most popular techniques in uncovering the temporal dependencies among time series; however it faces two main challenges: (i) the spurious effect of unobserved time series and (ii) the computational challenges in high dimensional settings. In this paper, we utilize the confounder path delays to find a subset of time series that via conditioning on them we are able to cancel out the spurious confounder effects. After study of consistency of different Granger causality techniques, we propose Copula-Granger and show that while it is consistent in high dimensions, it can efficiently capture non-linearity in the data. Extensive experiments on a synthetic and a social networking dataset confirm our theoretical results.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 2013 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Bahadori, Mohammad Taha and Liu, Yan},
	month = may,
	year = {2013},
	doi = {10.1137/1.9781611972832.52},
	pages = {467--475},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\JLFNHBJK\\Bahadori and Liu - 2013 - An Examination of Practical Granger Causality Inference.pdf:application/pdf},
}

@misc{purushotham_collaborative_2012,
	title = {Collaborative {Topic} {Regression} with {Social} {Matrix} {Factorization} for {Recommendation} {Systems}},
	url = {http://arxiv.org/abs/1206.4684},
	doi = {10.48550/arXiv.1206.4684},
	abstract = {Social network websites, such as Facebook, YouTube, Lastfm etc, have become a popular platform for users to connect with each other and share content or opinions. They provide rich information for us to study the influence of user's social circle in their decision process. In this paper, we are interested in examining the effectiveness of social network information to predict the user's ratings of items. We propose a novel hierarchical Bayesian model which jointly incorporates topic modeling and probabilistic matrix factorization of social networks. A major advantage of our model is to automatically infer useful latent topics and social information as well as their importance to collaborative filtering from the training data. Empirical experiments on two large-scale datasets show that our algorithm provides a more effective recommendation system than the state-of-the art approaches. Our results reveal interesting insight that the social circles have more influence on people's decisions about the usefulness of information (e.g., bookmarking preference on Delicious) than personal taste (e.g., music preference on Lastfm). We also examine and discuss solutions on potential information leak in many recommendation systems that utilize social information.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Purushotham, Sanjay and Liu, Yan and Kuo, C.-C. Jay},
	month = jun,
	year = {2012},
	note = {arXiv:1206.4684 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Social and Information Networks},
	annote = {Comment: ICML2012},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\RR2T68PQ\\Purushotham et al. - 2012 - Collaborative Topic Regression with Social Matrix Factorization for Recommendation Systems.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\5HG8EZRZ\\1206.html:text/html},
}

@misc{liu_sparse-gev_2012,
	title = {Sparse-{GEV}: {Sparse} {Latent} {Space} {Model} for {Multivariate} {Extreme} {Value} {Time} {Serie} {Modeling}},
	shorttitle = {Sparse-{GEV}},
	url = {http://arxiv.org/abs/1206.4685},
	doi = {10.48550/arXiv.1206.4685},
	abstract = {In many applications of time series models, such as climate analysis and social media analysis, we are often interested in extreme events, such as heatwave, wind gust, and burst of topics. These time series data usually exhibit a heavy-tailed distribution rather than a Gaussian distribution. This poses great challenges to existing approaches due to the significantly different assumptions on the data distributions and the lack of sufficient past data on extreme events. In this paper, we propose the Sparse-GEV model, a latent state model based on the theory of extreme value modeling to automatically learn sparse temporal dependence and make predictions. Our model is theoretically significant because it is among the first models to learn sparse temporal dependencies among multivariate extreme value time series. We demonstrate the superior performance of our algorithm to the state-of-art methods, including Granger causality, copula approach, and transfer entropy, on one synthetic dataset, one climate dataset and two Twitter datasets.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Liu, Yan and Bahadori, Taha and Li, Hongfei},
	month = jun,
	year = {2012},
	note = {arXiv:1206.4685 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Applications, Statistics - Methodology},
	annote = {Comment: ICML2012},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\JWQTFMLV\\Liu et al. - 2012 - Sparse-GEV Sparse Latent Space Model for Multivariate Extreme Value Time Serie Modeling.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\DSMSF2YX\\1206.html:text/html},
}

@misc{kang_transfer_2013,
	title = {Transfer {Topic} {Modeling} with {Ease} and {Scalability}},
	url = {http://arxiv.org/abs/1301.5686},
	doi = {10.48550/arXiv.1301.5686},
	abstract = {The increasing volume of short texts generated on social media sites, such as Twitter or Facebook, creates a great demand for effective and efficient topic modeling approaches. While latent Dirichlet allocation (LDA) can be applied, it is not optimal due to its weakness in handling short texts with fast-changing topics and scalability concerns. In this paper, we propose a transfer learning approach that utilizes abundant labeled documents from other domains (such as Yahoo! News or Wikipedia) to improve topic modeling, with better model fitting and result interpretation. Specifically, we develop Transfer Hierarchical LDA (thLDA) model, which incorporates the label information from other domains via informative priors. In addition, we develop a parallel implementation of our model for large-scale applications. We demonstrate the effectiveness of our thLDA model on both a microblogging dataset and standard text collections including AP and RCV1 datasets.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Kang, Jeon-Hyung and Ma, Jun and Liu, Yan},
	month = jan,
	year = {2013},
	note = {arXiv:1301.5686 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 2012 SIAM International Conference on Data Mining (SDM12) Pages: \{564-575\}},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\QUYFARHG\\Kang et al. - 2013 - Transfer Topic Modeling with Ease and Scalability.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\5P6LNF6D\\1301.html:text/html},
}

@inproceedings{qiu_granger_2012,
	title = {Granger {Causality} for {Time}-{Series} {Anomaly} {Detection}},
	url = {https://ieeexplore.ieee.org/document/6413806},
	doi = {10.1109/ICDM.2012.73},
	abstract = {Recent developments in industrial systems provide us with a large amount of time series data from sensors, logs, system settings and physical measurements, etc. These data are extremely valuable for providing insights about the complex systems and could be used to detect anomalies at early stages. However, the special characteristics of these time series data, such as high dimensions and complex dependencies between variables, as well as its massive volume, pose great challenges to existing anomaly detection algorithms. In this paper, we propose Granger graphical models as an effective and scalable approach for anomaly detection whose results can be readily interpreted. Specifically, Granger graphical models are a family of graphical models that exploit the temporal dependencies between variables by applying L1-regularized learning to Granger causality. Our goal is to efficiently compute a robust "correlation anomaly" score for each variable via Granger graphical models that can provide insights on the possible reasons of anomalies. We evaluate the effectiveness of our proposed algorithms on both synthetic and application datasets. The results show the proposed algorithm achieves significantly better performance than other baseline algorithms and is scalable for large-scale applications.},
	urldate = {2025-01-09},
	booktitle = {2012 {IEEE} 12th {International} {Conference} on {Data} {Mining}},
	author = {Qiu, Huida and Liu, Yan and Subrahmanya, Niranjan A. and Li, Weichang},
	month = dec,
	year = {2012},
	note = {ISSN: 2374-8486},
	keywords = {Algorithm design and analysis, Anomaly Detection, Data models, Graphical models, Optimization, Principal component analysis, Stochastic processes, Time series analysis, Time Series Analysis},
	pages = {1074--1079},
}

@incollection{bahadori_granger_2012,
	series = {Proceedings},
	title = {Granger {Causality} {Analysis} in {Irregular} {Time} {Series}},
	isbn = {978-1-61197-232-0},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972825.57},
	abstract = {Learning temporal causal structures between time series is one of the key tools for analyzing time series data. In many real-world applications, we are confronted with Irregular Time Series, whose observations are not sampled at equally-spaced time stamps. The irregularity in sampling intervals violates the basic assumptions behind many models for structure learning. In this paper, we propose a nonparametric generalization of the Granger graphical models called Generalized Lasso Granger (GLG) to uncover the temporal dependencies from irregular time series. Via theoretical analysis and extensive experiments, we verify the effectiveness of our model. Furthermore, we apply GLG to the application dataset of δ18O isotope of Oxygen records in Asia and achieve promising results to discover the moisture transportation patterns in a 800-year period.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 2012 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Bahadori, Mohammad Taha and Liu, Yan},
	month = apr,
	year = {2012},
	doi = {10.1137/1.9781611972825.57},
	pages = {660--671},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\GTQBJM84\\Bahadori and Liu - 2012 - Granger Causality Analysis in Irregular Time Series.pdf:application/pdf},
}

@inproceedings{zhou_community_2012,
	address = {New York, NY, USA},
	series = {{KDD} '12},
	title = {Community discovery and profiling with social messages},
	isbn = {978-1-4503-1462-6},
	url = {https://doi.org/10.1145/2339530.2339593},
	doi = {10.1145/2339530.2339593},
	abstract = {Discovering communities from social media and collaboration systems has been of great interest in recent years. Existing work show prospects of modeling contents and social links, aiming at discovering social communities, whose definition varies by application. We believe that a community depends not only on the group of people who actively participate, but also the topics they communicate about or collaborate on. This is especially true for workplace email communications. Within an organization, it is not uncommon that employees multifunction, and groups of employees collaborate on multiple projects at the same time. In this paper, we aim to automatically discovering and profiling users' communities by taking into account both the contacts and the topics. More specifically, we propose a community profiling model called COCOMP, where the communities labels are latent, and each social document corresponds to an information sharing activity among the most probable community members regarding the most relevant community issues. Experiment results on several social communication datasets, including emails and Twitter messages, demonstrate that the model can discover users' communities effectively, and provide concrete semantics.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 18th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Wenjun and Jin, Hongxia and Liu, Yan},
	month = aug,
	year = {2012},
	pages = {388--396},
}

@article{zhang_transfer_2011,
	title = {Transfer {Latent} {Semantic} {Learning}: {Microblog} {Mining} with {Less} {Supervision}},
	volume = {25},
	copyright = {Copyright (c) 2021 Proceedings of the AAAI Conference on Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Transfer {Latent} {Semantic} {Learning}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7916},
	doi = {10.1609/aaai.v25i1.7916},
	abstract = {The increasing volume of information generated on micro-blogging sites such as Twitter raises several challenges to traditional text mining techniques. First, most texts from those sites are abbreviated due to the constraints of limited characters in one post; second, the input usually comes in streams of large-volumes. Therefore, it is of significant importance to develop effective and efficient representations of abbreviated texts for better filtering and mining. In this paper, we introduce a novel transfer learning approach, namely transfer latent semantic learning, that utilizes a large number of related tagged documents with rich information from other sources (source domain) to help build a robust latent semantic space for the abbreviated texts (target domain). This is achieved by simultaneously minimizing the document reconstruction error and the classification error of the labeled examples from the source domain by building a classifier with hinge loss in the latent semantic space. We demonstrate the effectiveness of our method by applying them to the task of classifying and tagging abbreviated texts. Experimental results on both synthetic datasets and real application datasets, including Reuters-21578 and Twitter data, suggest substantial improvements using our approach over existing ones.},
	language = {en},
	number = {1},
	urldate = {2025-01-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Dan and Liu, Yan and Lawrence, Richard and Chenthamarakshan, Vijil},
	month = aug,
	year = {2011},
	note = {Number: 1},
	pages = {561--566},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\QZEPHBFN\\Zhang et al. - 2011 - Transfer Latent Semantic Learning Microblog Mining with Less Supervision.pdf:application/pdf},
}

@article{chang_detecting_2011,
	title = {Detecting {Multilingual} and {Multi}-{Regional} {Query} {Intent} in {Web} {Search}},
	volume = {25},
	copyright = {Copyright (c) 2021 Proceedings of the AAAI Conference on Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/8074},
	doi = {10.1609/aaai.v25i1.8074},
	abstract = {With rapid growth of commercial search engines, detecting multilingual and multi-regional intent underlying search queries becomes a critical challenge to serve international users with diverse language and region requirements. We introduce a query intent probabilistic model, whose input is the number of clicks on documents from different regions and in different language, while the output of this model is a smoothed probabilistic distribution of multilingual and multi-regional query intent. Based on an editorial test to evaluate the accuracy of the intent classifier, our probabilistic model could improve the accuracy of multilingual intent detection for 15\%, and improve multi-regional intent detection for 18\%. To improve web search quality, we propose a set of new ranking features to combine multilingual and multi-regional query intent with document language/region attributes, and apply different approaches in integrating intent information to directly affect ranking. The experiments show that the novel features could provide 2.31\% NDCG@1 improvement and 1.81\% NDCG@5 improvement.},
	language = {en},
	number = {1},
	urldate = {2025-01-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chang, Yi and Zhang, Ruiqiang and Reddy, Srihari and Liu, Yan},
	month = aug,
	year = {2011},
	note = {Number: 1},
	pages = {1134--1139},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\RVXRKVEY\\Chang et al. - 2011 - Detecting Multilingual and Multi-Regional Query Intent in Web Search.pdf:application/pdf},
}

@inproceedings{hauder_framework_2011,
	title = {A {Framework} for {Efficient} {Data} {Analytics} through {Automatic} {Configuration} and {Customization} of {Scientific} {Workflows}},
	url = {https://ieeexplore.ieee.org/document/6123302},
	doi = {10.1109/eScience.2011.59},
	abstract = {Data analytics involves choosing between many different algorithms and experimenting with possible combinations of those algorithms. Existing approaches however do not support scientists with the laborious tasks of exploring the design space of computational experiments. We have developed a framework to assist scientists with data analysis tasks in particular machine learning and data mining. It takes advantage of the unique capabilities of the Wings workflow system to reason about semantic constraints. We show how the framework can rule out invalid workflows and help scientists to explore the design space. We demonstrate our system in the domain of text analytics, and outline the benefits of our approach.},
	urldate = {2025-01-09},
	booktitle = {2011 {IEEE} {Seventh} {International} {Conference} on {eScience}},
	author = {Hauder, Matheus and Gil, Yolanda and Liu, Yan},
	month = dec,
	year = {2011},
	keywords = {Algorithm design and analysis, Clustering algorithms, Correlation, Data Analytics, Machine learning algorithms, Prediction algorithms, Scientific Workflows, Software, Software algorithms},
	pages = {379--386},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\\5ZIXB94H\\6123302.html:text/html},
}

@inproceedings{zhang_multi-view_2011,
	address = {New York, NY, USA},
	series = {{KDD} '11},
	title = {Multi-view transfer learning with a large margin approach},
	isbn = {978-1-4503-0813-7},
	url = {https://doi.org/10.1145/2020408.2020593},
	doi = {10.1145/2020408.2020593},
	abstract = {Transfer learning has been proposed to address the problem of scarcity of labeled data in the target domain by leveraging the data from the source domain. In many real world applications, data is often represented from different perspectives, which correspond to multiple views. For example, a web page can be described by its contents and its associated links. However, most existing transfer learning methods fail to capture the multi-view \{nature\}, and might not be best suited for such applications.To better leverage both the labeled data from the source domain and the features from different views, \{this paper proposes\} a general framework: Multi-View Transfer Learning with a Large Margin Approach (MVTL-LM). On one hand, labeled data from the source domain is effectively utilized to construct a large margin classifier; on the other hand, the data from both domains is employed to impose consistencies among multiple views. As an instantiation of this framework, we propose an efficient optimization method, which is guaranteed to converge to ε precision in O(1/ε) steps. Furthermore, we analyze its error bound, which improves over existing results of related methods. An extensive set of experiments are conducted to demonstrate the advantages of our proposed method over state-of-the-art techniques.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 17th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Dan and He, Jingrui and Liu, Yan and Si, Luo and Lawrence, Richard},
	month = aug,
	year = {2011},
	pages = {1208--1216},
}

@inproceedings{bahadori_learning_2011,
	address = {USA},
	series = {{ICDM} '11},
	title = {Learning with {Minimum} {Supervision}: {A} {General} {Framework} for {Transductive} {Transfer} {Learning}},
	isbn = {978-0-7695-4408-3},
	shorttitle = {Learning with {Minimum} {Supervision}},
	url = {https://doi.org/10.1109/ICDM.2011.92},
	doi = {10.1109/ICDM.2011.92},
	abstract = {Transductive transfer learning is one special type of transfer learning problem, in which abundant labeled examples are available in the source domain and only {\textbackslash}textit\{unlabeled\} examples are available in the target domain. It easily finds applications in spam filtering, microblogging mining and so on. In this paper, we propose a general framework to solve the problem by mapping the input features in both the source domain and target domain into a shared latent space and simultaneously minimizing the feature reconstruction loss and prediction loss. We develop one specific example of the framework, namely latent large-margin transductive transfer learning (LATTL) algorithm, and analyze its theoretic bound of classification loss via Rademacher complexity. We also provide a unified view of several popular transfer learning algorithms under our framework. Experiment results on one synthetic dataset and three application datasets demonstrate the advantages of the proposed algorithm over the other state-of-the-art ones.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 2011 {IEEE} 11th {International} {Conference} on {Data} {Mining}},
	publisher = {IEEE Computer Society},
	author = {Bahadori, Mohammad Taha and Liu, Yan and Zhang, Dan},
	month = dec,
	year = {2011},
	pages = {61--70},
}

@inproceedings{zhang_multiple_2011,
	title = {Multiple {Instance} {Learning} on {Structured} {Data}},
	volume = {24},
	url = {https://papers.nips.cc/paper_files/paper/2011/hash/ec5decca5ed3d6b8079e2e7e7bacc9f2-Abstract.html},
	abstract = {Most existing Multiple-Instance Learning (MIL) algorithms  assume data instances and/or data bags are independently and  identically distributed. But there often exists rich additional  dependency/structure information between instances/bags within many  applications of MIL. Ignoring this structure information limits the  performance of existing MIL algorithms. This paper explores the  research problem as multiple instance learning on structured  data (MILSD) and formulates a novel framework that considers  additional structure information. In particular, an effective and  efficient optimization algorithm has been proposed to solve the  original non-convex optimization problem by using a combination of  Concave-Convex Constraint Programming (CCCP) method and an adapted  Cutting Plane method, which deals with two sets of constraints caused  by learning on  instances within individual bags and learning on  structured data. Our method has the nice convergence property,  with specified precision on each set of constraints. Experimental  results on three different applications, i.e., webpage  classification, market targeting, and protein fold identification,  clearly demonstrate the advantages of the proposed method over  state-of-the-art methods.},
	urldate = {2025-01-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Dan and Liu, Yan and Si, Luo and Zhang, Jian and Lawrence, Richard},
	year = {2011},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\QL6RS9SC\\Zhang et al. - 2011 - Multiple Instance Learning on Structured Data.pdf:application/pdf},
}

@inproceedings{liu_latent_2011,
	address = {New York, NY, USA},
	series = {{KDD} '11},
	title = {Latent graphical models for quantifying and predicting patent quality},
	isbn = {978-1-4503-0813-7},
	url = {https://doi.org/10.1145/2020408.2020586},
	doi = {10.1145/2020408.2020586},
	abstract = {The number of patents filed each year has increased dramatically in recent years, raising concerns that patents of questionable validity are restricting the issuance of truly innovative patents. For this reason, there is a strong demand to develop an objective model to quantify patent quality and characterize the attributes that lead to higher-quality patents. In this paper, we develop a latent graphical model to infer patent quality from related measurements. In addition, we extract advanced lexical features via natural language processing techniques to capture the quality measures such as clarity of claims, originality, and importance of cited prior art. We demonstrate the effectiveness of our approach by validating its predictions with previous court decisions of litigated patents.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 17th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yan and Hseuh, Pei-yun and Lawrence, Rick and Meliksetian, Steve and Perlich, Claudia and Veen, Alejandro},
	month = aug,
	year = {2011},
	pages = {1145--1153},
}

@article{liu_temporal_2011,
	title = {Temporal graphical models for cross-species gene regulatory network discovery},
	volume = {9},
	issn = {1757-6334},
	doi = {10.1142/s0219720011005525},
	abstract = {Many genes and biological processes function in similar ways across different species. Cross-species gene expression analysis, as a powerful tool to characterize the dynamical properties of the cell, has found a number of applications, such as identifying a conserved core set of cell cycle genes. However, to the best of our knowledge, there is limited effort on developing appropriate techniques to capture the causality relations between genes from time-series microarray data across species. In this paper, we present hidden Markov random field regression with L(1) penalty to uncover the regulatory network structure for different species. The algorithm provides a framework for sharing information across species via hidden component graphs and is able to incorporate domain knowledge across species easily. We demonstrate our method on two synthetic datasets and apply it to discover causal graphs from innate immune response data.},
	language = {eng},
	number = {2},
	journal = {Journal of Bioinformatics and Computational Biology},
	author = {Liu, Yan and Niculescu-Mizil, Alexandru and Lozano, Aurélie and Lu, Yong},
	month = apr,
	year = {2011},
	pmid = {21523930},
	keywords = {Algorithms, Animals, Computational Biology, Computer Graphics, Computer Simulation, Gene Regulatory Networks, Humans, Immunogenetic Phenomena, Markov Chains, Mice, Microarray Analysis, Models, Genetic, Regression Analysis, Species Specificity},
	pages = {231--250},
}

@inproceedings{zhang_serendipitous_2011,
	address = {New York, NY, USA},
	series = {{KDD} '11},
	title = {Serendipitous learning: learning beyond the predefined label space},
	isbn = {978-1-4503-0813-7},
	shorttitle = {Serendipitous learning},
	url = {https://doi.org/10.1145/2020408.2020608},
	doi = {10.1145/2020408.2020608},
	abstract = {Most traditional supervised learning methods are developed to learn a model from labeled examples and use this model to classify the unlabeled ones into the same label space predefined by the models. However, in many real world applications, the label spaces for both the labeled/training and unlabeled/testing examples can be different. To solve this problem, this paper proposes a novel notion of Serendipitous Learning (SL), which is defined to address the learning scenarios in which the label space can be enlarged during the testing phase. In particular, a large margin approach is proposed to solve SL. The basic idea is to leverage the knowledge in the labeled examples to help identify novel/unknown classes, and the large margin formulation is proposed to incorporate both the classification loss on the examples within the known categories, as well as the clustering loss on the examples in unknown categories. An efficient optimization algorithm based on CCCP and the bundle method is proposed to solve the optimization problem of the large margin formulation of SL. Moreover, an efficient online learning method is proposed to address the issue of large scale data in online learning scenario, which has been shown to have a guaranteed learning regret. An extensive set of experimental results on two synthetic datasets and two datasets from real world applications demonstrate the advantages of the proposed method over several other baseline algorithms. One limitation of the proposed method is that the number of unknown classes is given in advance. It may be possible to remove this constraint if we model it by using a non-parametric way. We also plan to do experiments on more real world applications in the future.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 17th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Dan and Liu, Yan and Si, Luo},
	month = aug,
	year = {2011},
	pages = {1343--1351},
}

@inproceedings{wang_collaboration_2010,
	address = {New York, NY, USA},
	series = {{CIKM} '10},
	title = {Collaboration analytics: mining work patterns from collaboration activities},
	isbn = {978-1-4503-0099-5},
	shorttitle = {Collaboration analytics},
	url = {https://doi.org/10.1145/1871437.1871748},
	doi = {10.1145/1871437.1871748},
	abstract = {People are increasingly using more and more social softwares, generating flooding communications. User analytics may be performed to mine a person's activities on different social systems and extract patterns, be it interest patterns, social patterns, or work patterns. Such patterns may benefit both the individuals and the organizations the users associated with, as the information is valuable in numerous tasks, including recommendation, evaluation, management, and so on. In this article, we present an actionable solution of user analytics, namely collaboration analytics, by focusing on mining a person's work patterns from her collaboration activities. Our solution effectively makes use of a user's heterogeneous data collected from various collaboration tools to derive an integrated description of the user's collaborative work. A number of ``work areas'', each of which contains its work topics and people involved, are generated for every user. The challenges we face include the clustering of items with short texts and prioritizing/weighting data items based on importance/relevance. Our solutions to those issues will be described in this article. In particular, we mine users' background information from various types of data and use such information to enrich the semantics of the short texts contained in the activity instances on collaboration tools before clustering those instances into work areas. Finally, we have developed a prototype of our collaboration analytics solution and evaluated it with real-world data and people.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 19th {ACM} international conference on {Information} and knowledge management},
	publisher = {Association for Computing Machinery},
	author = {Wang, Qihua and Jin, Hongxia and Liu, Yan},
	month = oct,
	year = {2010},
	pages = {1861--1864},
}

@article{rosset_medical_2010,
	title = {Medical data mining: insights from winning two competitions},
	volume = {20},
	issn = {1573-756X},
	shorttitle = {Medical data mining},
	url = {https://doi.org/10.1007/s10618-009-0158-x},
	doi = {10.1007/s10618-009-0158-x},
	abstract = {Two major data mining competitions in 2008 presented challenges in medical domains: KDD Cup 2008, which concerned cancer detection from mammography data; and Informs Data Mining Challenge 2008, dealing with diagnosis of pneumonia based on patient information from hospital files. Our team won both of these competitions, and in this paper we share our lessons learned and insights. We emphasize the aspects that pertain to the general practice and methodology of medical data mining, rather than to the specifics of each modeling competition. We concentrate on three topics: information leakage, its effect on competitions and proof-of-concept projects; consideration of real-life model performance measures in model construction and evaluation; and relational learning approaches to medical data mining tasks.},
	language = {en},
	number = {3},
	urldate = {2025-01-09},
	journal = {Data Mining and Knowledge Discovery},
	author = {Rosset, Saharon and Perlich, Claudia and Świrszcz, Grzergorz and Melville, Prem and Liu, Yan},
	month = may,
	year = {2010},
	keywords = {Artificial Intelligence, Leakage, Medical data mining, Model evaluation, Relational learning},
	pages = {439--468},
}

@article{chen_learning_2010,
	title = {Learning {Spatial}-{Temporal} {Varying} {Graphs} with {Applications} to {Climate} {Data} {Analysis}},
	volume = {24},
	copyright = {Copyright (c) 2021 Proceedings of the AAAI Conference on Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7658},
	doi = {10.1609/aaai.v24i1.7658},
	abstract = {An important challenge in understanding climate change is to uncover the dependency relationships between various climate observations and forcing factors. Graphical lasso, a recently proposed L1 penalty based structure learning algorithm, has been proven successful for learning underlying dependency structures for the data drawn from a multivariate Gaussian distribution. However, climatological data often turn out to be non-Gaussian, e.g. cloud cover, precipitation, etc. In this paper, we examine nonparametric learning methods to address this challenge. In particular, we develop a methodology to learn dynamic graph structures from spatial-temporal data so that the graph structures at adjacent time or locations are similar. Experimental results demonstrate that our method not only recovers the underlying graph well but also captures the smooth variation properties on both synthetic data and climate data. An important challenge in understanding climate change is to  uncover the dependency relationships between various climate observations and forcing factors. Graphical lasso, a recently proposed An important challenge in understanding climate change is to uncover the dependency relationships between various climate observations and forcing factors. Graphical lasso, a recently proposed L1 penalty based structure learning algorithm, has been proven successful for learning underlying dependency structures for the data drawn from a multivariate Gaussian distribution. However, climatological data often turn out to be non-Gaussian, e.g. cloud cover, precipitation, etc. In this paper, we examine nonparametric learning methods to address this challenge. In particular, we develop a methodology to learn dynamic graph structures from spatial-temporal data so that the graph structures at adjacent time or locations are similar. Experimental results demonstrate that our method not only recovers the underlying graph well but also captures the smooth variation properties on both synthetic data and climate data.},
	language = {en},
	number = {1},
	urldate = {2025-01-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Xi and Liu, Yan and Liu, Han and Carbonell, Jaime},
	month = jul,
	year = {2010},
	note = {Number: 1},
	keywords = {Climate Data Analysis, Graph Structure Learning, Spatial-Temporal Data Mining},
	pages = {425--430},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\5ZXGXM2U\\Chen et al. - 2010 - Learning Spatial-Temporal Varying Graphs with Applications to Climate Data Analysis.pdf:application/pdf},
}

@inproceedings{liu_learning_2010,
	address = {Madison, WI, USA},
	series = {{ICML}'10},
	title = {Learning temporal causal graphs for relational time-series analysis},
	isbn = {978-1-60558-907-7},
	abstract = {Learning temporal causal graph structures from multivariate time-series data reveals important dependency relationships between current observations and histories, and provides a better understanding of complex systems. In this paper, we examine learning tasks where one is presented with multiple multivariate time-series, as well as a relational graph among the different time-series. We propose an L1 regularized hidden Markov random field regression framework to leverage the information provided by the relational graph and jointly infer more accurate temporal causal structures for all time-series. We test the proposed model on climate modeling and cross-species microarray data analysis applications.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Liu, Yan and Niculescu-Mizil, Alexandru and Lozano, Aurélie and Lu, Yong},
	month = jun,
	year = {2010},
	pages = {687--694},
}


@inproceedings{liu_topic-link_2009,
	address = {New York, NY, USA},
	series = {{ICML} '09},
	title = {Topic-link {LDA}: joint models of topic and author community},
	isbn = {978-1-60558-516-1},
	shorttitle = {Topic-link {LDA}},
	url = {https://doi.org/10.1145/1553374.1553460},
	doi = {10.1145/1553374.1553460},
	abstract = {Given a large-scale linked document collection, such as a collection of blog posts or a research literature archive, there are two fundamental problems that have generated a lot of interest in the research community. One is to identify a set of high-level topics covered by the documents in the collection; the other is to uncover and analyze the social network of the authors of the documents. So far these problems have been viewed as separate problems and considered independently from each other. In this paper we argue that these two problems are in fact inter-dependent and should be addressed together. We develop a Bayesian hierarchical approach that performs topic modeling and author community discovery in one unified framework. The effectiveness of our model is demonstrated on two blog data sets in different domains and one research paper citation data from CiteSeer.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yan and Niculescu-Mizil, Alexandru and Gryc, Wojciech},
	month = jun,
	year = {2009},
	pages = {665--672},
}

@inproceedings{lozano_grouped_2009,
	address = {New York, NY, USA},
	series = {{KDD} '09},
	title = {Grouped graphical {Granger} modeling methods for temporal causal modeling},
	isbn = {978-1-60558-495-9},
	url = {https://doi.org/10.1145/1557019.1557085},
	doi = {10.1145/1557019.1557085},
	abstract = {We develop and evaluate an approach to causal modeling based on time series data, collectively referred to as "grouped graphical Granger modeling methods." Graphical Granger modeling uses graphical modeling techniques on time series data and invokes the notion of "Granger causality" to make assertions on causality among a potentially large number of time series variables through inference on time-lagged effects. The present paper proposes a novel enhancement to the graphical Granger methodology by developing and applying families of regression methods that are sensitive to group information among variables, to leverage the group structure present in the lagged temporal variables according to the time series they belong to. Additionally, we propose a new family of algorithms we call group boosting, as an improved component of grouped graphical Granger modeling over the existing regression methods with grouped variable selection in the literature (e.g group Lasso). The introduction of group boosting methods is primarily motivated by the need to deal with non-linearity in the data. We perform empirical evaluation to confirm the advantage of the grouped graphical Granger methods over the standard (non-grouped) methods, as well as that specific to the methods based on group boosting. This advantage is also demonstrated for the real world application of gene regulatory network discovery from time-course microarray data.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 15th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Lozano, Aurelie C. and Abe, Naoki and Liu, Yan and Rosset, Saharon},
	month = jun,
	year = {2009},
	pages = {577--586},
}

@inproceedings{liu_learning_2009,
	address = {New York, NY, USA},
	series = {{KDD} '09},
	title = {Learning dynamic temporal graphs for oil-production equipment monitoring system},
	isbn = {978-1-60558-495-9},
	url = {https://doi.org/10.1145/1557019.1557151},
	doi = {10.1145/1557019.1557151},
	abstract = {Learning temporal graph structures from time series data reveals important dependency relationships between current observations and histories. Most previous work focuses on learning and predicting with "static" temporal graphs only. However, in many applications such as mechanical systems and biology systems, the temporal dependencies might change over time. In this paper, we develop a dynamic temporal graphical models based on hidden Markov model regression and lasso-type algorithms. Our method is able to integrate two usually separate tasks, i.e. inferring underlying states and learning temporal graphs, in one unified model. The output temporal graphs provide better understanding about complex systems, i.e. how their dependency graphs evolve over time, and achieve more accurate predictions. We examine our model on two synthetic datasets as well as a real application dataset for monitoring oil-production equipment to capture different stages of the system, and achieve promising results.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 15th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yan and Kalagnanam, Jayant R. and Johnsen, Oivind},
	month = jun,
	year = {2009},
	pages = {1225--1234},
}

@incollection{ide_proximity-based_2009,
	series = {Proceedings},
	title = {Proximity-{Based} {Anomaly} {Detection} using {Sparse} {Structure} {Learning}},
	isbn = {978-0-89871-682-5},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972795.9},
	abstract = {We consider the task of performing anomaly detection in highly noisy multivariate data. In many applications involving real-valued time-series data, such as physical sensor data and economic metrics, discovering changes and anomalies in the way variables depend on one another is of particular importance. Our goal is to robustly compute the “correlation anomaly” score of each variable by comparing the test data with reference data, even when some of the variables are highly correlated (and thus collinearity exists). To remove seeming dependencies introduced by noise, we focus on the most significant dependencies for each variable. We perform this “neighborhood selection” in an adaptive manner by fitting a sparse graphical Gaussian model. Instead of traditional covariance selection procedures, we solve this problem as maximum likelihood estimation of the precision matrix (inverse covariance matrix) under the L1 penalty. Then the anomaly score for each variable is computed by evaluating the distances between the fitted conditional distributions within the Markov blanket for that variable, for the (two) data sets to be compared. Using real-world data, we demonstrate that our matrix-based sparse structure learning approach successfully detects correlation anomalies under collinearities and heavy noise.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 2009 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Idé, Tsuyoshi and Lozano, Aurelie C. and Abe, Naoki and Liu, Yan},
	month = apr,
	year = {2009},
	doi = {10.1137/1.9781611972795.9},
	pages = {97--108},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\U76CJSUP\\Idé et al. - 2009 - Proximity-Based Anomaly Detection using Sparse Structure Learning.pdf:application/pdf},
}

@inproceedings{lozano_spatial-temporal_2009,
	address = {New York, NY, USA},
	series = {{KDD} '09},
	title = {Spatial-temporal causal modeling for climate change attribution},
	isbn = {978-1-60558-495-9},
	url = {https://doi.org/10.1145/1557019.1557086},
	doi = {10.1145/1557019.1557086},
	abstract = {Attribution of climate change to causal factors has been based predominantly on simulations using physical climate models, which have inherent limitations in describing such a complex and chaotic system. We propose an alternative, data centric, approach that relies on actual measurements of climate observations and human and natural forcing factors. Specifically, we develop a novel method to infer causality from spatial-temporal data, as well as a procedure to incorporate extreme value modeling into our method in order to address the attribution of extreme climate events, such as heatwaves. Our experimental results on a real world dataset indicate that changes in temperature are not solely accounted for by solar radiance, but attributed more significantly to CO2 and other greenhouse gases. Combined with extreme value modeling, we also show that there has been a significant increase in the intensity of extreme temperatures, and that such changes in extreme temperature are also attributable to greenhouse gases. These preliminary results suggest that our approach can offer a useful alternative to the simulation-based approach to climate modeling and attribution, and provide valuable insights from a fresh perspective.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 15th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Lozano, Aurelie C. and Li, Hongfei and Niculescu-Mizil, Alexandru and Liu, Yan and Perlich, Claudia and Hosking, Jonathan and Abe, Naoki},
	month = jun,
	year = {2009},
	pages = {587--596},
}

@inproceedings{liu_who_2009,
	title = {Who is the expert? {Analyzing} gaze data to predict expertise level in collaborative applications},
	shorttitle = {Who is the expert?},
	url = {https://ieeexplore.ieee.org/abstract/document/5202640},
	doi = {10.1109/ICME.2009.5202640},
	abstract = {In this paper, we analyze complex gaze tracking data in a collaborative task and apply machine learning models to automatically predict skill-level differences between participants. Specifically, we present findings that address the two primary challenges for this prediction task: (1) extracting meaningful features from the gaze information, and (2) casting the prediction task as a machine learning (ML) problem. The results show that our approach based on profile hidden Markov models are up to 96\% accurate and can make the determination as fast as one minute into the collaboration, with only 5\% of gaze observations registered. We also provide a qualitative analysis of gaze patterns that reveal the relative expertise level of the paired users in a collaborative learning user study.},
	urldate = {2025-01-09},
	booktitle = {2009 {IEEE} {International} {Conference} on {Multimedia} and {Expo}},
	author = {Liu, Yan and Hsueh, Pei-Yun and Lai, Jennifer and Sangin, Mirweis and Nussli, Marc-Antoine and Dillenbourg, Pierre},
	month = jun,
	year = {2009},
	note = {ISSN: 1945-788X},
	keywords = {Casting, Collaboration, Collaborative work, Data analysis, Data mining, Eye-tracking, Feature extraction, Hidden Markov models, Machine learning, Modeling and prediction of user behavior, Pattern analysis, Predictive models},
	pages = {898--901},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\\IZJKQ2B9\\5202640.html:text/html},
}

@article{perlich_breast_2008,
	title = {Breast cancer identification: {KDD} {CUP} winner's report},
	volume = {10},
	issn = {1931-0145},
	shorttitle = {Breast cancer identification},
	url = {https://doi.org/10.1145/1540276.1540289},
	doi = {10.1145/1540276.1540289},
	abstract = {We describe the ideas and methodologies that we developed in addressing the KDD Cup 2008 on early breast cancer detection, and discuss how they contributed to our success. The most important components of our solution were 1) the identification of predictive information in the patient identifier, 2) a linear SVM on the 117 provided features, and 3) a heuristic post-processing approach to optimize the evaluation criteria.},
	number = {2},
	urldate = {2025-01-09},
	journal = {SIGKDD Explor. Newsl.},
	author = {Perlich, Claudia and Melville, Prem and Liu, Yan and Świrszcz, Grzegorz and Lawrence, Richard and Rosset, Saharon},
	month = dec,
	year = {2008},
	pages = {39--42},
}

@inproceedings{he_graph-based_2008,
	title = {Graph-{Based} {Rare} {Category} {Detection}},
	url = {https://ieeexplore.ieee.org/document/4781187},
	doi = {10.1109/ICDM.2008.122},
	abstract = {Rare category detection is the task of identifying examples from rare classes in an unlabeled data set. It is an open challenge in machine learning and plays key roles in real applications such as financial fraud detection, network intrusion detection, astronomy, spam image detection, etc. In this paper, we develop a new graph-based method for rare category detection named GRADE. It makes use of the global similarity matrix motivated by the manifold ranking algorithm, which results in more compact clusters for the minority classes; by selecting examples from the regions where probability density changes the most, it relaxes the assumption that the majority classes and the minority classes are separable. Furthermore, when detailed information about the data set is not available, we develop a modified version of GRADE named GRADE-LI, which only needs an upper bound on the proportion of each minority class as input. Besides working with data with structured features, both GRADE and GRADE-LI can also work with graph data, which can not be handled by existing rare category detection methods. Experimental results on both synthetic and real data sets demonstrate the effectiveness of the GRADE and GRADE-LI algorithms.},
	urldate = {2025-01-10},
	booktitle = {2008 {Eighth} {IEEE} {International} {Conference} on {Data} {Mining}},
	author = {He, Jingrui and Liu, Yan and Lawrence, Richard},
	month = dec,
	year = {2008},
	note = {ISSN: 2374-8486},
	keywords = {Astronomy, Clustering algorithms, Data mining, graph, Helium, Image sampling, Intrusion detection, Machine learning, Manifolds, Object detection, rare category detection, Upper bound},
	pages = {833--838},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\selvam\\Zotero\\storage\\IAW6F2V5\\4781187.html:text/html},
}

@article{melville_finding_nodate,
	title = {Finding {New} {Customers} {Using} {Unstructured} and {Structured} {Data}},
	abstract = {Identifying new customers is a critical task for any salesoriented company. Of particular interest are companies that sell to other businesses, for which there is a wealth of structured information available through ﬁnancial and ﬁrmographic databases. We demonstrate that the content of company web sites can often be an even richer source of information in identifying particular business alignments. We show how supervised learning can be used to build eﬀective predictive models on unstructured web content as well as on structured ﬁrmographic data. We also explore methods to leverage the strengths of both sources by combining these data sources. Extensive empirical evaluation on a real-world marketing case study show promising results of our modeling eﬀorts.},
	language = {en},
	author = {Melville, Prem and Liu, Yan and Lawrence, Richard and Khabibrakhmanov, Ildar and Pendus, Cezar and Bowden, Timothy},
	file = {PDF:C\:\\Users\\selvam\\Zotero\\storage\\S9M7DAQX\\Melville et al. - Finding New Customers Using Unstructured and Structured Data.pdf:application/pdf},
}

@inproceedings{arnold_temporal_2007,
	address = {New York, NY, USA},
	series = {{KDD} '07},
	title = {Temporal causal modeling with graphical granger methods},
	isbn = {978-1-59593-609-7},
	url = {https://doi.org/10.1145/1281192.1281203},
	doi = {10.1145/1281192.1281203},
	abstract = {The need for mining causality, beyond mere statistical correlations, for real world problems has been recognized widely. Many of these applications naturally involve temporal data, which raises the challenge of how best to leverage the temporal information for causal modeling. Recently graphical modeling with the concept of "Granger causality", based on the intuition that a cause helps predict its effects in the future, has gained attention in many domains involving time series data analysis. With the surge of interest in model selection methodologies for regression, such as the Lasso, as practical alternatives to solving structural learning of graphical models, the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling. In this paper, we examine a host of related algorithms that, loosely speaking, fall under the category of graphical Granger methods, and characterize their relative performance from multiple viewpoints. Our experiments show, for instance, that the Lasso algorithm exhibits consistent gain over the canonical pairwise graphical Granger method. We also characterize conditions under which these variants of graphical Granger methods perform well in comparison to other benchmark methods. Finally, we apply these methods to a real world data set involving key performance indicators of corporations, and present some concrete results.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 13th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Arnold, Andrew and Liu, Yan and Abe, Naoki},
	month = aug,
	year = {2007},
	pages = {66--75},
}

@incollection{yang_harmonium_2007,
	series = {Proceedings},
	title = {Harmonium {Models} for {Semantic} {Video} {Representation} and {Classification}},
	isbn = {978-0-89871-630-6},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611972771.34},
	abstract = {Accurate and efficient video classification demands the fusion of multimodal information and the use of intermediate representations. Combining the two ideas into the one framework, we propose a probabilistic approach for video classification using intermediate semantic representations derived from multi-modal features. Based on a class of bipartite undirected graphical models named harmonium, our approach represents the video data as latent semantic topics derived by jointly modeling the transcript keywords and color-histogram features, and performs classification using these latent topics under a unified framework. We show satisfactory classification performance of our approach on a benchmark dataset as well as interesting insights into the data.},
	urldate = {2025-01-10},
	booktitle = {Proceedings of the 2007 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Yang, Jun and Liu, Yan and Xing, Eric P. and Hauptmann, Alexander G.},
	month = apr,
	year = {2007},
	doi = {10.1137/1.9781611972771.34},
	pages = {378--389},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\DT4RISUS\\Yang et al. - 2007 - Harmonium Models for Semantic Video Representation and Classification.pdf:application/pdf},
}

@article{rosset_making_2007,
	title = {Making the most of your data: {KDD} {Cup} 2007 "{How} {Many} {Ratings}" winner's report},
	volume = {9},
	issn = {1931-0145},
	shorttitle = {Making the most of your data},
	url = {https://doi.org/10.1145/1345448.1345463},
	doi = {10.1145/1345448.1345463},
	abstract = {We describe the ideas and methodologies that we developed in addressing the KDD Cup 2007 How Many Ratings task, and discuss how they contributed to our success.},
	number = {2},
	urldate = {2025-01-10},
	journal = {SIGKDD Explor. Newsl.},
	author = {Rosset, Saharon and Perlich, Claudia and Liu, Yan},
	month = dec,
	year = {2007},
	pages = {66--69},
}

@article{liu_predicting_2007,
	title = {Predicting who rated what in large-scale datasets},
	volume = {9},
	issn = {1931-0145},
	url = {https://doi.org/10.1145/1345448.1345462},
	doi = {10.1145/1345448.1345462},
	abstract = {KDD Cup 2007 focuses on movie rating behaviors. The goal of the task "Who Rated What" is to predict whether "existing" users will review "existing" movies in the future. We cast the task as a link prediction problem and address it via a simple classification approach. Compared with other applications for link prediction, there are two major challenges in our task: (1) the huge size of the Netflix data; (2) the prediction target is complicated by many factors, such as a general decrease of interest in old movies and more tendency to review more movies by Netflix users due to the success of the internet DVD rental industries. We address the first challenge by "selective" subsampling and the second by combining information from the review scores, movie contents and graph topology effectively.},
	number = {2},
	urldate = {2025-01-10},
	journal = {SIGKDD Explor. Newsl.},
	author = {Liu, Yan and Kou, Zhenzhen},
	month = dec,
	year = {2007},
	pages = {62--65},
}

@article{liu_protein_2002,
	title = {Protein {Quaternary} {Fold} {Recognition} {Using} {Conditional} {Graphical} {Models}},
	url = {https://kilthub.cmu.edu/articles/journal_contribution/Protein_Quaternary_Fold_Recognition_Using_Conditional_Graphical_Models/6608723/1},
	doi = {10.1184/R1/6608723.v1},
	abstract = {Protein fold recognition is a crucial step in inferring biological structure and function. This paper focuses on machine learning methods for predicting quaternary structural folds, which consist of multiple protein chains that form chemical bonds among side chains to reach a structurally stable domain. The complexity associated with modeling the quaternary fold poses major theoretical and computational challenges to current machine learning methods. We propose methods to address these challenges and show how (1) domain knowledge is encoded and utilized to characterize structural properties using segmentation conditional graphical models; and (2) model complexity is handled through efficient inference algorithms. Our model follows a discriminative approach so that any informative features, such as those representative of overlapping or long-range interactions, can be used conveniently. The model is applied to predict two important quaternary folds, the triple β- spirals and double-barrel trimers. Cross-family validation shows that our method outperforms other state-of-the art algorithms.},
	language = {en},
	urldate = {2025-01-10},
	author = {Liu, Yan and Carbonell, Jaime G. and Gopalakrishnan, Vanathi and Wiegele, Peter},
	month = aug,
	year = {2002},
	note = {Publisher: Carnegie Mellon University},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\B94GY2YC\\Liu et al. - 2002 - Protein Quaternary Fold Recognition Using Conditional Graphical Models.pdf:application/pdf},
}

@inproceedings{probst_semi-supervised_2007,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'07},
	title = {Semi-supervised learning of attribute-value pairs from product descriptions},
	abstract = {We describe an approach to extract attribute-value pairs from product descriptions. This allows us to represent products as sets of such attribute-value pairs to augment product databases. Such a representation is useful for a variety of tasks where treating a product as a set of attribute-value pairs is more useful than as an atomic entity. Examples of such applications include product recommendations, product comparison, and demand forecasting. We formulate the extraction as a classification problem and use a semi-supervised algorithm (co-EM) along with (Naïve Bayes). The extraction system requires very little initial user supervision: using unlabeled data, we automatically extract an initial seed list that serves as training data for the supervised and semi-supervised classification algorithms. Finally, the extracted attributes and values are linked to form pairs using dependency information and co-location scores. We present promising results on product descriptions in two categories of sporting goods.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 20th international joint conference on {Artifical} intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Probst, Katharina and Ghani, Rayid and Krema, Marko and Fano, Andrew and Liu, Yan},
	month = jan,
	year = {2007},
	pages = {2838--2843},
}

@article{ghani_text_2006,
	title = {Text mining for product attribute extraction},
	volume = {8},
	issn = {1931-0145},
	url = {https://doi.org/10.1145/1147234.1147241},
	doi = {10.1145/1147234.1147241},
	abstract = {We describe our work on extracting attribute and value pairs from textual product descriptions. The goal is to augment databases of products by representing each product as a set of attribute-value pairs. Such a representation is beneficial for tasks where treating the product as a set of attribute-value pairs is more useful than as an atomic entity. Examples of such applications include demand forecasting, assortment optimization, product recommendations, and assortment comparison across retailers and manufacturers. We deal with both implicit and explicit attributes and formulate both kinds of extractions as classification problems. Using single-view and multi-view semi-supervised learning algorithms, we are able to exploit large amounts of unlabeled data present in this domain while reducing the need for initial labeled data that is expensive to obtain. We present promising results on apparel and sporting goods products and show that our system can accurately extract attribute-value pairs from product descriptions. We describe a variety of application that are built on top of the results obtained by the attribute extraction system.},
	number = {1},
	urldate = {2025-01-10},
	journal = {SIGKDD Explor. Newsl.},
	author = {Ghani, Rayid and Probst, Katharina and Liu, Yan and Krema, Marko and Fano, Andrew},
	month = jun,
	year = {2006},
	pages = {41--48},
}

@article{liu_protein_2006,
	title = {Protein {Fold} {Recognition} {Using} {Segmentation} {Conditional} {Random} {Fields} ({SCRFs})},
	volume = {13},
	copyright = {http://www.liebertpub.com/nv/resources-tools/text-and-data-mining-policy/121/},
	issn = {1066-5277, 1557-8666},
	url = {http://www.liebertpub.com/doi/10.1089/cmb.2006.13.394},
	doi = {10.1089/cmb.2006.13.394},
	abstract = {Protein fold recognition is an important step towards understanding protein three-dimensional structures and their functions. A conditional graphical model, i.e., segmentation conditional random ﬁelds (SCRFs), is proposed as an effective solution to this problem. In contrast to traditional graphical models, such as the hidden Markov model (HMM), SCRFs follow a discriminative approach. Therefore, it is ﬂexible to include any features in the model, such as overlapping or long-range interaction features over the whole sequence. The model also employs a convex optimization function, which results in globally optimal solutions to the model parameters. On the other hand, the segmentation setting in SCRFs makes their graphical structures intuitively similar to the protein 3-D structures and more importantly provides a framework to model the long-range interactions between secondary structures directly. Our model is applied to predict the parallel β-helix fold, an important fold in bacterial pathogenesis and carbohydrate binding/cleavage. The cross-family validation shows that SCRFs not only can score all known β-helices higher than non-β-helices in the Protein Data Bank (PDB), but also accurately locates rungs in known beta-helix proteins. Our method outperforms BetaWrap, a state-of-the-art algorithm for predicting beta-helix folds, and HMMER, a general motif detection algorithm based on HMM, and has the additional advantage of general application to other protein folds. Applying our prediction model to the Uniprot Database, we identify previously unknown potential β-helices.},
	language = {en},
	number = {2},
	urldate = {2025-01-10},
	journal = {Journal of Computational Biology},
	author = {Liu, Yan and Carbonell, Jaime and Weigele, Peter and Gopalakrishnan, Vanathi},
	month = mar,
	year = {2006},
	pages = {394--406},
	file = {PDF:C\:\\Users\\selvam\\Zotero\\storage\\TMIY6J33\\Liu et al. - 2006 - Protein Fold Recognition Using Segmentation Conditional Random Fields (SCRFs).pdf:application/pdf},
}

@article{liu_predicting_2005,
	title = {Predicting {Protein} {Folds} with {Structural} {Repeats} {Using} a {Chain} {Graph} {Model}},
	url = {https://kilthub.cmu.edu/articles/journal_contribution/Predicting_Protein_Folds_with_Structural_Repeats_Using_a_Chain_Graph_Model/6608555/1},
	doi = {10.1184/R1/6608555.v1},
	abstract = {Protein fold recognition is a key step towards inferring the tertiary structures from amino-acid sequences. Complex folds such as those consisting of interacting structural repeats are prevalent in proteins involved in a wide spectrum of biological functions. However, extant approaches often perform inadequately due to their inability to capture long-range interactions between structural units and to handle low sequence similarities across proteins (under 25\% identity). In this paper, we propose a chain graph model built on a causally connected series of segmentation conditional random fields (SCRFs) to address these issues. Specifically, the SCRF model captures long-range interactions within recurring structural units and the Bayesian network backbone decomposes cross-repeat interactions into locally computable modules consisting of repeat-specific SCRFs and a model for sequence motifs. We applied this model to predict β-helices and leucine-rich repeats, and found it significantly outperforms extant methods in predictive accuracy and/or computational efficiency.},
	language = {en},
	urldate = {2025-01-10},
	author = {Liu, Yan and P Xing, Eric and Carbonell, Jaime G.},
	month = jan,
	year = {2005},
	note = {Publisher: Carnegie Mellon University},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\GNK6KTSS\\Liu et al. - 2005 - Predicting Protein Folds with Structural Repeats Using a Chain Graph Model.pdf:application/pdf},
}

@inproceedings{liu_segmentation_2005,
	address = {Berlin, Heidelberg},
	series = {{RECOMB}'05},
	title = {Segmentation conditional random fields ({SCRFs}): a new approach for protein fold recognition},
	isbn = {978-3-540-25866-7},
	shorttitle = {Segmentation conditional random fields ({SCRFs})},
	url = {https://doi.org/10.1007/11415770_31},
	doi = {10.1007/11415770_31},
	abstract = {Protein fold recognition is an important step towards understanding protein three-dimensional structures and their functions. A conditional graphical model, i.e. segmentation conditional random fields (SCRFs), is proposed to solve the problem. In contrast to traditional graphical models such as hidden markov model (HMM), SCRFs follow a discriminative approach. It has the flexibility to include overlapping or long-range interaction features over the whole sequence, as well as global optimally solutions for the parameters. On the other hand, the segmentation setting in SCRFs makes its graphical structures intuitively similar to the protein 3-D structures and more importantly, provides a framework to model the long-range interactions directly.Our model is applied to predict the parallel β-helix fold, an important fold in bacterial infection of plants and binding of antigens. The cross-family validation shows that SCRFs not only can score all known β-helices higher than non β-helices in Protein Data Bank, but also demonstrate more success in locating each rung in the known β-helix proteins than BetaWrap, a state-of-the-art algorithm for predicting β-helix fold, and HMMER, a general motif detection algorithm based on HMM. Applying our prediction model to Uniprot database, we hypothesize previously unknown β-helices.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 9th {Annual} international conference on {Research} in {Computational} {Molecular} {Biology}},
	publisher = {Springer-Verlag},
	author = {Liu, Yan and Carbonell, Jaime and Weigele, Peter and Gopalakrishnan, Vanathi},
	month = may,
	year = {2005},
	pages = {408--422},
}

@article{liu_comparison_2004,
	title = {Comparison of probabilistic combination methods for protein secondary structure prediction},
	volume = {20},
	issn = {1367-4803},
	doi = {10.1093/bioinformatics/bth370},
	abstract = {MOTIVATION: Protein secondary structure prediction is an important step towards understanding how proteins fold in three dimensions. Recent analysis by information theory indicates that the correlation between neighboring secondary structures are much stronger than that of neighboring amino acids. In this article, we focus on the combination problem for sequences, i.e. combining the scores or assignments from single or multiple prediction systems under the constraint of a whole sequence, as a target for improvement in protein secondary structure prediction.
RESULTS: We apply several graphical chain models to solve the combination problem and show that they are consistently more effective than the traditional window-based methods. In particular, conditional random fields (CRFs) moderately improve the predictions for helices and, more importantly, for beta sheets, which are the major bottleneck for protein secondary structure prediction.},
	language = {eng},
	number = {17},
	journal = {Bioinformatics (Oxford, England)},
	author = {Liu, Yan and Carbonell, Jaime and Klein-Seetharaman, Judith and Gopalakrishnan, Vanathi},
	month = nov,
	year = {2004},
	pmid = {15217817},
	keywords = {Algorithms, Computer Simulation, Models, Chemical, Models, Molecular, Models, Statistical, Protein Conformation, Protein Folding, Protein Structure, Secondary, Proteins, Sequence Alignment, Sequence Analysis, Protein, Structure-Activity Relationship},
	pages = {3099--3107},
	file = {Full Text:C\:\\Users\\selvam\\Zotero\\storage\\3GX6WGUS\\Liu et al. - 2004 - Comparison of probabilistic combination methods for protein secondary structure prediction.pdf:application/pdf},
}

@inproceedings{lafferty_kernel_2004,
	address = {New York, NY, USA},
	series = {{ICML} '04},
	title = {Kernel conditional random fields: representation and clique selection},
	isbn = {978-1-58113-838-2},
	shorttitle = {Kernel conditional random fields},
	url = {https://doi.org/10.1145/1015330.1015337},
	doi = {10.1145/1015330.1015337},
	abstract = {Kernel conditional random fields (KCRFs) are introduced as a framework for discriminative modeling of graph-structured data. A representer theorem for conditional graphical models is given which shows how kernel conditional random fields arise from risk minimization procedures defined using Mercer kernels on labeled graphs. A procedure for greedily selecting cliques in the dual representation is then proposed, which allows sparse representations. By incorporating kernels and implicit feature spaces into conditional graphical models, the framework enables semi-supervised learning algorithms for structured data through the use of graph kernels. The framework and clique selection methods are demonstrated in synthetic data experiments, and are also applied to the problem of protein secondary structure prediction.},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the twenty-first international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Lafferty, John and Zhu, Xiaojin and Liu, Yan},
	month = jul,
	year = {2004},
	pages = {64},
}

@article{jin_new_2003,
	title = {A {New} {Boosting} {Algorithm} {Using} {Input}-{Dependent} {Regularizer}},
	copyright = {In Copyright},
	url = {https://kilthub.cmu.edu/articles/A_New_Boosting_Algorithm_Using_Input-Dependent_Regularizer/6620711/1},
	doi = {10.1184/R1/6620711.V1},
	abstract = {AdaBoost has proved to be an effective method to improve the performance of base classifiers both theoretically and empirically. However, previous studies have shown that AdaBoost might suffer from the overfitting problem, especially for noisy data. In addition, most current work on boosting assumes that the combination weights are fixed constants and therefore does not take particular input patterns into consideration. In this paper, we present a new boosting algorithm, “WeightBoost”, which tries to solve these two problems by introducing an input-dependent regularization factor to the combination weight. Similarly to AdaBoost, we derive a learning procedure for WeightBoost, which is guaranteed to minimize training errors. Empirical studies on eight different UCI data sets and one text categorization data set show that WeightBoost almost always achieves a considerably better classification accuracy than AdaBoost. Furthermore, experiments on data with artificially controlled noise indicate that the WeightBoost is more robust to noise than AdaBoost.},
	urldate = {2025-01-10},
	author = {Jin, Rong and Liu, Yan and Si, Luo and Carbonell, Jaime G. and Hauptmann, Alexander G.},
	year = {2003},
	note = {Artwork Size: 215209 Bytes
Publisher: Carnegie Mellon University},
	keywords = {80399 Computer Software not elsewhere classified, FOS: Computer and information sciences},
	pages = {215209 Bytes},
}

@inproceedings{yan_predicting_2003,
	title = {On predicting rare classes with {SVM} ensembles in scene classification},
	volume = {3},
	url = {https://ieeexplore.ieee.org/document/1199097},
	doi = {10.1109/ICASSP.2003.1199097},
	abstract = {Scene classification is an important technique to infer high-level semantic scene categories from low-level visual features. However, in the real world the positive data for many scenes may be rare, which degrades the performance of many classifiers. In this paper, we propose SVM ensembles to address the rare class problem. Various classifier combination strategies are investigated, including majority voting, sum rule, neural network gater and hierarchical SVMs. We also compare our method with two other common approaches for dealing with the rare class problem. Our experimental results show that hierarchical SVMs can achieve significantly better and more stable performance than other strategies, as well as high computational efficiency.},
	urldate = {2025-01-10},
	booktitle = {2003 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}, 2003. {Proceedings}. ({ICASSP} '03).},
	author = {Yan, Rong and Liu, Yan and Jin, Rong and Hauptmann, A.},
	month = apr,
	year = {2003},
	note = {ISSN: 1520-6149},
	keywords = {Boosting, Computer science, Computer vision, Kernel, Layout, Quadratic programming, Sampling methods, Support vector machine classification, Support vector machines, Testing},
	pages = {III--21},
}

@inproceedings{liu_new_2003,
	address = {Berlin, Heidelberg},
	title = {A {New} {Pairwise} {Ensemble} {Approach} for {Text} {Classification}},
	isbn = {978-3-540-39857-8},
	doi = {10.1007/978-3-540-39857-8_26},
	abstract = {Text classification, whether by topic or genre, is an important task that contributes to text extraction, retrieval, summarization and question answering. In this paper we present a new pairwise ensemble approach, which uses pairwise Support Vector Machine (SVM) classifiers as base classifiers and “input-dependent latent variable” method for model combination. This new approach better captures the characteristics of genre classification, including its heterogeneous nature. Our experiments on two multi-genre collections and one topic-based classification datasets show that the pairwise ensemble method outperforms both boosting, which has been demonstrated as a powerful ensemble approach, and Error-Correcting Output Codes (ECOC), which applies pairwise-like classifiers for multiclass classification problems.},
	language = {en},
	booktitle = {Machine {Learning}: {ECML} 2003},
	publisher = {Springer},
	author = {Liu, Yan and Carbonell, Jaime and Jin, Rong},
	editor = {Lavrač, Nada and Gamberger, Dragan and Blockeel, Hendrik and Todorovski, Ljupčo},
	year = {2003},
	keywords = {Ensemble Approach, Hierarchical Mixture, Latent Variable Approach, Support Vector Machine, Text Categorization},
	pages = {277--288},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\JRPAGJDN\\Liu et al. - 2003 - A New Pairwise Ensemble Approach for Text Classification.pdf:application/pdf},
}

@article{liu_boosting_nodate,
	title = {Boosting to {Correct} {Inductive} {Bias} in {Text} {Classiﬁcation}},
	abstract = {This paper studies the eﬀects of boosting in the context of diﬀerent classiﬁcation methods for text categorization, including Decision Trees, Naive Bayes, Support Vector Machines (SVMs) and a Rocchio-style classiﬁer. We identify the inductive biases of each classiﬁer and explore how boosting, as an error-driven resampling mechanism, reacts to those biases. Our experiments on the Reuters-21578 benchmark show that boosting is not eﬀective in improving the performance of the base classiﬁers on common categories. However, the eﬀect of boosting for rare categories varies across classiﬁers: for SVMs and Decision Trees, we achieved a 1317\% performance improvement in macro-averaged F1 measure, but did not obtain substantial improvement for the other two classiﬁers. This interesting ﬁnding of boosting on rare categories has not been reported before.},
	language = {en},
	author = {Liu, Yan and Yang, Yiming and Carbonell, Jaime},
	file = {PDF:C\:\\Users\\selvam\\Zotero\\storage\\EQY4CIZX\\Liu et al. - Boosting to Correct Inductive Bias in Text Classiﬁcation.pdf:application/pdf},
}
