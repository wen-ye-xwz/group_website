
@misc{cao_tempo_2024,
	title = {{TEMPO}: {Prompt}-based {Generative} {Pre}-trained {Transformer} for {Time} {Series} {Forecasting}},
	shorttitle = {{TEMPO}},
	url = {http://arxiv.org/abs/2310.04948},
	doi = {10.48550/arXiv.2310.04948},
	abstract = {The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Cao, Defu and Jia, Furong and Arik, Sercan O. and Pfister, Tomas and Zheng, Yixiang and Ye, Wen and Liu, Yan},
	month = apr,
	year = {2024},
	note = {arXiv:2310.04948 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted by ICLR 2024. Camera Ready Version},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\YCUVGU77\\Cao et al. - 2024 - TEMPO Prompt-based Generative Pre-trained Transformer for Time Series Forecasting.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\38M74X4X\\2310.html:text/html},
}

@misc{cao_timedit_2024,
	title = {{TimeDiT}: {General}-purpose {Diffusion} {Transformers} for {Time} {Series} {Foundation} {Model}},
	shorttitle = {{TimeDiT}},
	url = {http://arxiv.org/abs/2409.02322},
	doi = {10.48550/arXiv.2409.02322},
	abstract = {With recent advances in building foundation models for texts and video data, there is a surge of interest in foundation models for time series. A family of models have been developed, utilizing a temporal auto-regressive generative Transformer architecture, whose effectiveness has been proven in Large Language Models. While the empirical results are promising, almost all existing time series foundation models have only been tested on well-curated ``benchmark'' datasets very similar to texts. However, real-world time series exhibit unique challenges, such as variable channel sizes across domains, missing values, and varying signal sampling intervals due to the multi-resolution nature of real-world data. Additionally, the uni-directional nature of temporally auto-regressive decoding limits the incorporation of domain knowledge, such as physical laws expressed as partial differential equations (PDEs). To address these challenges, we introduce the Time Diffusion Transformer (TimeDiT), a general foundation model for time series that employs a denoising diffusion paradigm instead of temporal auto-regressive generation. TimeDiT leverages the Transformer architecture to capture temporal dependencies and employs diffusion processes to generate high-quality candidate samples without imposing stringent assumptions on the target distribution via novel masking schemes and a channel alignment strategy. Furthermore, we propose a finetuning-free model editing strategy that allows the seamless integration of external knowledge during the sampling process without updating any model parameters. Extensive experiments conducted on a varity of tasks such as forecasting, imputation, and anomaly detection, demonstrate the effectiveness of TimeDiT.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Cao, Defu and Ye, Wen and Zhang, Yizhou and Liu, Yan},
	month = sep,
	year = {2024},
	note = {arXiv:2409.02322 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 23 Pages, 6 Figures, 11 Tables. First present at ICML 2024 Workshop on Foundation Models in the Wild},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\82CLVH34\\Cao et al. - 2024 - TimeDiT General-purpose Diffusion Transformers for Time Series Foundation Model.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\W5FPLSNS\\2409.html:text/html},
}

@article{jia_gpt4mts_2024,
	title = {{GPT4MTS}: {Prompt}-based {Large} {Language} {Model} for {Multimodal} {Time}-series {Forecasting}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{GPT4MTS}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/30383},
	doi = {10.1609/aaai.v38i21.30383},
	abstract = {Time series forecasting is an essential area of machine learning with a wide range of real-world applications. Most of the previous forecasting models aim to capture dynamic characteristics from uni-modal numerical historical data. Although extra knowledge can boost the time series forecasting performance, it is hard to collect such information. In addition, how to fuse the multimodal information is non-trivial. In this paper, we first propose a general principle of collecting the corresponding textual information from different data sources with the help of modern large language models (LLM). Then, we propose a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously, named GPT4MTS. In practice, we propose a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication. Through extensive experiments, we demonstrate the effectiveness of our proposed method on forecasting tasks with extra-textual information.},
	language = {en},
	number = {21},
	urldate = {2025-01-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Jia, Furong and Wang, Kevin and Zheng, Yixiang and Cao, Defu and Liu, Yan},
	month = mar,
	year = {2024},
	note = {Number: 21},
	keywords = {AI For Accessibility},
	pages = {23343--23351},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\Y6SNJ5WI\\Jia et al. - 2024 - GPT4MTS Prompt-based Large Language Model for Multimodal Time-series Forecasting.pdf:application/pdf},
}

@misc{ye_beyond_2024,
	title = {Beyond {Forecasting}: {Compositional} {Time} {Series} {Reasoning} for {End}-to-{End} {Task} {Execution}},
	shorttitle = {Beyond {Forecasting}},
	url = {http://arxiv.org/abs/2410.04047},
	doi = {10.48550/arXiv.2410.04047},
	abstract = {In recent decades, there has been substantial advances in time series models and benchmarks across various individual tasks, such as time series forecasting, classification, and anomaly detection. Meanwhile, compositional reasoning in time series is prevalent in real-world applications (e.g., decision-making and compositional question answering) and is in great demand. Unlike simple tasks that primarily focus on predictive accuracy, compositional reasoning emphasizes the synthesis of diverse information from both time series data and various domain knowledge, making it distinct and extremely more challenging. In this paper, we introduce Compositional Time Series Reasoning, a new task of handling intricate multistep reasoning tasks from time series data. Specifically, this new task focuses on various question instances requiring structural and compositional reasoning abilities on time series data, such as decision-making and compositional question answering. As an initial attempt to tackle this novel task, we developed TS-Reasoner, a program-aided approach that utilizes large language model (LLM) to decompose a complex task into steps of programs that leverage existing time series models and numerical subroutines. Unlike existing reasoning work which only calls off-the-shelf modules, TS-Reasoner allows for the creation of custom modules and provides greater flexibility to incorporate domain knowledge as well as user-specified constraints. We demonstrate the effectiveness of our method through a comprehensive set of experiments. These promising results indicate potential opportunities in the new task of time series reasoning and highlight the need for further research.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Ye, Wen and Zhang, Yizhou and Yang, Wei and Tang, Lumingyuan and Cao, Defu and Cai, Jie and Liu, Yan},
	month = oct,
	year = {2024},
	note = {arXiv:2410.04047 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\9RV83KWQ\\Ye et al. - 2024 - Beyond Forecasting Compositional Time Series Reasoning for End-to-End Task Execution.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\H5HT7GFC\\2410.html:text/html},
}

@inproceedings{zhang_toward_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Toward {Mitigating} {Misinformation} and {Social} {Media} {Manipulation} in {LLM} {Era}},
	isbn = {979-8-4007-0172-6},
	url = {https://dl.acm.org/doi/10.1145/3589335.3641256},
	doi = {10.1145/3589335.3641256},
	abstract = {The pervasive abuse of misinformation to influence public opinion on social media has become increasingly evident in various domains, encompassing politics, as seen in presidential elections, and healthcare, most notably during the recent COVID-19 pandemic. This threat has grown in severity as the development of Large Language Models (LLMs) empowers manipulators to generate highly convincing deceptive content with greater efficiency. Furthermore, the recent strides in chatbots integrated with LLMs, such as ChatGPT, have enabled the creation of human-like interactive social bots, posing a significant challenge to both human users and the social-bot-detection systems of social media platforms.These challenges motivate researchers to develop algorithms to mitigate misinformation and social media manipulations. This tutorial introduces the advanced machine learning researches that are helpful for this goal, including (1) detection of social manipulators, (2) learning causal models of misinformation and social manipulation, and (3) LLM-generated misinformation detection. In addition, we also present possible future directions.},
	urldate = {2025-01-07},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Yizhou and Sharma, Karishma and Du, Lun and Liu, Yan},
	month = may,
	year = {2024},
	pages = {1302--1305},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\W2YRQ67F\\Zhang et al. - 2024 - Toward Mitigating Misinformation and Social Media Manipulation in LLM Era.pdf:application/pdf},
}

@article{zhang_hierarchical_2022,
	title = {Hierarchical {Gaussian} {Mixture} based {Task} {Generative} {Model} for {Robust} {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=A4fSkNAs6E1},
	abstract = {Meta-learning enables quick adaptation of machine learning models to new tasks with limited data. While tasks could come from varying distributions in reality, most of the existing meta-learning methods consider both training and testing tasks as from the same uni-component distribution, overlooking two critical needs of a practical solution: (1) the various sources of tasks may compose a multi-component mixture distribution, and (2) novel tasks may come from a distribution that is unseen during meta-training. In this paper, we demonstrate these two challenges can be solved jointly by modeling the density of task instances. We develop a meta-training framework underlain by a novel Hierarchical Gaussian Mixture based Task Generative Model (HTGM). HTGM extends the widely used empirical process of sampling tasks to a theoretical model, which learns task embeddings, fits mixture distribution of tasks, and enables density-based scoring of novel tasks. The framework is agnostic to the encoder and scales well with large backbone networks. The model parameters are learned end-to-end by maximum likelihood estimation via an Expectation-Maximization algorithm. Extensive experiments on benchmark datasets indicate the effectiveness of our method for both sample classification and novel task detection.},
	language = {en},
	urldate = {2025-01-08},
	author = {Zhang, Yizhou and Ni, Jingchao and Cheng, Wei and Chen, Zhengzhang and Tong, Liang and Chen, Haifeng},
	month = sep,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\selvam\\Zotero\\storage\\ULDV5CBN\\Zhang et al. - 2022 - Hierarchical Gaussian Mixture based Task Generative Model for Robust Meta-Learning.pdf:application/pdf},
}

@misc{zhang_examination_2024,
	title = {An {Examination} on the {Effectiveness} of {Divide}-and-{Conquer} {Prompting} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.05359},
	doi = {10.48550/arXiv.2402.05359},
	abstract = {Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, simple instructional prompts suffer from inaccurate responses. Existing works show that more complicated prompting strategies, such as Chain-of-Thoughts and Least-to-Most, can unlock LLM's powerful capacity in diverse areas. Recent researches reveal that simple divide-and-conquer prompting strategy, i.e. simply dividing the input sequence to multiple sub-inputs, can also substantially improve LLM's performance in some specific tasks such as misinformation detection. In this paper, we aim at examining the utility of divide-and-conquer prompting strategy and answer on which kind of tasks this strategy gets advantages. Specifically, we provide a theoretic analysis to divide-and-conquer prompting strategy and help us identify the specific tasks where DaC prompting can bring performance boost with theoretic guarantee. We then present two cases (large integer arithmetic and fact verification) where experimental results aligns with our theoretic analysis.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Zhang, Yizhou and Du, Lun and Cao, Defu and Fu, Qiang and Liu, Yan},
	month = jul,
	year = {2024},
	note = {arXiv:2402.05359 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Preprint},
	file = {Preprint PDF:C\:\\Users\\selvam\\Zotero\\storage\\Z84SYSFW\\Zhang et al. - 2024 - An Examination on the Effectiveness of Divide-and-Conquer Prompting in Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\selvam\\Zotero\\storage\\AIVXZCJV\\2402.html:text/html},
}
