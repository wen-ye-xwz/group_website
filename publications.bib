
@misc{cao_tempo_2024,
	title = {{TEMPO}: {Prompt}-based {Generative} {Pre}-trained {Transformer} for {Time} {Series} {Forecasting}},
	shorttitle = {{TEMPO}},
	url = {http://arxiv.org/abs/2310.04948},
	doi = {10.48550/arXiv.2310.04948},
	abstract = {The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.},
	urldate = {2024-12-29},
	publisher = {arXiv},
	author = {Cao, Defu and Jia, Furong and Arik, Sercan O. and Pfister, Tomas and Zheng, Yixiang and Ye, Wen and Liu, Yan},
	month = apr,
	year = {2024},
	note = {arXiv:2310.04948 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted by ICLR 2024. Camera Ready Version},
	file = {Preprint PDF:/Users/csadmin/Zotero/storage/GFVAWFGQ/Cao et al. - 2024 - TEMPO Prompt-based Generative Pre-trained Transfo.pdf:application/pdf;Snapshot:/Users/csadmin/Zotero/storage/WKL94CTT/2310.html:text/html},
}


@article{cao2024timedit,
  title={Timedit: General-purpose diffusion transformers for time series foundation model},
  author={Cao, Defu and Ye, Wen and Zhang, Yizhou and Liu, Yan},
  journal={arXiv preprint arXiv:2409.02322},
  year={2024}
}
